{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c4aa71",
   "metadata": {},
   "source": [
    "<center><img width=\"25%\" src=\"https://www.camaramirimdoce.sc.gov.br/media/noticia/resumo-da-sessao-10-06-2019-111.png?w=848&h=450&t=P&c=f0f0f0&q=80&v=2\"></center>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Nível 1 - Fundamentos de Analytics\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04552a0f",
   "metadata": {},
   "source": [
    "# **Sumário:**\n",
    "1. [Parte A: Análise Exploratória de Dados (EDA)](#Parte-A:-Análise-Exploratória-de-Dados-(EDA))\n",
    "    * Introdução ao Mundo dos Dados\n",
    "    * Microsoft Excel para Analytics\n",
    "    * Como resumir e interpretar dados?\n",
    "    * Análise de Correlação e Associação\n",
    "2. [Parte B: Análise Preditiva e Probabilidades](#Parte-B:-Análise-Preditiva-e-Probabilidades)\n",
    "    * Prevendo o futuro: Probabilidade Clássica e Frequentista\n",
    "    * Prevendo o futuro: Modelos de Probabilidade, Uniforme e Curva Normal\n",
    "3. [Parte C: Inferência e Metodologia de Projetos](#Parte-C:-Inferência-e-Metodologia-de-Projetos)\n",
    "    * Como trabalhar com amostras de dados\n",
    "    * SQL para Análise de Dados - Nível 1\n",
    "    * Método CRISP-DM\n",
    "    * Calculando o Valor de um Projeto de Dados e Apresentando os Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125ae5a",
   "metadata": {},
   "source": [
    "### **Parte A: Análise Exploratória de Dados (EDA)** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990d656",
   "metadata": {},
   "source": [
    "### <u>Introdução ao Mundo dos Dados</u><br>\n",
    "#### A Revolução Industrial 4.0 já começou e você pode estar atrasado: <br>\n",
    "##### O futuro é da automação e inteligência artificial\n",
    "* 1° revolução industrial: Mecanização dos equipamentos\n",
    "* 2° revolução industrial: Eletricidade e Produção em Massa\n",
    "* 3° revolução industrial: Produção Flexível e Informática\n",
    "* 4° revolução industrial: \n",
    "    * Conectivadade\n",
    "    * Internet das Coisas (Iot)\n",
    "    * Big Data - Volume: Milhões de registros e armazenamento, Velocidade: Processamento em lote e Tempo real / próximo, Variedade: Estruturados e Não estruturado, Veracidade: Qualidade e Disponibilidade e Valor: Tem utilidade? e Por que armazenar?)\n",
    "    * Ciência de Dados e IA: Junção das técnicas analíticas, ferramentas computacionais e conhecimento de negócio de forma a otimizar o processo decisócio das empresas ou mesmo resolver problemas de qualquer espécie com o apoio dos dados.\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### Quais as habilidades a serem desenvolvidas neste ano em diante: <br>\n",
    "* Flexibidade: Capacidade de se adaptar facilmente a cenários diferentes do habitual.\n",
    "* Metodologias ágeis: Nomes como MVP (produto mínimo viável), validação, Sprints, Cultura do fail Fast (Tentativa e erro) e Testes A/B fazem parte dessa cultura. (Nova forma como as equipes trabalham para entregar valor no menor tempo possível).\n",
    "* Inteligência emocional: Capacidade de colocar-se no lugar do outro (Empatia).\n",
    "* Capacidade de auto gestão: Capacidade de gerir o prórprio desempenho e produtividade.\n",
    "* Criatividade: Propor novas formas de solucionar problemas.\n",
    "* Comunicação: habilidade para se comunicar em diversos tipos de cenários, contextos e pessoas.\n",
    "* Life Long Learning (Formação contínua): Capacidade de aprender de forma contínua. \n",
    "* Empreendedorismo: Transformar ideias em produtos tangíveis. \n",
    "* Análise de dados e mentalidade analítica: Atrelado ao pensamento crítico\n",
    "\n",
    "<br>\n",
    "\n",
    "#### O que são os \"dados\" e qual o papel das técnicas e ferramentas nisso tudo? <br>\n",
    "* Dados brutos e não estruturados → Preparação dos dados (Configurar) → Dados estruturados\n",
    "    * Dados estruturados: Requer menos espaço de armazenamento, mostrados como linhas, colunas e em bancos relacionais, mais fácil de ser protegidos com regras de negócios, são números, datas e textos e corresponde a 20% dos dados das empresas.\n",
    "    * Dados não estruturados: Requer mais espaço de armazenamento, não são mostrados como tabelas, é mais difícil de ser protegidos com regras de negócios, são imagens, vídeos, áudios, redes sociais, reviwes de usuários e correspondem a 80% dos dados das empresas.\n",
    "* Dados → Ferramentas de dados → Técnicas e Metodologias de dados → Valor gerado pelos dados (Tomada de decisão)\n",
    "    * Técnicas e Metodologias: Respondem as perguntas de negócio (Quanto mais técnicas conhecer, mas perguntas vai responder e mais valor vai gerar)\n",
    "    * Ferramentas: Geram produtividade para responder as perguntas.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Análise de Dados: É uma máquina de solução de problemas <br>\n",
    "* Técnicas (Arsenal analítico): Matemática, Ténicas de otimização, Estatística descritiva, Probabilidades, Inferência (ex: teste de hipótese), Modelagem estatísticas, Machine learning,..\n",
    "    * Estatísticas e Probabilidades: Resumir uma base de dados e entender a relação entre as variáveis.\n",
    "    * Inferência estatística: Estimar valores através de amostras e testar hipotéses através de amostras.\n",
    "    * Machine learning e AI: Estimar valores de forma automática, classificar coisas de forma automática e gerar dados de forma automática.\n",
    "* Ferramentas: Planilhas e tipos de arquivos, Linguagem de programação, Banco de dados, Visualização de dados, Infra estrutura (DWs, Data Lakes), ETL / Pipelines de dados, etc.\n",
    "* Conhecimento de negócio: Entendimento dos processos internos, Alinhamento com os objetivos da empresa, Entrosamento com as partes interessadas, Análise de concorrência, etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Técnicas de Análise de Dados: Estatísticas e Probabilidades <br>\n",
    "* Estatística: O conjunto de técnicas analíticas que permite, organizar, descrever, analisar e interpretar os dados oriundos de estudos ou experimentos, realizados em qualquer área do conhecimento. Pode ser dividida em: Estatística descritiva, Probabilidade e Inferência estatística. \n",
    "* Estatística Descritiva: Conjunto de técnicas para resumir dados de todos os tipos e tamanhos. Utilizada sempre que quiser resumir uma ou mais colunas de dados para interpretar o comportamento (Análise unidimensional). Exemplos: Tabelas de frequência, Medidas de resumo (Média, Mediana, Quartis, Desvio Padrão etc) e Gráficos analíticos (Barras, linhas, Pareto, Histograma, Box-plots etc) - Interpretações iniciais\n",
    "* Análise de Associação: Conjunto de técnica para analisar o comportamento de duas variáveis. Utilizada sempre que quiser resumir simultaneamente duas colunas e interpretar o comportamento e associação entre elas (Análise bidimensional). Exemplos: Correlação de Pearson (Para dados lineares), Coeficiente de Determinação (R Quadrado), Information Value (IV) e Estudo de Causalidade.\n",
    "* Probabilidade: Conjunto de técnicas para prever o acontecimento de algum evento. Utilizada quando envolver projeções futuras de indicadores e eventos de interesse como inadimplência de clientes, Churn, TurnOver, Vendas, etc. Exemplos: Teoria frequentista, Modelos de Probabilidade (Uniforme, t-student, Normal etc) e Machine Learning. A probabilidade pode ser pensada como a teoria matmática utilizada para se estiudar a incerteza oriunda de fenômenos de caráter aleatório (Situação ou acontecimento cujos resultados não podem ser previstos com certeza)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Técnicas de Análise de Dados: Inferência estatística <br>\n",
    "* Estimação: Conjunto de técnicas para estimar valores de interesse através de amostra de dados. Utilizada sempre que não tiver acesso a base completa (população) do evento de interesse. Exemplos: Extração de amostras randomizadas e com o menor viés possível e estimação por intervalos de confiança (Cálculo do Tamanho da Amostra e Controle da Margem de Erro)\n",
    "* Testes de hipóteses: Técnica para se testar valores da população através de amostras de dados. Utilizada sempre que tiver uma hipótese sobre a população e quiser ter um suporte estatístico para a tomada de decisão. Exemplos: Experimentos (Grupo de controle e teste), Testes de hipóteses para médias e proporções, Testes de hipóteses para grupos (Teste A/B) e Análise do p-valor.\n",
    "* Extensões dos resultados depois de se obter as primeiras interpretações (Estatística descritiva)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Técnicas de Análise de Dados: Machine Learning e AI <br>\n",
    "* Modelos Supervisionados: Conjunto de técnicas que estimam valores numéricos (regressores) ou classificam \"eventos\" de forma automática (classificadores) = Variáveis explicativas, onde é obrigado a ter uma variável resposta (target) mensurada. Utilizada sempre que precisa tomar uma decisão estatística de forma automatizada, via sistema, algoritmo, etc. Exemplos: Regressões, Árvore de decisão, Redes neurais artificiais, Modelos combinados (ensemble).\n",
    "* Modelos não supervisionados: Conjunto de técnicas buscam padrões nos dados sem depender de uma variável resposta (target) mensurada. Utilizada sempre que quise entender concentrações, dados fora de padrão (outliers) e realizar visualizações de dados complexas com multiplas variáveis. Exemplos: Clusterizações (Método k-means, DBScan), Detecções de anomalias, PCA (Componentes principais) e t-SNE.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Principais ferramentas para Análise de Dados <br>\n",
    "* Excel\n",
    "* SQL\n",
    "* Power BI\n",
    "* Python\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Metodologia em Projetos de Dados <br>\n",
    "##### A metodologia CRISP-DM é praticada em 6 grandes passos.\n",
    "1. Entendimento do negócio: Objetivo, Premissas, Riscos envolvidos, Custos x Benefícios, Critérios de sucesso e Planejamento do projeto e início da documentação.\n",
    "2. Entendimento dos dados: Descrição dos dados, Coleta, Análise Exploratória e Verificação da qualidade dos dados (Sanity Check).\n",
    "3. Preparação dos dados: Seleção de variáveis, Limpeza dos dados e Formatação dos dados.\n",
    "4. Modelagem / Análise: Escolha da técnica estatística que responde o problema, Desenvolvimento do estudo analítico ou modelo e Finalização.\n",
    "5. Validação: Verificação dos critérios de sucesso, Validação do estudo / modelo, Aprovação pelo cliente do trabalho e Atualização do Roadmap.\n",
    "6. Deploy (Implantação): Plano de implantação, Monitoramento do modelo/KPI’s e Finalização da documentação.\n",
    "##### Antes de olhar um resultado de um trabalho, olhe como ele foi construído. Só assim é possível confiar no resultado. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Cultura Data-Driven <br>\n",
    "##### Uma empresa com uma Cultura Data-Driven utiliza os dados em todo processo decisório. A empresa trata os dados como um ATIVO estratégico (Não é um subproduto) e busca disseminar o acesso aos dados de forma ampla para todos os colaboradores. Este processo foca em capturar, limpar e armazenar dados relevantes. A empresa promove a experimentação e aprendizado constantes através dos dados.\n",
    "* Dados → Informações → Conhecimento → Insight → Sabedoria\n",
    "* Passos para implementação:\n",
    "    * 1 - Mudança de Mentalidade (Os dados são o centro do processo decisório)\n",
    "        * Dados coletados, organizados e validados devem ser utilizados para a tomada de decisão.\n",
    "        * Experiências passadas precisam cada vez mais estar acompanhadas de análise de dados\n",
    "    * 2 - Fornecimento de Dados (Utilização de um único repositório de dados que tenha uma documentação de qualidade - Tenha um processo de governança)\n",
    "        * Cultura de coleta (Falta de acesso ou inexistência de dados → Quais dados serão armazenados? Onde serão armazenados? Por qual período? Em qual formato?)\n",
    "        * Governança e qualidade (As empresas devem ter dados consistentes, documentação acessíveis com as informações do tratamento e das transformações dos dados e considerar a LGPD)\n",
    "    * 3 - Alfabetização de Dados (Os colaboradores precisam ser capazes de explorar os dados da empresa para resolver problemas de negócio)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Letramento de dados (Data Literacy) <br>\n",
    "##### A Gartner define “Alfabetização de Dados” como a habilidade para ler, escrever e comunicar os dados em seu devido contexto, incluindo um entendimento de fontes de dados e suas transformações, dos métodos e técnicas analíticas aplicadas, descrevendo assim as aplicações dessas técnicas no problema de negócio e como espera-se obter valor disso (resultado tangível e esperado).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### As principais funções analíticas: <br>\n",
    "* Processo de Transformação dos Dados = Dados precisam ser organizados, limpos, padronizados para serem armazenados no repositório.\n",
    "* Decisões são tomadas = Desenvolvimento de análises exploratórias e Dashboards interativos.\n",
    "* Decisões automáticas são tomadas = Desenvolvimento de modelos estatísticos e de Machine Learning/AI.\n",
    "* Produtização da decisão = Construção de códigos que asseguram uma eficiente implantação dos produtos de dados anteriores.\n",
    "\n",
    "* Dados Brutos → Processo de Transformação dos Dados → Data Lake (repositório) \n",
    "    * Data Lake (repositório) → Decisões são tomadas → Analytics e Dashboards → Produtização da decisão → Produção/Deploy → Valor Tangível que os dados podem entregar\n",
    "    * Data Lake (repositório) → Decisões automáticas são tomadas → Machine Learning e AI → Produtização da decisão → Produção/Deploy → Valor Tangível que os dados podem entregar\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Profissionais dos dados: <br>\n",
    "* Arquiteto de Dados\n",
    "    * Dados Brutos → Processo de Transformação dos Dados → Data Lake (repositório)\n",
    "    * Planeja como o dado será armazenado, seu fluxo de tratamento e segurança.\n",
    "    * Funções por hierarquia de necessidades → Coletar: Instrumentação, Registro, Sensores, Dados externos e Conteúdo gerado pelo usuário. \n",
    "* Engenheiro de Dados\n",
    "    * Dados Brutos → Processo de Transformação dos Dados → Data Lake (repositório)\n",
    "    * Constrói fluxos de extração de dados brutos, com seus respectivos tratamentos e cargas em repositórios de dados.\n",
    "    * Funções por hierarquia de necessidades → Mover/Armazenar: Fluxo de dados confiável, Infraestrutura, Pipelines, ETL, Armazenamento de dados estruturados e não estruturados.\n",
    "    * Objetivo → Disponibilizar dados de qualidade para todas as áreas.\n",
    "    * Principais responsabilidades → Documentar as fontes de informação, Desenvolver pipelines de dados, desde a fonte até o dado transformado e útil e Sustentação e guardião da qualidade da esteira de Dados.\n",
    "* Analista de Dados\n",
    "    * Data Lake (repositório) →  Decisões são tomadas →  Analytics e Dashboards\n",
    "    * Desenvolve análises exploratórias e Dashboards interativos\n",
    "    * Funções por hierarquia de necessidades → Explorar/Transformar: Limpeza, Detecção de anomalias e Preparação.\n",
    "    * Funções por hierarquia de necessidades → Agregar/Rotular: Análises, Métricas, Segmentos, Agregados, Recursos, Dados de treinamento, Teste A/B, Experimentação e Algoritmos simples de ML.\n",
    "    * Objetivo → Realizar análises de dados, construir relatórios, KPI’s e Dashboards.\n",
    "    * Principais responsabilidades → Realizar análises de dados que forneçam insights acionáveis, Propor relatórios de acompanhamentos de métricas e KPI’s e Ser um acelerador para a Alfabetização de Dados da empresa.\n",
    "* Cientista de Dados\n",
    "    * Data Lake (repositório) →  Decisões automáticas são tomadas → Machine Learning e AI \n",
    "    * Desenvolve modelos estatísticos e de Machine Learning/AI\n",
    "    * Funções por hierarquia de necessidades → Explorar/Transformar: Limpeza, Detecção de anomalias e Preparação.\n",
    "    * Funções por hierarquia de necessidades → Agregar/Rotular: Análises, Métricas, Segmentos, Agregados, Recursos, Dados de treinamento, Teste A/B, Experimentação e Algoritmos simples de ML.\n",
    "    * Objetivo → Desenvolver modelos preditivos e otimizações complexas.\n",
    "    * Principais responsabilidades → Planejar testes A/B, Desenvolver estudos, modelos descritivos e preditivos e Identificar padrões que permitam otimizar os mais diversos processos utilizando dados.\n",
    "* Engenheiro de Analytics ou Machine Learning\n",
    "    * Analytics e Dashboards →  Produtização da decisão → Produção/Deploy → Valor Tangível que os dados podem entregar\n",
    "    * Machine Learning e AI → Produtização da decisão → Produção/Deploy → Valor Tangível que os dados podem entregar\n",
    "    * Constrói códigos que asseguram uma eficiente implantação dos produtos de dados anteriores\n",
    "    * Funções por hierarquia de necessidades → Aprender/Otimizar: AI e Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883bb0e0",
   "metadata": {},
   "source": [
    "### <u>Microsoft Excel para analytics:</u><br>\n",
    "#### Parte 1 - Fundamento de Excel <br>\n",
    "* Planilhas, linhas, colunas, atalhos e teclas de funções\n",
    "* Formatação e Comando \"IR para especial\"\n",
    "* Auto preenchimento de séries numéricas e textuais\n",
    "* Colar especial utilizando valores, formatos, operações matemática e transposição.\n",
    "* Separação de texto para colunas\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Parte 2 - Fórmulas e funções do Excel <br>\n",
    "* Célula com constantes, Célula com fórmulas, Fórmulas aritméticas, Operadores aritmético e de comparação e Ordem de precedência.\n",
    "* Referências relativas e absolutas\n",
    "* Formatação condicional\n",
    "* Validação de dados\n",
    "* Função / Fórmula pré definidas e argumentos\n",
    "* Funções de texto (Concatenar, Esquerda, Direita, Núm.caract, Minúscula, Maiúscula, Pri.maiúscula e Rept)\n",
    "* Funções lógicas (E, OU e SE) e Operadores lógicos\n",
    "* Funções de pesquisa (PROCV, PROCH, Índice e Corresp)\n",
    "* Funções de datas e horas (Data, Datadif, Dia, Mês, Ano, Hoje, Dia.Da.Semana, Hora, Minuto, Segundo, Agora e Tempo) Dica: Segundos para formato tempo = Nr/86400\n",
    "* Funções matemáticas (Mod, Aleatório, Arred, Soma, INT, Aleatorioentre, LN, Somarproduto, Combin e Somase)\n",
    "* Funções estatística (Média, Correl, Cont.núm, Cont.valores, Máximo, Mínimo, Med, Modo, Desvpad.p/Desvpad.a, Var.a/Var.p, Cont.se e Quartil.inc)\n",
    "* Erros comuns\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Parte 3 - Tabelas Dinâmicas <br>\n",
    "* Tabelas de Frequência (Relativa: % do total de colunas e Relativa acumulada: % da soma acumulada em)\n",
    "* Medidas de Resumo \n",
    "* Colocar duas variáveis na linha: \n",
    "    * Aba design → Layout do relatório → Mostrar em formato de tabela \n",
    "    * Aba design → Subtotais → Não mostrar subtotais\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Parte 4 - Gráficos no Excel <br>\n",
    "* Formatação\n",
    "    * Eixo X\n",
    "    * Eixo Y \n",
    "    * Escala do Eixo X\n",
    "    * Escala do Eixo Y\n",
    "    * Eixo secundário de X\n",
    "    * Eixo secundário de Y\n",
    "    * Título do gráfico \n",
    "    * Área do gráfico \n",
    "    * Área de plotagem \n",
    "    * Legenda \n",
    "    * Séries de dados\n",
    "    * Rótulo de dados\n",
    "* Gráfico de Pizza e Rosca\n",
    "    * Tipo de uso: Composição do total de dados estáticos (dados não mudam com o tempo)\n",
    "* Gráfico de Barra e Coluna\n",
    "    * Tipo de uso: Comparação entre itens/categorias de variáveis\n",
    "* Gráfico de Barra empilhada e Coluna empilhada \n",
    "    * Tipo de uso: Composição absoluta que muda com o tempo\n",
    "* Gráfico de Barra 100% empilhadas e Coluna 100% empilhadas\n",
    "    * Tipo de uso: Composição relativa que muda com o tempo\n",
    "* Gráfico de Linha \n",
    "    * Tipo de uso: Comparação entre valores ao longo do tempo\n",
    "    * Melhor visualização: Inclusão de um subcategoria no eixo X para separar a data em dia e mês ou em mês e ano\n",
    "* Gráfico de Histograma\n",
    "    * Tipo de uso: Distribuição (frequência) de valores numéricos em faixas de valores iguais\n",
    "    * Difere dos gráficos de barra e coluna por que agrupa os valores do eixo x por faixa de valores/intervalos\n",
    "* Gráfico de Pareto \n",
    "    * Frequência absoluta com frequência relativa acumulada para saber qual é a concentração dos dados = 80% a 20%\n",
    "* Gráfico de Caixas Estreitas (Box Plot) \n",
    "    * Tipo de uso: Distribuição de valores numéricos em relação aos quartis e também usado para relação entre variáveis\n",
    "    * Analisa-se a amplitude, a concentração em quartis e os outliers\n",
    "* Gráfico de Dispersão (Scatter Plot)\n",
    "    * Tipo de uso: Distribuição conjunta de valores numéricos e também usado para relação entre duas variáveis\n",
    "    * Verifica a relação entre duas variáveis = Inclusão de linha de tendência, equação da reta, valor de R² e previsão da variável y em relação ao variável x\n",
    "* Gráfico de Cascata (Waterfall)\n",
    "    * Tipo de uso: Aumentos ou diminuição de um total\n",
    "    * Quando existe uma dependência entre o valor anterior e o seguinte. Os dados possuem subtotais/são saldos (São composições de valores) = Selecionar as categorias que são subtotais e alterar para \"Definir como total\" para alterar a escala do eixo X e deixa-la igual ao eixo Y. \n",
    "* Gráfico de Bolhas\n",
    "    * Tipo de uso: Relação entre três variáveis \n",
    "* Gráfico de Tree map (Gráfico Mapa de Árvore)\n",
    "    * Tipo de uso: Composição do total com comparação relativa entre itens\n",
    "    * O gráfico no Excel não pode ser feito diretamente de uma tabela dinamica.\n",
    "* Gráfico de Área empilhada\n",
    "    * Tipo de uso: Composição absoluta que muda com o tempo (com muitos períodos de datas)\n",
    "* Gráfico de Área 100% empilhada\n",
    "    * Tipo de uso: Composição relativa que muda com o tempo (com muitos períodos de datas)\n",
    "    \n",
    "<center><img width=\"60%\" src=\"_imagens/Tipos de gráficos.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Parte 5 - Construindo Dashboards no Excel <br>\n",
    "* Gráficos dinâmicos e segmentação de dados\n",
    "* Criação de dashboards interativos utiliza-se tabelas e gráficos dinâmicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054699b6",
   "metadata": {},
   "source": [
    "### <u>Como resumir e interpretar dados:</u><br>\n",
    "##### Parte 1 - Como fazer uma análise exploratória\n",
    "* Estatística: Estatística descritiva, Probabilidade e Inferência estatística. \n",
    "* Variáveis (Colunas da tbaela) e Observações (Linhas da tabela)\n",
    "* Tipos de variáveis: Qualitativa Nominal, Qualitativa Ordinal, Quantitativa Discreta e Quantitativa Contínua\n",
    "* Tabelas de frequências → \n",
    "    * Frequência absoluta é a contagem de vezes que o valor de uma variável ocorre em um conjunto de dados.\n",
    "    * Frequência relativa mostra a quantidade de vezes que um valor aparece no conjunto dos dados em relação ao total de observações. (40% das pessoas têm o ensino médio)\n",
    "    * Frequência relativa acumulada mostra a soma das frequências relativas até um determinado valor do conjutno de dados. (60% das pessoas têm escolaridade até o ensino médio) \n",
    "    * Faixas de valores ou intervalos e Amplitude de intervalo.\n",
    "* Medidas-Resumo são úteis para resumir o conjunto de dados através de medidas objetivas e realizar comparações entre diferentes conjuntos de dados ou grupos dentro do mesmo conjunto de dados. \n",
    "    * Medidas de Posição/Tendência Central: Média, Mínimo, Máximo, Moda, Mediana e Quartis.\n",
    "        * Média é influenciada por outliers (valores mariores)\n",
    "        * Mediana é influenciada pela quantidade de valores.\n",
    "        * Amplitude é a diferença do máximo e mínimo.\n",
    "        * Quartis indica que X % das observações tem valores inferiores a ele.\n",
    "    * Medidas de Dispersão/Variabilidade: Variância e Desvio Padrão\n",
    "        * Quanto maior for a diferença entre a média e cada valor, mais dispersas estarão as observações e por consequência, maior será o desvio. \n",
    "        * O quão dispersas estão as observações do conjunto de dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Parte 2 - Visualização dos dados (Uso de gráficos facilita ainda mais a interpretação do comportamento dos dados)\n",
    "* Gráfico de Pizza: Frequências relativas de variáveis qualitativas com poucas categorias.\n",
    "* Gráfico de Barras: Resumo de variáveis quantitativas (contagem, média, etc - Eixo Y) por categorias de variáveis qualitativas (Eixo X). \n",
    "* Gráfico de Linhas: Resumo de variáveis quantitativas (contagem, média, etc - Eixo Y) por categorias de variáveis qualitativas ordinais (Eixo X) - Indica continuidade\n",
    "* Gráfico de Pareto: Para resumir graficamente uma variável utilizando suas frequências relativas e acumuladas. Para demonstrar que para muitos eventos, aproximadamente 80 % dos efeitos (Eixo Y) vem de 20 % das causas (Eixo X). \n",
    "* Gráfico de Histograma: Resumo de variáveis quantitativas. A área da barra equivale a frequência relativa da faixa de valor.\n",
    "* Gráfico de Box Plot: A diferença interquartil (DQ) é a diferença entre o 1° quartil e o 3° quartil e fornece uma medida de variabilidade, pois indica como 50 % das observações estão dispersas (Tamanho da caixa), ou seja, quanto maior o valor da DQ, maiores serão os limites superior e inferior\n",
    "    * LS = Q3° + 1,5 x DQ\n",
    "    * LI = Q1° - 1,5 x DQ\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Parte 3 - Aplicações \n",
    "* Aplicação 1: Vendas de Produtos Bancários <br>\n",
    "    Contextualização: O Banco XPTOKW fez um levantamento de clientes que após tentativas de vendas de determinado produto financeiro, observaram quais aceitaram o produto e quais não aceitaram. Entretanto, para melhorar a eficiência de suas vendas eles te contrataram para realizar a análise destes dados e mostrar a eles algumas soluções para maximizar a eficiência em novas vendas.\n",
    "    Uma primeira reunião foi realizada contigo e um arquivo com os dados que eles levantaram foi te encaminhado, o arquivo se chama bank_marketing.xlsx \n",
    "    Com os dados em mãos, você iniciou sua análise exploratória para responder as questões levantadas durante a reunião:\n",
    "    * a) Qual o % de clientes que compraram o Título? \n",
    "    * b) Quais profissões com maiores frequências são responsáveis por cerca de 80% da base? (Quais das técnicas aprendidas melhor leva a resposta a esta pergunta?)\n",
    "    * c) Uma das frases ditas durante a reunião foi: “Podemos afirmar que 66% dos clientes não tem Ensino Superior”. Considerando os dados fornecidos, o que pode dizer sobre essa afirmação? \n",
    "\n",
    "<br>\n",
    "\n",
    "* Aplicação 2: IDHM dos municípios brasileiros - Parte I <br>\n",
    "    Contextualização: Um dos candidatos à presidência do Brasil solicitou a uma consultoria um levantamento de diversas informações sobre os municípios brasileiros. Com isso, pretende estudar algumas características e propor políticas adequadas para as áreas da Educação, Saúde e aumento de Renda. Considerando a base de dados brazil_cities.xlsx, extraia dos dados as seguintes informações para auxiliar o candidato: <br>\n",
    "    \n",
    "    1 - Panorama geral: <br>\n",
    "    O primeiro indicador a ser analisado é o índice de desenvolvimento humano (IDHM), uma medida composta de indicadores de três dimensões do desenvolvimento humano: longevidade, educação e renda. O índice varia de 0 a 1. Quanto mais próximo de 1, maior o desenvolvimento humano. \n",
    "    * a) Interessado a saber quais são os municípios com piores IDHM, você decidiu utilizar o conceito de quartis. Qual seria o valor que separaria os 25% dos municípios com IDHM mais baixos? \n",
    "    * b) Considerando esses municípios com menor IDHM, há uma concentração em algum estado ou distrito? Dica: utilize o conceito de Pareto. \n",
    "    * c) A ONU classifica os países seguindo o seguinte intervalo para o valor de IDH: \n",
    "        * < 0,550: Baixo \n",
    "        * 0,550 - 0,699: Médio \n",
    "        * 0,700 - 0,799: Alto \n",
    "        * = > 0,800: Muito Alto \n",
    "        \n",
    "    Replicando essa classificação ao IDHM dos municípios, qual seria a sua análise da situação atual do Brasil e cada região? <br>\n",
    "    \n",
    "    2 - Educação Infantil\n",
    "    * d) Construa um boxplot da variável IDHM_Educacao por região: NO, NE, SE, S e CO. Comente as diferenças e semelhanças entre os gráficos. \n",
    "    * e) Para a região com a menor média da variável IDHM_Educacao, vamos priorizar os municípios com mais crianças. Dessa forma, identifique os municípios que estão acima do 3º quartil nacional em relação a quantidade de crianças com idade entre 1 e 4 anos (variável IBGE_1-4). \n",
    "    * f) Identifique também os municípios que estão no 1º quartil nacional em relação a quantidade de empresas de educação (variável COMP_P). Dessa forma, vamos priorizar os municípios com poucas empresas do setor educacional. \n",
    "    * g) Considerando as análises anteriores, proponha uma ordem de priorização para investimento na educação dessa região. \n",
    "\n",
    "<br>\n",
    "\n",
    "##### Parte 4 - Estudo de caso: Perfil de Clientes da Campanha de Marketing \n",
    "* Contextualização: Estudo do Perfil dos Clientes\n",
    "    Com a finalidade de entender melhor o perfil dos clientes para potencializar a venda de um produto de investimento, a área de marketing criou uma campanha selecionando alguns clientes para realizar a oferta. A duração da campanha foi de 3 meses e abrangeu todo o Brasil. \n",
    "    Finalizada a campanha, o marketing disponibilizou a base de dados bank_marketing.xlsx e solicitou uma análise do perfil dos clientes à área de Analytics. A expectativa é que esse estudo forneça informações suficientes para que as próximas campanhas sejam direcionadas para o público mais propenso a comprar o produto de investimento.\n",
    "    Sua missão, como Analista de Dados, é responder a seguinte pergunta de negócios: Qual é o perfil dos nossos clientes? Para responder de forma adequada, você deve:\n",
    "    * Realizar uma Análise Exploratória buscando por insights sobre a venda dos títulos, o perfil dos clientes: Idade, Profissão, Estado Civil, Formação, Situação de Crédito e sobre a utilização de outros produtos: Hipoteca e Empréstimo.\n",
    "    * Construir um relatório que embase com dados os insights encontrados, e traduza termos mais técnicos para a linguagem de negócios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3507280",
   "metadata": {},
   "source": [
    "### <u>Análise de Correlação e Associação:</u><br>\n",
    "#### Analisando a correlação e associações entre variáveis (Analisar o comportamento de 2 variáveis simultaneamente): \n",
    "##### Aula 1 - Medida de Correlação Pearson (Estatística descritiva: Análise bidimensional) <br>\n",
    "* A análise de duas variáveis quantitativas inicia-se com o gráfico de dispersão. Nele uma das variáveis fica no eixo x e a outra no eixo y.\n",
    "* Quando existi uma correlação entre variáveis é possivel desenhar uma reta linear em que a maioria dos pontos ficam próximos a ela. \n",
    "* A existência de correlação entre variáveis indica que elas estão de alguma forma associadas, mas nem sempre isso que dizer que uma variável causa a outra.\n",
    "* Nem toda correlação é causalidade, mas toda causalidade gera uma correlação.\n",
    "* Uma forma de medirmos a força da correlação entre variáveis quantitativas é calculando o coeficiente de Correlação de Pearson, que varia entre -1 e +1 e indica:\n",
    "    * Correlação positiva forte: Coeficiente próximo a 1. (Quando a variável X aumenta, a variável Y aumenta)\n",
    "    * Correlação inexistente ou neutra: Coeficiente próximo a 0. (Quando a variável X aumenta, a variável Y não se altera)\n",
    "    * Correlação negativa forte: Coeficiente próximo a -1. (Quando a variável X aumenta, a variável Y diminui)\n",
    "* Fórmula Excel: CORREL(X;Y), sendo: -1,00 < r < -0,7 (Fortemente negativa), -0,6 < r < 0,6 (Fraca) e 0,7 < r < 1,0 (Fortemente positiva)\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Aula 2 - Medida de Information Value (valor da informação): <br>\n",
    "* A medida verifica a correlação de duas variáveis qualitativas. É responsável por fornecer o \"poder de separação\" que uma variável qualitativa de 2 ou mais categorias possui sobre outra variável de 2 categorias (variável binária). Exemplo:\n",
    "| 1° VarQual | Freq absoluta | Freq relativa | Qtde 1°Categ 2° VarQual |  Qtde 2°Categ 2° VarQual | % 1°Categ 2° VarQual | % 2°Categ 2° VarQual | % Taxa 1°Categ 2° VarQual | ODDS | LN(ODDS) | IV | \n",
    "| ------- | -------- | -------- | ------- | -------- | -------- | ------- | -------- | -------- | ------- | -------- |\n",
    "| Categoria 1 | 123 | 5% | 7 | 116 | 1,3% | 5,6% | 5,7% | 0,2 | -1,441 | 0,06 |\n",
    "| Categoria 2 | 800 | 31% | 87 | 713 | 16,5% | 34,5% | 10,9% | 0,5 | -0,737 | 0,13 |\n",
    "| Categoria 3 | 557 | 21% | 107 | 450 | 20,3% | 21,8% | 19,2% | 0,9 | -0,070 | 0,00 |\n",
    "| Categoria Z | 1.114 | 43% | 326 | 788 | 61,9% | 38,1% | 29,3% | 1,6 | 0,484 | 0,11 | \n",
    "| Total | 2.594 | 100% | 527 | 2067 | 100% | 100% | 20,3% | | | 0,31 |\n",
    "    \n",
    "    * % 1°Categ 2° VarQual = Qtde 1°Categ 2° VarQual / Pelo seu total, ou seja, 7 / 527 = 1,3%\n",
    "    * % 2°Categ 2° VarQual = Qtde 2°Categ 2° VarQual / Pelo seu total, ou seja, 116 / 2067 = 5,6%\n",
    "    * % Taxa 1°Categ 2° VarQual = Qtde 1°Categ 2° VarQual / Freq absoluta, ou seja, 7 / 123 = 5,7%\n",
    "    * ODDS = % 1°Categ 2° VarQual / % 2°Categ 2° VarQual, ou seja, 1,3 / 5,6 = 0,2 (0,2 é a chance de encontrar a 1°Categ 2° VarQual, ou seja, tem mais 2°Categ 2° VarQual)\n",
    "    * LN(ODDS) = Logarítimo da Odds, ou seja, LN(0,2) = -1,441\n",
    "    * IV = Produto da diferença das % 1°Categ 2° VarQual com 2°Categ 2° VarQual pelo LN(ODDS), ou seja, (1,3% - 5,6%) * -1,44 = 0,06\n",
    "    * Total do IV foi de 0,31, ou seja, forte. Então existe uma correlação entre as variáveis, logo pode ter uma boa estimativa que a categoria 1 será a 1°Categ 2° VarQual ou 2°Categ 2° VarQual\n",
    "    * Outras análises: \n",
    "        20,3% de 2594 é uma % pequena em relação ao total (Considerada uma % \"ruim\"), sendo que a categoria 1 (5,7%), 2 (10,9%) e 3 (19,2%) estão abaixo de 20,3%.\n",
    "        36% (Categoria 1 e 2) dos clientes não performaram bem, tendo uma taxa de 5,7% e 10,9% de 20,3%\n",
    "* Observações:    \n",
    "    * ODDS = Quando divide a 1° categoria (numerador) com 2° categoria (denominador) e o resultado é menor que 1 (<1), significa que o numerador é menor que o denominador e a chance de encontrar 2° categoria é maior que a 1°. Se o resultado for maior que 1, significa que o numerado é maior que o denominador e a chance de encontrar a 1° categoria é maior que a 2°.  \n",
    "    * IV = Após calcular o total do IV de todas as categorias, analisamos se a variável 1 possui um alto poder de separação com relação a variável 2\n",
    "        * IV Total for < 0,02 = Poder de separação é muito fraco (Existe uma associação muito fraca entre a variável 1 e as categorias da variável 2)\n",
    "        * IV Total for 0,02 a 0,1 = Poder de separação é fraco\n",
    "        * IV Total for 0,1 a 0,3 = Poder de separação é médio\n",
    "        * IV Total for 0,3 a 0,5 = Poder de separação é forte\n",
    "        * IV Total for > 0,05 = Poder de separação é muito forte (Existe uma associação muito forte entre a variável 1 e as categorias da variável 2)   \n",
    "###### O grau de separabilidade (IV) entre as variáveis, seria o quanto a variável que esta analisando (1° VarQual) consegue diferenciar a massa de dados em duas possibilidades (1°Categ 2° VarQual e 2°Categ 2° VarQual). <br>\n",
    "\n",
    "##### Aula 3 - Medida de R²: Coeficiente de Determinação <br>\n",
    "* A medida verifica a correlação de 1 variável qualitativa e 1 variável quantitativa. \n",
    "* Uma forma de analisarmos os dados é criar uma tabela com as medidas resumo separadas pela variável qualitativa. Podemos analisar também utilizando o gráfico box plot\n",
    "    * R² = 1 - Variância Poderada / Variância Total\n",
    "    * Se os dois box plot estiverem próximos (Limites inferior e superior quase iguais) significa que R² está próximo de 0.\n",
    "* O R² mede quanto da variância total é explicada pela introdução da variável qualitativa e é medida que varia entre 0 e 1. Dessa forma:\n",
    "    * R² igual a 0 = Indica a inexistência de associação entre as variáveis\n",
    "    * R² igual a 1 = Indica forte asssociação entre as variáveis, ou seja, a variável qualitativa explica X % da diferença da variável quantitativa dos clientes\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Aula 4 - Correlação vs. Causalidade <br>\n",
    "* Correlação: Relação de dependência, associação ou separação entre duas variáveis.\n",
    "* Causalidade: Relação entre um evento A e um evento B, sendo que o evento B é consequência do evento A. \n",
    "    * Por que a variável A causa a variável B?\n",
    "    * Como a variável A causa a variável B? \n",
    "* A correlação está relacionada com a dependência ou associação e a causalidade relacionada a consequência.\n",
    "* Correlação espúria é quando existe uma correlação, mas não existe uma causalidade. Provavelmente, existe uma terceira variável para explicar e não foi observada.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Aplicações: <br>\n",
    "* Aplicação 2: IDHM dos municípios brasileiros - Parte II<br>\n",
    "    Contextualização: Um dos candidatos à presidência do Brasil solicitou a uma consultoria um levantamento de diversas informações sobre os municípios brasileiros. Com isso, pretende estudar algumas características e propor políticas adequadas para as áreas da Educação, Saúde e aumento de Renda. Considerando a base de dados brazil_cities.xlsx, extraia dos dados as seguintes informações para auxiliar o candidato: <br>\n",
    "    * h) A renda anual per capita (por pessoa) pode ser analisada pela variável GDP_Capita. O atual presidente, concorrente do candidato que  você está auxiliando, disse em entrevista que “a maioria dos brasileiros tem uma renda anual superior a 21 mil”. Como você avalia essa frase? \n",
    "    * i) O IDHM_Renda é outra variável na qual podemos avaliar a situação da renda dos habitantes de cada município. Ela tem relação com o GDP_Capita? \n",
    "    * j) Considerando ainda o IDHM_Renda, qual a análise dessa variável por região? \n",
    "    * k) Conclua traçando o perfil de renda per capita dos municípios do Brasil e quais propostas faria para essa área. \n",
    "    \n",
    "<br>\n",
    "\n",
    "* Aplicação 3: Marketing de Produto Bancário<br>\n",
    "    Contextualização: Após a primeira reunião com o Banco XPTOKW (Aplicação 1), o próximo passo é identificar como direcionar a oferta de seu produto através de marketing em redes sociais e abordagem dos clientes por parte da equipe comercial. Utilize novamente as informações contidas no arquivo bank_marketing.xlsx para tomar suas decisões.\n",
    "    * a) Para o marketing em redes sociais, a plataforma permite a seleção do público baseado em até 2 variáveis dentre as seguintes: idade, profissão, estado civil e escolaridade. Quais variáveis você escolheria com o objetivo de separar as pessoas que compram ou não o produto. (Dica: utilize o IV  para as variáveis acima e avalie os resultados)\n",
    "    * b) Sobre a linha de comunicação, foi sugerido direcionar ao público com idade entre 30-40, pois, dos 4.778 clientes que compraram o produto, essa foi a faixa com maior quantidade. Você concorda com essa análise? Qual seria a sua sugestão?\n",
    "    * c) Por fim, para a equipe comercial, qual seria a sua sugestão de priorização de clientes, considerando as variáveis abaixo?\n",
    "        - Tem ou não hipoteca.\n",
    "        - Tem ou não empréstimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4d38f",
   "metadata": {},
   "source": [
    "#### Uma dúvida como saber e diferenciar quando eu uso a análise exploratória Uni e quando uso a exploratória Bidimensional, ou seja, como saber quando eu opto por usar uma ou outra? \n",
    "#### Normalmente fazemos a análise unidimensional para entender o comportamento das nossas variáveis e complementamos com a análise bidimensional para identificar quais variávies estão relacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f648a",
   "metadata": {},
   "source": [
    "### **Parte B: Análise Preditiva e Probabilidades** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c87f5",
   "metadata": {},
   "source": [
    "### <u>Prevendo o futuro: Probabilidade Clássica e Frequentista</u><br>\n",
    "#### Aula 1 - Introdução a Probabilidade <br>\n",
    "* Fenômenos determinísticos: Quando temos apenas um resultado possível como consequência de uma ação. \n",
    "* Fenômenos aleatórios: Quando temos mais de um resultado possível como consequência de uma ação.\n",
    "    * São muito recorrentes na natureza. Logo, estudar técnicas que \"quantificam\" sua probabilidade de ocorrência é fundamental.\n",
    "    * A teoria matemática utilizada para se estudar a incerteza oriunda de fenômenos de caráter aleatório é denominada probabilidade.\n",
    "    * As 3 principais abordagens para estudar probabilidade: Teoria clássica, teoria frequestista e teoria axiomática.\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### Aula 2 - Teoria Clássica <br>\n",
    "* O conjunto de todos os possíveis resultados de um fenômeno de caracter aleatório é chamado de Espaço Amostral.\n",
    "* Eventos são qualquer subconjuntos do espaço amostral de um fenômeno de caráter aleatório.\n",
    "* Em qualquer teoria de probabilidades, deseja-se atribuir um valor a um evento do espaço amostral.\n",
    "* Na teroria clássica, consideramos que todos os elementos/eventos têm a mesma chance de ocorrência.\n",
    "    \n",
    "        P(Evento de interesse) = Tamanho do evento de interesse / Tamanho do espaço amostral (números de casos favoraveis / números de casos possíveis).\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 3 - Teoria Frequentista <br>\n",
    "* A teoria frequentista basea-se em um número suficiente de experimentos e anotar a frequência de ocorrência do evento de interesse.\n",
    "* A probabilidade empírica, ou seja, a frequencia relativa da variável X é de cerca Y%.\n",
    "* O ponto central da teoria frequentista é que a probabilidade do evento de interesse é confiável ao se obter um número suficiente de experimentos, ou seja, em termos formais, \n",
    "    \n",
    "        P(Evento de interesse) = Número do evento de interesse / Número n de experimentos (Ex: Número de compras/Número n de pessoas na loja)\n",
    "        Sendo considerado a probabilidade \"boa\" quando n for grande o suficiente para percebermos uma estabilidade na estimativa \n",
    "* Muitas vezes, comete-se erros de assumir como verdade uma probabilidade sem antes verificar o experimento realizado para chegar a essa estimativa. Só podemos ter mais certeza do número quando temos uma quantidade N de experimentos suficientes (Grande)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 4 - Teoria Axiomática <br>\n",
    "* Considera o Diagram de Venn dos eventos para realizar o cálculo das probabilidades.\n",
    "* Embora a teoria frequentista já resolva grande parte dos problemas para se estimar a probabilidade de um evento, pode-se argumentar que não é possível garantir que essa probabilidade seja a mesma em uma segunda ou terceira repetição do experimento.\n",
    "* 4 axiomas (premissas incontestáveis):\n",
    "    * 0 <= P(Evento) <= 1 → A probabilidade de um evento é um número entre 0 e 1.\n",
    "    * P(Espaço Amostral) = 1 → A probabilidade de ocorrência de pelo menos um dos eventos do espaço amostral é igual a 1.\n",
    "    * P(A∪B) = P(A) + P(B) → A probabilidade da união dos eventos A e B é igual a soma das probabilidades de A e B, se e somente se A e B não tiverem chance de ocorrer simultaneamente.\n",
    "    * P(A∪B) = P(A) + P(B) - P(A∩B) → A probabilidade da união dos eventos A e B é igual a soma das probabilidades de A e B menos a probabilidade de ocorrência de A e B simultaneamente (Interseção).\n",
    "* Regra de multiplicação de probabilidades: P(Evento A∩B) = P(A) * P(B) → A e B são independentes. Quando a ocorrência de A não depende da ocorrência de B\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 5 - Probabilidades <br>\n",
    "##### Não entender a diferença dos conceitos: Evento possível (Menor probabilidade) x Evento provável (Maior probabilidade) <br>\n",
    "##### Quando participamos de um sorteio de rifa, por exemplo, a gente compra mais números para ter uma probabilidade maior de ganhar, mesmo sabendo que só um número vai ser o vencedor.\n",
    "\n",
    "#### Aplicações: <br>\n",
    "* Aplicação 1: Condições de Vida da População Brasileira <br>\n",
    "    Contextualização: O IBGE divulgou em 2019 em seu livro Síntese de Indicadores Sociais: Uma Análise das Condições de Vida da População Brasileira – 2019(1), os dados e informações relativas as condições de vida dos brasileiros. Um dos temas tratados foi o sobre a população brasileira com alguma ocupação com 14 anos de idade ou mais em algum trabalho principal. A tabela abaixo foi retirada da página 19 deste livro.\n",
    "\n",
    "<center><img width=\"40%\" src=\"_imagens/Tabela retirada da página 19 do livro  Síntese de Indicadores Sociais.png\"></center>\n",
    "\n",
    "Você foi convidado a realizar algumas análises envolvendo probabilidades com relação aos dados desta tabela. Considerando apenas o ano de 2018:\n",
    "* a) Qual é o espaço amostral da população ocupada de 14 anos ou mais de idade?\n",
    "* b) Qual são os dois principais eventos destacados em negrito na tabela?\n",
    "* c) Qual a probabilidade de cada um dos dois principais eventos ocorrerem, separadamente, em relação ao espaço amostral?\n",
    "* d) Qual a probabilidade de o evento “Conta própria não contribuinte” ocorrer em relação ao espaço amostral?\n",
    "* e) Qual a probabilidade de o evento “Empregador contribuinte” ocorrer em relação:\n",
    "    * Ao espaço amostral?\n",
    "    * Ao grupo das pessoas ocupadas com 14 anos ou mais de idade que possuem um trabalho formal?\n",
    "* f) Suponha que você veja a frase: “38,3 milhões de brasileiros tinham trabalhos informais em 2018, correspondendo a 71% dos trabalhadores formalizados”. Esta frase lhe causa algum incômodo? Por quê?\n",
    "\n",
    "<br>\n",
    "\n",
    "* Aplicação 2: Probabilidades na Megasena <br>\n",
    "    Contextualização: Você foi contratado para realizar alguns estudo preliminares com relação aos jogos da Mega Sena. A principal questão aqui é levantar qual a probabilidade de algum apostador ter levado o prêmio nos jogos apresentados em sorteios_megasena.xlsx. Assim:\n",
    "    * a) Preencha a coluna “Probabilidade de Algum Ganhador” com a probabilidade de ao menos um jogador ganhar o concurso.\n",
    "    Dica: o total de possibilidade de jogos com 6 números na Mega Sena é dado pela fórmula: COMBIN(60;6), resultando num total de 50.063.860 jogos possíveis.\n",
    "    * b) Preencha a coluna “Algum Ganhador?” com 1, caso tenha havido um ganhador no concurso, e 0 caso contrário.\n",
    "    * c) Vá na aba “Análises” e atualize a tabela de frequência. Calcule a probabilidade de algum ganhador utilizando as quantidades de concursos e quantidade de concursos com algum ganhador.\n",
    "    * d) Compare a probabilidade calculada com a faixa de probabilidade na coluna A. Analise se está condizente e argumente o motivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311eb7f",
   "metadata": {},
   "source": [
    "### <u>Prevendo o futuro: Modelos de Probabilidade, Uniforme e Curva Normal</u><br>\n",
    "#### Aula 1 - Introdução às Variáveis Aleatórias <br>\n",
    "* Em estatística descritiva, as tabelas de frequência são utéis para resumir uma variável de um conjunto de dado. \n",
    "    * Frequência absoluta\n",
    "    * Frequência relativa\n",
    "    * Frequência relativa acumualda\n",
    "* Variáveis cuja probabilidade de ocorrência tem caráter aleatório são chamadas de variáveis aleatórias. Existem 2 tipos: Variáveis aleatórias discretas ou qualitativas e Variáveis aleatórias contínuas\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 2 - Variáveis Aleatórias Discretas ou Qualitativas<br>\n",
    "* Método: Tabela de frequência com a distribuição de probabilidades/Frequência relativa e Histograma\n",
    "* Verificar como os 100% dos dados estão distribuidos em todos os possíveis resultados da variável.\n",
    "* A função que atribui uma probabilidade a cada valor da variável aleatória é chamada de função de probabilidade. Notação: P(X=xi) = p(xi) = pi\n",
    "| X | x1 | x2 | x3 |\n",
    "| -- | -- | -- | -- |\n",
    "| Pi | p1 | p2 | p3 |\n",
    "* Se for sorteado aleatoriamente um elemento dentro de N elementos, qual é a probabilidade de encontrarmos o valor X.\n",
    "    * Frequência absoluta = Valor x \n",
    "    * Frequência relativa = Probabilidade = Probabilidade de encontrar o valor x\n",
    "    * Frequência relativa acumualda = Probabilidade acumulada = Probabilidade acumulada de encontrar até o valor x\n",
    "* A função que acumula as probabilidades até um dado valor da variável aleatória é chamada Função de Distribuição de Probabilidade. Notação: F(x) = P(X <= x)\n",
    "* Dois eventos A e B são complementares se: P(A∪B) = 100% e P(A∩B) = 0%\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 3 - Distribuição de Probabilidades Uniforme <br>\n",
    "* Quando os resultados de um fenômeno de caráter aleatório possuem a mesma probabilidade de ocorrência.\n",
    "* Uma variável aleatória X segue a distribuição uniforme se a sua função de probabilidade atribuir a mesma probabilidade 1/k para um dos k valores da variável, ou seja, sua função é dada por:\n",
    "        \n",
    "        P(X = xi) = 1/k, sendo i = 1,2,3,...,k ou P(X = xi) = 1/(b - a)\n",
    "        Notação: X~U(a,b), sendo a e b o menor e o maior valor que a variável pode assumir respectivamente\n",
    "        \n",
    "* Excel: \n",
    "    * ALEATÓRIO(): Gera um número entre 0 e 1\n",
    "    * ALEATÓRIOENTRE(a;b): Gera um número inteiro entre a e b\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 4 - Distribuição de Probabilidades Binomial <br>\n",
    "* Uma variável aleatória X segue a distribuição binominal se o fenômeno que se deseja descrever é o número de sucessos em uma sequência de tentativas. Cada tentativa possui apenas 2 resultados possíveis: Sucesso ou Fracasso, as tentativas são independentes e a probabilidade de sucesso se mantém constante em todas as tentativas.\n",
    "        \n",
    "        P(X=k) = C(n, k) * p^k * (1-p)^(n-k), sendo 0 <= k <= n\n",
    "        → P(X=k) representa a probabilidade de obter exatamente k sucessos em n experimentos.\n",
    "        → C(n, k) é o coeficiente binomial, que representa o número de combinações de n elementos tomados k a k.\n",
    "        → p é a probabilidade de sucesso em cada experimento.\n",
    "        → (1-p) é a probabilidade de fracasso em cada experimento.\n",
    "        → n é o número total de experimentos/número de tentativas.\n",
    "        → k é o número de sucessos\n",
    "        Notação: X~Bin(n,p)\n",
    "        \n",
    "* Excel: \n",
    "    * DISTR.BINOM(k;n;p;FALSO): Calcula a probabilidade para X = k\n",
    "    * DISTR.BINOM(k;n;p;VERDADEIRO): Calcula a probabilidade para X <= k (probabilidade acumulada)\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### Aula 5 - Distribuição de Probabilidades Poisson <br>\n",
    "* Uma variável aleatória X segue a distribuição poisson com parâmetro λ > 0 se o fenômeno que se deseja descrever é a quantidade de ocorrências de um evento em um intervalo de tempo determinado. \n",
    "\n",
    "        P(X = k) = (e^(-λ) * λ^k) / k!, sendo k = 0,1,2,...\n",
    "        → P(X = k) é a probabilidade de ocorrerem exatamente k eventos raros no intervalo de tempo ou espaço;\n",
    "        → e é a base do logaritmo natural, aproximadamente igual a 2.71828;\n",
    "        → λ é o parâmetro médio da distribuição, que representa a média de ocorrência dos eventos raros no intervalo de tempo ou espaço (Taxa de ocorrçencia);\n",
    "        → k é o número de eventos raros que se deseja calcular a probabilidade;\n",
    "        → k! é o fatorial de k, ou seja, o produto de todos os números inteiros de 1 a k.\n",
    "        Notação: X~Po(λ)\n",
    "        \n",
    "* Excel: \n",
    "    * DISTR.POISSON(k;λ;FALSO): Calcula a probabilidade para X = k\n",
    "    * DISTR.POISSON(k;λ;VERDADEIRO): Calcula a probabilidade para X <= k (probabilidade acumulada)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 6 - Variáveis Aleatórias Contínuas <br>\n",
    "* Quando analisamos uma variável aleatória contínua, a tabela de frequencia não é o método mais adequado, pois ela não permite obter as probabilidades em intervalos diferentes daqueles existentes na tabela.\n",
    "* Método: Funções de probabilidades contínuas e Histograma\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 7 - Distribuição de Probabilidades Normal <br>\n",
    "* Uma variável aleatória contínua X segue a distribuição normal com parâmetros 𝜇 e 𝜎² se sua função densidade de probabilidade é dada por: \n",
    "\n",
    "        f(x) = (1 / (σ * √(2π))) * (e ^ -(x - μ)^2 / (2σ^2)), sendo:\n",
    "        → x é o valor que se deseja encontrar a probabilidade\n",
    "        → 𝜇 é a média da distribuição\n",
    "        → 𝜎 é o desvio padrão da distribuição\n",
    "        → π é uma constante aproximada igual a 3,14159\n",
    "        → e é uma constante aproximada igual a 2,71828\n",
    "        → 𝜎² / σ^2 é a variância da distribuição\n",
    "        Notação: X~N(𝜇,𝜎²)\n",
    "        \n",
    "* 1° passo: Definir se é uma distribuição de probabilidade normal (Ao analisar o histograma, deparamos com uma distribuição simétrica, concentração de valores na posição central, densidade tende a zero para valores muitos baixos ou valores muito altos e o formato de \"sino\") \n",
    "* 2° passo: Calcular as medidas resumo média e desvio padrão\n",
    "* 3° passo: Calcular a função densidade de probabilidade normal\n",
    "* Excel: \n",
    "    * Esquerda: DIST.NORM.N(k;Média;Desvio;FALSO): Calcula a densidade de probabilidade para X = k\n",
    "    * Esquerda: DIST.NORM.N(k;Média;Desvio;VERDADEIRO): Calcula a probabilidade para X <= k (probabilidade acumulada)\n",
    "    * Direita: 1 - DIST.NORM.N(k;Média;Desvio;VERDADEIRO)\n",
    "* Propriedades da função de densidade normal são:\n",
    "    1. O valor máximo de f(x) é o ponto x = 𝜇\n",
    "    2. f(x) tende a 0 quando x tende ao mais infinito e ao menos infinito\n",
    "    3. f(x) é simétrica em relação a 𝜇, quando 50% dos valores de x ficam a esquerda e a direita de 𝜇 (Média = Mediana)\n",
    "\n",
    "#### OBS: Verdadeiro = 1 (função de distribuição cumulativa) e Falso = 0 (função de densidade de probabilidade)\n",
    "\n",
    "<br>\n",
    "    \n",
    "#### Aula 8 - Outras distribuições de probabilidades <br>\n",
    "<center><img width=\"40%\" src=\"_imagens/Outras distribuições de probabilidades.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aplicações: \n",
    "* Aplicação 3: Envio de e-mails promocionais<br>\n",
    "    Contextualização: Uma empresa mandou e-mails promocionais para sua carteira composta de 2.000 clientes. Como nem todos os clientes leem os informativos, a empresa continuou o envio de e-mails até que sua carteira de clientes inteira fosse impactada. Os resultados dessa ação seguem na tabela abaixo:\n",
    "    \n",
    "    <center><img width=\"40%\" src=\"_imagens/Exercicio - Aplicação 3 de Probabilidade.png\"></center>\n",
    "    \n",
    "    * a) Qual a probabilidade de um destes clientes ser sorteado ao acaso ter feito a primeira leitura no primeiro e-mail recebido?\n",
    "    * b) Qual a probabilidade de um destes clientes ser sorteado ao acaso ter recebido mais do que 3 e-mails até a primeira leitura?\n",
    "    * c) Crie na tabela uma linha com a quantidade de e-mails enviados total (qtd e-mails enviados * qtd de clientes).\n",
    "    * d) Analisando os itens anteriores, o que você achou dessa ação da empresa? Faria alguma modificação na estratégia?\n",
    "\n",
    "<br>\n",
    "\n",
    "* Aplicação 4: Checando a Megasena<br>\n",
    "    Contextualização: Segundo a organização da Megasena, todos os 60 números têm a mesma chance de ocorrência. Em linguagem estatística, o número Y (número sorteado da Megasena) é uma variável aleatória que segue a distribuição Uniforme. Ou seja, Y~U(1;60). Para verificar se a organização fala a verdade, siga os passos abaixo:\n",
    "    * a) Abra o arquivo sorteios_megasena.xlsx \n",
    "    * b) Preencha a seguinte tabela usando as informações do enunciado e do arquivo indicado no item a) <br>\n",
    "    Responda:\n",
    "    * A. O modelo Uniforme é adequado para o conjunto de dados da Megasena? Justifique.\n",
    "    * B. Você apostaria na sequência 01, 02, 03, 04, 05 e 06? Justifique.\n",
    "    \n",
    "<br>\n",
    "\n",
    "* Aplicação 5: Avaliação de Idades<br>\n",
    "    Contextualização: Uma empresa realizou uma avaliação demográfica de seus funcionários, através de uma amostra de 500 pessoas. Uma das variáveis analisadas foi a idade, identificando que ela se aproxima a uma distribuição Normal com média 30 e variância 16. A maneira que podemos escrever isso em notação é: X~N(30;16), sendo X a idade dos funcionários. Outras notações importantes são: P(X≤i) - probabilidade da idade ser menor ou igual que i, e P(X≥i) - a probabilidade da idade ser maior ou igual que i. Dessa forma, responda:\n",
    "    * a) P(X ≥ 32) – Qual a probabilidade de um funcionário ter 32 anos ou mais?\n",
    "    * b) P(X ≤ 30) – Qual a probabilidade de um funcionário ter até a média de idade, ou seja, menor ou igual a 30 anos?\n",
    "    * c) P(28 ≤ X ≤ 32) – Qual a probabilidade de um funcionário ter entre 28 e 32 anos?\n",
    "    * d) P(20 ≤ X ≤ 24) – Qual a probabilidade de um funcionário ter entre 20 e 24 anos?\n",
    "    * e) P(36 ≤ X ≤ 40) – Qual a probabilidade de um funcionário ter entre 36 e 40 anos?\n",
    "    * f) Analisando os itens c) e d), responda: por que as probabilidades são diferentes apesar da amplitude do intervalo ser igual? Faça a mesma análise para o item d) e e): as probabilidades são iguais ou diferentes? Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc1ff7",
   "metadata": {},
   "source": [
    "### **Parte C: Inferência e Metodologia de Projetos** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc6b0d",
   "metadata": {},
   "source": [
    "### <u>Como trabalhar com amostras de dados</u><br>\n",
    "#### Aula 1 - Conceitos Fundamentais: População, Amostra e Viés <br>\n",
    "##### Algumas situações que não temos acesso a população de interesse: \n",
    "* Problemas econômicos: É muito custoso entrevistar todas as pessoas de interesse e nem todas as pessoas estão acessíveis.\n",
    "* Problemas físicos: Imagine o problema de se testar a durabilidade das lâmpadas. O teste poderia queimar todas elas.\n",
    "* Problemas éticos: Testes de remédios em seres vivos pode causar grandes efeitos colaterais. Testar em toda a população seria prudente? \n",
    "\n",
    "<br>\n",
    "\n",
    "##### Uma amostra com viés de seleção ou enviezada não representa uma população e pode ser considerada uma amostra por conveniência. Para representar bem uma população, a amostra precisa identificar corretamente os parâmetros de interesse, ter uma tamanho adequado e não ter erros de leitura ou coleta. <br>\n",
    "\n",
    "##### Amostragem aleatória simples (AAS) é uma técnica que sorteia aleatoriamente um conjunto de dados. Tem como principal benefício a garantia de obter amostras que tenham a mesma probabilidade de ocorrência. Com isso, é mais seguro extrapolar suas conclusões para a população de interesse. Temos basicamente dois tipos de AAS. São eles:\n",
    "* AAS sem reposição: Quando removemos a unidade sorteada para não ocorrer o risco de ser sorteada novamente na próxima coleta.\n",
    "* AAS com reposição: Quando não removemos a unidade sorteada. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 2 - Conceitos Fundamentais: Estimadores <br>\n",
    "* Usando os valores da amostra, escolhemos um estimador para o parâmetro de interesse. Através do estimador, calculamos uma estimativa para o parâmetro de interesse.\n",
    "* Um estimador é considerado bom quando não é viesado e tem uma menor variância possível.\n",
    "\n",
    "<center><img width=\"40%\" src=\"_imagens/Estimadores.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 3 - O importante Teorema do Limite Central <br>\n",
    "##### O princípio essencial do TLC é que uma amostra grande, adequadamente escolhida, será muito próxima da população que foi retirada. \n",
    "* Se você obter amostras grandes, aleatórias, de qualquer população, as médias dessas amostras serão distribuídas \"normalmente\" em torno da média da população.\n",
    "* A maioria das médias de amostras estará razoavelemente perto da média da população. O desvio padrão da população é o que define \"razoavelmente perto\".\n",
    "* Quanto menos provável for um resultado observado, mais confiantes podemos estar em presumir que a população de origem é bem diferente da amostra que estamos analisando. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 4 - Intervalos de Confiança: Estimando a Média da População <br>\n",
    "##### É uma estimativa de um intervalo utilizado na estatística, que contém um parâmetro populacional.\n",
    "* População → Censo → Parâmetro de interesse\n",
    "* Amostra → Estimação → Estimativa \n",
    "* Espera-se que a que estimativa seja uma boa aproximação do parâmetro de interesse\n",
    "\n",
    "<br>\n",
    "\n",
    "##### O intervalo de confiança é importante para indicar a margem de incerteza (ou imprecisão) frente a um cálculo efetuado. <br>\n",
    "##### O Intervalo de Confiança é uma faixa de valores que estimam um parâmetro populacional com um determinado grau de confiança. Quanto maior o coeficiente de confiança, maior será a amplitude desse intervalo, pois é necessário garantir uma maior probabilidade de que o parâmetro populacional esteja contido nesse intervalo. <br>\n",
    "##### O cálculo de um intervalo de confiança é uma estratégia que considera a amostragem de erro. <br>\n",
    "##### Para quantificarmos com maior precisão o erro na estimação da média populacional, utilizamos os intervalos de confiança, por exemplo. <br>\n",
    "##### Quanto mais estreito for o intervalo de confiança, maior é a probabilidade da porcentagem da população de estudo representar o número real da população de origem dando maior certeza quanto ao resultado do objeto de estudo. <br>\n",
    "##### Cálculo: \n",
    "* Reúna os dados da amostra: n;\n",
    "* Calcule a média da amostra x̅;\n",
    "* Determine se um desvio padrão populacional (σ) é conhecido ou desconhecido;\n",
    "* Se um desvio padrão populacional for desconhecido, podemos usar uma estatística t para o nível de confiança correspondente;\n",
    "* Assim, encontram-se os limites inferior e superior do intervalo de confiança usando as seguintes fórmulas:\n",
    "* Desvio padrão de uma população desconhecida:    \n",
    "\n",
    "<center><img width=\"20%\" src=\"_imagens/Intervalo de confiança - Desvio padrão desconhecido.png\"></center>\n",
    "\n",
    "##### Excel: \n",
    "* Média: MÉDIA(Célula:Célula)\n",
    "* Desvio padrão: RAIZ(VAR.A(Célula:Célula))\n",
    "* T-student: ABS(INV.T(Prob; n-1))\n",
    "    * Prob = (1 - Nível de confiança %)/2\n",
    "\n",
    " <br>\n",
    " \n",
    "##### Interpretação:    \n",
    "* É 95% confiante de que o intervalo entre X (limite inferior) e Y (limite superior) contém o verdadeiro valor do parâmetro populacional.\n",
    "* Se realizassemos um grande número de amostras aleatórias de tamanho x e calculassemos o intervalo de confiança para todas elas, X% desses intervalos conteriam a real média da população (parâmetro 𝜇)\n",
    "\n",
    " <br>\n",
    " \n",
    "#### Aula 5 - Intervalos de Confiança: Estimando a Proporção da População <br>\n",
    "Em problemas relacionados com percentual ou proporção populacional, procedemos da mesma forma, exceto pelas seguintes mudanças;\n",
    "\n",
    "<center><img width=\"20%\" src=\"_imagens/Proporção da População.png\"></center>\n",
    "\n",
    "* Reúna os dados da amostra: n;\n",
    "* Calcule a proporção populacional p (Média amostral);\n",
    "* Calcule a variância da proporção amostral\n",
    "* T-student: ABS(INV.T(Prob; n-1))\n",
    "        \n",
    "        Proporção amostral (p) = Frequência do evento de interesse / número total de eventos (Média amostral)\n",
    "        Variância da proporção amostral (σp²) = p(1 - p) ou (σp²) = x̅(1 - x̅)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 6 - Calculando a Margem de Erro e o Tamanho da Amostra <br>\n",
    "* Margem de erro na estimação do parâmetro 𝜇 com nível de confiança de X% é de N°, pois essa é a distância entre a média amostral/proporção amostral (Estimativa pontual) e os limites do intervalo de confiança. \n",
    "* Intervalo de confiança = [Limite inferior, Limite superior] = [𝜇 - Erro, 𝜇 + Erro]\n",
    "* Erro = T * RAIZ(VAR.A/N), então se aumenta T (Nível de confiança), aumenta a margem de erro e se aumenta N (tamanho da amostra), diminui a margem de erro\n",
    "* Tamanho da amostra (n) = (S² * Z²) / E²\n",
    "    * S² = Variância = VAR.A(Célula:Célula)\n",
    "    * Z² = Nível de confiança = (INV.NORMP.N((1 - Nível de confinaça)/2)) ^ 2\n",
    "    * E² = Margem de erro desejada pelo usuário ^ 2\n",
    "    * Interpretação: Para obter uma margem de erro inferior de X precisamos ter uma amostra com pelo menos X dados.\n",
    "* E = √((S² * Z²) / n)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aplicações: <br>\n",
    "* Aplicação 1: Validação da qualidade das amostras <br>\n",
    "    Contextualização: Discuta a validade da amostragem nos casos a seguir:\n",
    "    * a) Para avaliar a eficácia de uma campanha de vacinação no Estado do Rio de Janeiro, 150 mães de recém-nascidos de uma maternidade de Copacabana foram entrevistadas a respeito da última vez em que vacinaram seus filhos.\n",
    "    * b) Para verificar a audiência de um programa de TV, a emissora contabilizou a quantidade de pessoais que tuitaram a hashtag #ProgramaDoGentilli.\n",
    "    * c) Uma amostra de sangue foi retirada de um paciente com suspeita de uma doença.\n",
    "    * d) Para avaliar a intenção de voto para presidente dos brasileiros, 2.000 pessoas foram entrevistadas no Nordeste.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Aplicação 2: Adequação de automóveis aos suecos - Parte 1 <br>\n",
    "    Contextualização: Um fabricante de automóveis decidiu exportar o novo modelo produzido para a Suécia. Para verificar se os ajustes do banco do motorista e passageiro dianteiro desenvolvidos no Brasil são adequados à altura da população Sueca, solicitou a uma consultoria um estudo e a coleta de uma amostra da altura da população que lá vive. Com os dados da amostra coletada no arquivo altura_suecia.xlsx, calcule:\n",
    "    * a) Estimativa Pontual da altura média dos habitantes da Suécia.\n",
    "    * b) Intervalos de Confiança considerando o coeficiente de confiança igual a 90% e 99%.\n",
    "    * c) Por que o Intervalo de Confiança com coeficiente de confiança igual a 99% é maior do que o com coeficiente de confiança igual 90%?\n",
    "    * d) Esse novo modelo foi desenvolvido para a altura média do brasileiro, que segundo o IBGE, é de 1,73m. Nesse caso, será necessária uma adaptação do ajuste dos bancos para exportação?\n",
    "    \n",
    "<br>\n",
    "\n",
    "* Aplicação 3: Conversão de Vendas <br>\n",
    "    Contextualização: Ao analisar a conversão de vendas no site de sua empresa, o analista de dados verificou que houve 10 vendas em 50 visitas.\n",
    "    * a) Calcule a taxa de conversão (vendas/visitas) e a variância dessa taxa.\n",
    "    * b) Qual o intervalo de confiança considerando um coeficiente de confiança de 95%?\n",
    "    * c) Qual a margem de erro?\n",
    "    * d) Qual seria o tamanho da amostra caso quisermos uma margem de erro de 5p.p?\n",
    "\n",
    "<br>\n",
    "\n",
    "* Aplicação 4: Pesquisa em produtos fitness <br>\n",
    "    Contextualização: Uma empresa de produtos fitness gostaria de realizar uma pesquisa de interesse pela sua nova barra de cereal com cobertura de chocolate e zero calorias para estimar qual o percentual do público de academias compraria o produto. Para isso contratou uma consultoria que se comprometeu em calcular o tamanho da amostra necessária para que a margem de erro fosse de no máximo 5%.\n",
    "    * a) Calcule o tamanho da amostra considerando o coeficiente de confiança 95%.\n",
    "    * Dica: como se trata de um problema relacionado com percentual e não existem dados de uma amostra piloto, considere a variância amostral de 0,25.\n",
    "    * b) Devido a problemas operacionais, não foi possível realizar a pesquisa com o número de pessoas calculado anteriormente. A consultoria conseguiu entrevistar 300 pessoas e a variância amostral obtida foi de 0,22. Qual é a margem de erro com esse tamanho de amostra, considerando o coeficiente de confiança de 95%?\n",
    "\n",
    "<br>\n",
    "\n",
    "* Aplicação 5: Adequação de automóveis aos suecos - Parte 2 <br>\n",
    "    Contextualização: Preocupado com a estimativa da altura média dos Suecos, o fabricante de automóveis decidiu solicitar uma nova análise para ter mais acurácia na tomada de decisão de adaptar ou não o ajuste dos bancos para exportação de seu novo modelo. Para isso solicitou os seguintes cálculos (considere o mesmo arquivo altura_suecia.xlsx utilizado no exercício anterior e com os valores já calculados):\n",
    "    * a) A Margem de Erro para os dois Intervalos de Confiança calculados no exercício 2.\n",
    "    * b) O número de pessoas que devem ter sua altura medida para que a Margem de Erro seja inferior a 1 cm, considerando os coeficientes de confiança de 90% e 99%.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Estudo de Vendas e Frete em um Marketplace <br>\n",
    "    Contextualização: Uma empresa presente nos maiores marketplaces do Brasil resolveu fazer um estudo para avaliar o valor dos produtos vendidos e o valor do frete. Para isso, coletou informações de mais de 100 mil vendas com seus respectivos fretes. Utilizando o arquivo olist_order_items_dataset.xlsx, realize as seguintes análises:\n",
    "    * Estimativa Pontual e por Intervalo\n",
    "        * a) Calcule a estimativa pontual do preço médio dos produtos e do valor médio do frete.\n",
    "        * b) Calcule os intervalos de confiança para preço e frete considerando o coeficiente de confiança igual a 95%.\n",
    "        * c) Qual é a probabilidade de que esses intervalos de confiança contenham o verdadeiro valor da média populacional? Argumente.\n",
    "    * Margem de Erro e Tamanho da Amostra\n",
    "        * d) Calcule a margem de erro utilizando as estimativas pontais e por intervalo. Por que a margem de erro do frete é inferior a margem de erro do preço?\n",
    "        * e) Calcule o mínimo tamanho de amostra para que a margem de erro na estimativa do preço seja inferior a 0,10 e do frete inferior a 0,05. Considere o mesmo coeficiente de confiança de 95%.\n",
    "    * Percentual de pessoas com preço acima da média\n",
    "        * f) Crie uma variável na tabela em que o valor seja igual a 1 caso o preço seja maior do que a média, calculada no item a), e 0 caso contrário. Vamos chamar essa nova coluna de \"preço > média\".\n",
    "        * g) Calcule as estimativas pontuais e por intervalo do percentual de pessoas com compras acima da média. Considerando o valor da estimativa pontual para o percentual de pessoas com compras acima da média, é possível afirmar que o preço médio é maior ou menor do que o preço mediano? Argumente.\n",
    "        * h) Com esse tamanho de amostra é possível obter uma margem de erro inferior a 0,2%? Caso não seja, qual o tamanho da amostra para atingir essa margem de erro desejada?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf3bf2",
   "metadata": {},
   "source": [
    "#### O desvio-padrão demonstra a distância dos valores em relação à média do conjunto, quanto mais próximo de 0 for o desvio-padrão, menos dispersos são os dados daquele conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df67d4",
   "metadata": {},
   "source": [
    "### <u>SQL para Análise de Dados - Nível 1</u><br>\n",
    "#### Introdução ao SQL e configuração <br>\n",
    "##### O que é o SQL e por qual razão devemos usa-lo:\n",
    "* Para gerar conhecimento os dados devem ser: Processados, armazenados e analisados.\n",
    "* Banco de dados: Persistência, compartilhado, inter-relacionado, segurança e confiável.\n",
    "* SGBD: Necessidade de gerenciamento do banco de dados → MySQL, SQL Server, PostgreSQL e Oracle.\n",
    "* Banco de dados realiza a comunicação com sites, aplicativos e analises.\n",
    "* SQL é a linguagem que, através de comandos, manipulamos e consultamos nosso banco de dados.\n",
    "* Coleta de dados:\n",
    "    * Permite acessar dados de diferentes aplicações e sistemas\n",
    "    * Autonomia dos times de TI\n",
    "    * Flexibilidade para acessar diferentes formatos de dados\n",
    "* Entendimento dos dados:\n",
    "    * Realizar consultas\n",
    "    * Calcular medidas de resumo\n",
    "    * Tabelas de frequência\n",
    "* Tratamentos dos dados:\n",
    "    * Modificar tipos de variáveis\n",
    "    * Criar novas variáveis\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Instalando Dbeaver > https://dbeaver.io/download/ <br>\n",
    "##### Configurando seu ambiente para fazer consultas em SQL \n",
    "1. Databse\n",
    "2. New Database Connection\n",
    "3. Escolha o SGBD → Next → Fazer as configurações necessárias → Test Connection...\n",
    "    * Host: preditiva-postgres-1.cmaonsvyciyn.us-east-1.rds.amazonaws.com\n",
    "    * Port: 5432\n",
    "    * Database: alunos\n",
    "    * Username: aluno\n",
    "    * Password: PreditivaAlunoGVD\n",
    "4. Na tela inicial do DBeaver, selecione a conexão criada e clique em New SQL Editor\n",
    "5. Clique em SQL → New SQL script (Para executar a consulta: CRTL + ENTER)\n",
    "\n",
    "<br>\n",
    " \n",
    "#### Consultando dados <br>\n",
    "##### Antes de realizar uma consulta:\n",
    "* O que eu quero saber? \n",
    "* Onde está esta informação?\n",
    "* Verificar os metadados da tabela onde esta a informação\n",
    "* Comandos:\n",
    "    * SELECT + FROM\n",
    "    * Aliases (AS): Evitar espaços, evitar acentos e evitar caracteres especiais\n",
    "    * LIMIT Nr (No SQL Server, utiliza-se o TOP Nr): Limita a quantidade de linhas iniciais que serão mostradas\n",
    "    * OFFSET Nr: Defini em qual linha será iniciado a consulta\n",
    "    * WHERE: Filtra as observações baseado em uma condição\n",
    "    * Operadores de comparação\n",
    "    * Operador LIKE ou NOT LIKE: 'Texto%', '%Texto', '%Texto%' (Significa semelhante)\n",
    "    * Operador IN ou NOT IN: Filtra valores contido em uma coluna\n",
    "    * IS NULL: Filtrar valores nulos\n",
    "    * IS TRUE: Filtrar valores verdadeiros\n",
    "    * IS FALSE: Filtras valores falsos\n",
    "    * Operadores lógicos \n",
    "    * BETWEEN: Coluna BETWEEN Nr AN Nr\n",
    "    * CASE WHEN: Criar colunas ou variáveis, verificar uma condição e mudar formato\n",
    "    * ORDER BY\n",
    "    * NULLS FIRST: Determina a ordem que os valores nulos aparecerão → Inicio (Na clausula do ORDER BY) \n",
    "        * No SQL Server: ORDER BY (CASE WHEN Coluna IS NULL THEN 0 ELSE 1 END), Coluna\n",
    "    * NULLS LAST: Determina a ordem que os valores nulos aparecerão → Final (Na clausula do ORDER BY)\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### Agregando dados <br>\n",
    "* Comandos:\n",
    "    * COUNT(Coluna) ou COUNT( * ) ou COUNT(1): Não conta valores nulos\n",
    "    * SUM, AVG, MAX, MIN, VARIANCE, STDDEV\n",
    "    * GROUP BY\n",
    "    * HAVING: Filtra as observações baseado em uma condição depois do agrupamento \n",
    "\n",
    "<br>\n",
    "\n",
    "##### OBS: \n",
    "* count(*) as qtde → Group by com having count( * ) > 1: seleciona apenas os registros que aparecem mais de uma vez na tabela \n",
    "* Para eliminar os valores duplicados, basta usar a cláusula \"distinct\" na consulta.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Junção de tabelas <br>\n",
    "* Comandos:\n",
    "    * JOIN (Chave primária e chave secundária/estrangeira)\n",
    "    * INNER JOIN (Interseção: Elementos comuns)\n",
    "    * LEFT JOIN (Todos elementos da tabela da esquerda)\n",
    "    * RIGHT JOIN (Todos elementos da tabela da direita)\n",
    "    * FULL JOIN (Todos os elementos)\n",
    "    * COALESCE\n",
    "    * Usar o filtro no ON ao invés de no WHERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422e270",
   "metadata": {},
   "source": [
    "### <u>Método CRISP-DM</u><br>\n",
    "#### Aula 1 - Entendimento do Negócio <br>\n",
    "##### a) Objetivo: Antes de começar qualquer projeto de dados temos que ter muita clareza do que deve ser revolvido. Nesta etapa, sempre tenha as respostas para as seguintes perguntas:\n",
    "* Qual p objetivo deste trabalho?\n",
    "* O que queremos conhecer? O que queremos mudar na área com esse projeto?\n",
    "* Já existe algo realizado ou em andamento feito por alguém? Quais os resultados?\n",
    "* Suponho que consigamos atingir o objetivo, o que vem depois? Como este trabalho será usado? \n",
    "* Dicas: \n",
    "    * Faça várias perguntas sobre o negócio para o cliente até que ocorra o entendimento do processo.\n",
    "    * Sempre ajude o cliente a priorizar os projetos. Isso envolve question-alo sobre o potencial de resultado e de como esse resultado será usado na prática. Se o uso do resultado não estiver claro é uma boa ideia deixa-lo para uma próxima oportunidade.\n",
    "    \n",
    "<br>    \n",
    "\n",
    "##### b) Premissas: É a assunção de alguma verdade (É quando uma pessoa acredita em algo como verdadeiro sem ter evidências concretas para comprovar essa crença). Ex: Vamos assumir que essa amostra extraída dos dados seja aleatória e que não tem vieses de seleção. São listadas tadas as premissas que o trabalho irá assumir. Na entrevista com o cliente já se pode entender algumas delas e no entendimento dos dados também, pois nem sempre o que o cliente conhece é o que de fato acontece nos dados extraídos para análise.\n",
    "* Dica: \n",
    "    * Após a etapa de analise exploratoria dos dados muitas vezes temos mais clarezas das premissas que devem ser assumidas. Atualize sempre a lista de premissas.\n",
    "    * As premissas devem ser informadas para seu cliente desde o início. Isso é muito importante para que o cliente tenha clareza se o resultado da análise serpa realmente útil para ele. \n",
    "\n",
    "<br>\n",
    "\n",
    "##### c) Riscos envolvidos: Todo processo tem risco. Não devemos evitar o risco, mas sim controla-lo. Por tanto, nesta etapa precisamos ter clareza de quais riscos nosso projeto de dados está exposto e de como mitiga-lo (diminuí-lo), se possível. Exemplos:\n",
    "* Os dados da análise não estão estruturados em um repositório de dados validado, como o DW ou Data Lake. Desta forma corremos o risco de juntar os dados de forma incorreta devido ao trabalho manual. Além disso, o trabalho levará mais tempo. \n",
    "* As áreas de negócio não tem familiaridade com interpretação de dados ou uso de ferramentas analíticas, fazendo com que o resultado do projeto de análise possa ser mal utilizado, produzindo resultados ruins.\n",
    "* Coformidade com a LGPD: Todos nossos projetos de dados devem estar em conformidade com os requisitos da lei. Sempre verifique isso com as áreas de Governança e Privacidade da empresa.\n",
    "* O sistema ou infraestrutura de dados anda congestionada. Com isso a coleta dos dados pode demorar mais do que o previsto.\n",
    "##### Dica: A análise SWOT do processo pode ser uma ferramenta interessante para entender quais são os riscos internos e externos do projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### d) Custos x Benefícios: Todo projeto tem um custo. Nem que esse custo seja o seu tempo ou da equipe. Por tanto, devemos levantar os custos e os benefícios esperados do projeto para avaliar se vale a pena continuar. Para alcançar o objetivo precisamos coletar os dados necessário e desta forma, precisamos levantar algums perguntas como: Onde esses dados estão? Estão disponíveis e atualizados? Se não, qual o custo para começar a coletá-lo? São dados que precisam ser adquiridos de um fornecedor externo? Se sim, qual o custo? Qual o benefício esperado de seu uso? \n",
    "* Etapa 1 - Criação de planilha com o levantamento dos dados disponível\n",
    "\n",
    "<br>\n",
    "\n",
    "| Dados | Custo tangível | Cisto intangível | Benefício | Nota para custo (1 a 5) | Nota para benefício (1 a 5) |\n",
    "| ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| Ex: Dívida no mercado | 0,12 por CPF | Construção de API para pegar os dados | Melhoria consideravel do modelo de crédito | 4 | 5 | \n",
    "| Ex: Uso de Aplicativo | Nenhum, dado próprio | Construção de ETL para carregamento | Não muito caro. É preciso realizar uma análise exploratória | 3 | 2 | \n",
    "\n",
    "<br>\n",
    "\n",
    "* Etapa 2 - Criação de uma matriz de custo x benefício esperado\n",
    "\n",
    "<center><img width=\"30%\" src=\"_imagens/Matriz custo x beneficio dos dados a serem coletados.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "* Etapa 3 - Verificar em qual quadrante os dados estão concentrados para decidir se o projeto vale o esforço do time. (Lembre-se que o objetivo de um projeto de dados é maximizar o resultado com o menor custo. \n",
    "\n",
    "<br>\n",
    "\n",
    "##### e) Critérios de sucesso: Funciona como critério de parada para sabermos quando encerrar o projeto, pois o objetivo já foi satisfeitos. Bons critérios de sucesso são criados levando em consideração as métricas do negócio ou as próprias métricas obtidas do modelo estatísitco desenvolvido. Exemplos:\n",
    "* Diminuição esperad de 10% na taxa de churn de clientes (Métricas de negócio)\n",
    "* Melhoria do KS do modelo de crédito para um patamar de pelo menos 30% (Métrica de modelos)\n",
    "* Mitigar todos os riscos levantados pelo time de controles internos (Métrica regulatório)\n",
    "##### Dica: Busque referências nas áreas de negócio de bons indicadores e/ou trabalhos passados. O que funcionou bem e o que não funcionou? Se o projeto tem como objetivo melhorar um produto de dados anterior, leia a documentação, se existir, e verifique o que funcionou anteriormente que vale a pena continuar no projeto, para não precisar começar do zero.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### f) Planejamento do projeto: Criação de um cronograma contendo os dias estimados para cada etapa e tarefa do projeto. Além da inclusão das reuniões, semanais ou quinzenais, com o cliente para reportar o andamento.\n",
    "* Dicas: \n",
    "    * O cronograma deve ser aprovado pelo cliente, através de um email de confirmação ou através de algum acordo formal do cliente\n",
    "    * O tempo de conclusão depende de fatores como: Facilidade de acesso aos dados, disponibilidade do cliente, critérios de sucessos claros, tipo de risco envolvidos (Ex: Indisponibilidade sistemica), sofisticação da técnica de estatística utilizada, entre outros, por tanto, sempre dê uma estimativa otimista e uma pessimista. \n",
    "    * Planilha contendo as seguintes colunas: Etapa, tarefa, Responsavel, Estimativa em dias dividido em Pessimista e Otimista e Mês dividido em semanas. Incluir legendas como Concluido, em andamento, a iniciar e reuniões de reporte. Totalize os dias e os meses das colunas Pessimista e Otimista.\n",
    "    * As três etapas iniciais geralmente são as mais demoradas (cerca de 60% do projeto). Não sub-estime o prazo dessas etapas e sempre informe ao cliente de desvios em relação aos prazos combinados. \n",
    "    \n",
    "<br>\n",
    "\n",
    "##### g) Início da documentação: É essencial para continuidade em caso de mudanças nos times e escopo das áreas de negócio. Nesta etapa devemos consolidar todo o conhecimento obtido em um documento de fácil acesso para você, time e cliente do projeto. Deve envolver clareza na esrita e organização de seus tópicos. Principais tópicos são:\n",
    "* Todos os critérios e conhecimentos obtidos na etapa de entendimento de negócio, incluindo premisas, riscos mapeados, custos x benefícios e critérios de sucesso.\n",
    "* Cronograma do projeto\n",
    "* Análise exploratoria e transformações de dados realizados na base (Filtros, correções, padronizações, tratamento de valores faltantes, etc.)\n",
    "* Estudo e modelo desenvolvido e suas conclusões.\n",
    "* Crie um glossário dos termos de negócio.\n",
    "* Sempre mantanha um atualizado histórico de versões do documento.\n",
    "* Plano de implantação e acompanhamento.\n",
    "* Sempre documente as aprovações e alinhamentos com o cliente (guarde as ATA's de reunião na pasta do projeto)\n",
    "OBS: A documentação é parte das melhores prátias da metodologia de Gestão do conhecimento.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 2 - Entendimento dos Dados <br>\n",
    "##### a) Descrição dos dados e coleta: Precisamos ter acesso ao dicionário de dados (a.k.a Metadados) do repositório de dados. A partir daí, utilizamos a linguagem SQL, por exemplo, para realizar as devidas consultas, manipulações e extrações dos dados necessários para análise. Algumas empresas ou processos podem não ter os dados já estruturados, neste caso, será o responsável por estruturar. É muito comum ter acesso ao documento ERD (Diagram de relacionamento de entidades) do banco de dados também. Dica: Evite fazer um SELECT * FROM de seu banco de dados para extrair a base completa para sua ferramenta de análise (Excel, Power BI, Python etc). Extraia apenas a informação necessária, pois será mais performático desta forma. <br>\n",
    "\n",
    "##### b) Análise Exploratória e verificação da qualidade dos dados (Sanity Check): Nesta etapa estará aplicando técnicas de estatística descritiva para entender cada variável de sua base e suas relações entre as outras variáveis. Ao resumir os dados terá condições de verificar a qualidade (Sanity Check) dos dados. Se algo estranho for encontrado (valores fora de um limite razoavel, outliers etc), sempre poderá questionar as áreas que liberaram a informação para verificação. Principais técnicas de análise exploratória:\n",
    "* Tabelas de Frequência: Frequência absoluta, relativa e acumulada.\n",
    "* Visualização de dados: Box-plots, Histograma, Gráficos de linhas e colunas.\n",
    "* Medidas resumo: Média, Mediana, Quartis e Desvio Padrão.\n",
    "* Análise Bidimensional: Correlação de Pearson, Coeficiente de Determinação e Information Value (IV).\n",
    "* Etapas: 1 - Faça a análise de forma univariavel (Variável a variável), 2 - Faça a análise bivariada/bidimensional (Foque na variável que interessa e evite ficar calculando cada combinação de variáveis dois a dois sem um motivo claro), 3 - Faça um boxplot para as variáveis numéricas, pois rapidamente conhecerá as concentrações e possíveis outliers para investigar e 4 - Para realizar o Sanity Check (Teste de qualidade) das variáveis numéricas verifique a amplitude, calculando o máximo e o mínimo e para as variáveis qualitativas, faça contagens para entender as categorias da variável que estão disponiveis.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 2 - Preparação dos Dados <br>\n",
    "##### a) Seleção de variáveis: É comum coletarmos vários dados que pensavamos serem importantes na etapa de conhecimento de negócio que se mostraram pouco relevantes após a análise exploratória inicial. Se o problema for do tipo bidimensional, quando quer entender os fatores que influenciam uma determinada variável de seu interesse (target), podemos aplicar técnicas de associação de variáveis para ranquear as variáveis mais relacionadas com o target. Desta forma, focamos nas variáveis que realmente vão trazer valor para o problema a ser resolvido. Principais técnicas para escolher as variáveis mais importantes: <br>\n",
    "* Técnica de Information Value (IV) → Ranking de IV's do maior para o menor\n",
    "* Cálculo do P-valor em modelos lineares como a Regressão Linear Múltipla e Logística (Ex.: Ténica Stepwise) ou Distribuição de probabilidade (Ex.: Algoritmo Boruta)\n",
    "##### Após selecionar as variáveis mais importantes, construa uma query automatizada para que utilize os dados futuramente quando o projeto for implantado. <br>\n",
    "##### b) Limpeza e formatação dos dados (Data Wrangling ou Data Cleaning): Preparação dos dados para a aplicação das técnicas estatísticas.\n",
    "* Tratamento de dados faltantes (missing values) → Evite excluir dados faltantes sem antes entender o motivo do por que não estão no conjunto de dados\n",
    "    * Algum sistema não tem uma validação de campo obrigatório no formulário?\n",
    "    * O time de engenharia de dados pecou em alguma etapa do pipeline? \n",
    "    * Dependendo da origem do problema, pode ajustar facilmente, extraindo os dados novamente ou ainda usando técnicas de imputação de dados (Imputação de missing)\n",
    "* Padronização de case-sensitive\n",
    "* Categorização de variáveis numéricas\n",
    "* união de tabelas (joins)\n",
    "* Feature engineering (Extração de características)\n",
    "    * Criação de novas variaveis apartir de uma variavel bruta. Ex: Sr.X para Sr. → Gênero Masculino, Data de nascimento → Idade ou Idade → Faixa etária\n",
    "* Conversão de tipos de dados\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 3 - Desenvolvendo o estudo ou modelo <br>\n",
    "##### a) Escolha da técnica estatística que responde o problema: É preciso ter clareza do tipo de \"produto de dados\" que deve ser entregue e escolher a tecnica mais adequada para cada contexto. Resumo das principais técnicas estatísticas: <br>\n",
    "* 1. O que você precisa resolver? \n",
    "* 1.1 - Resumir e interpretar uma base de dados → \n",
    "    * a) Análise Unidimensional (Tabelas de frequência, visualização de dados e medidas de resumo) \n",
    "    * b) Análise Bidimensional (Correlação de Pearson, Coeficiente de Determinação e Information Value (IV))\n",
    "* 1.2 - Atribuir probabilidade a um evento do meu interesse → Todos os eventos podem acontecer com a mesma chance? → \n",
    "    * a) Sim → Teoria Clássica (Eventos / Espaço Amostral)\n",
    "    * b) Não → Teoria Frequentista (Verificar a frequência do passado para entender qual será a probabilidade de ocorrência/frequencia dos dados no futuro)\n",
    "* 1.3 - Estimar o valor de uma populçaõ atraves da amostra → Esse valor depende de outras variaveis? → \n",
    "    * a) Sim → O valor a ser estimado é numérico? → \n",
    "        * Sim → Regressão Linear multipla\n",
    "        * Não → Regressão Logística\n",
    "    * b) Não → Você precisa de uma estimativa com margem de erro? → \n",
    "        * Sim → Intervalo de confiança\n",
    "        * Não → Teste de hipóteses → Você quer testar se esse valor da população é igual a um número específico? → \n",
    "            * Sim → Teste para média e Proporção (Teste T)\n",
    "            * Não →  Esse valor da população pertence a grupos diferentes ou pareados (Antes x Depois)? → \n",
    "                * Diferentes → Teste T de duas populações (Teste A/B)\n",
    "                * Pareados → Teste T pareado\n",
    "    \n",
    "<br>\n",
    "\n",
    "##### b) Desenvolvimento do estudo analítico ou modelo:\n",
    "* Validação da amostra: Se representa a população, a quantidade e a qualidade antes de realizar os cálculos e comparações.\n",
    "* Escolha a técnica estatística (Atenção para as premissas e pontos fracos e fortes de cada técnica)\n",
    "* Ter tempo suficiente para limpar os dados.\n",
    "* Sempre valide um modelo utilizando técnicas de validação cruzada.\n",
    "* Tome cuidado com o Data Leakage (Quando uma variável em um período futuro ou da partição de teste vaz para a partição de treino do modelo)\n",
    "* Concevrse com o cliente, por que ele podem fornecer muito direcionamento para interpretar e validar os insights obtidos.\n",
    "* Busque estudos anteriores na literatura.\n",
    "* Comece com o modelo mais simples e depois vai sofisticando.\n",
    "* Tente combinar variáveis.\n",
    "* Desenvolva o estudo ou modelo já pensando em como ele pode ser implantado.\n",
    "* Interprete o modelo. Não informe apenas a performance.\n",
    "* Extraia uma \"foto\" dos seus dados e não mude mais. Sempre use random-state nos pacotes do Python. Isso é importante para reperformar seus resultados na etapa de validação.\n",
    "* Seja Data Centric em vez de Model Centric. Os modelos não fazem milagre, se não tiver boas variáveis. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 4 - Validação do Trabalho <br>\n",
    "##### a) Verificação dos critérios de sucesso e validação do estudo / modelo: Nesta etapa verificamos se estamos satisfeitos com os resultados obtidos, utilizando como base os critérios de sucesso. Além disso, revisamos todo o processo em busca de falhas para validar o estudo ou modelo antes de entrera em implantação. Um bom estudo ou modelo é aquele que dá boas perspectivas de melhoria do processo ou negócio envolvido. Realize um Peer Review, para validar se o estudo ou modelo está correto no ponto de vista estatistico ou mesmo que todas as etapas operacionais foram cumpridas corretamente, através de áreas específicas focadas em validação ou atráves de um validador que pode ser um colega ou gestor. <br>\n",
    "##### b) Aprovação pelo cliente do trabalho e atualização do Roadmap: Apresentação do projeto deve conter no mínimo:\n",
    "* Objetivo do projeto\n",
    "* Premissas e alinhamentos iniciais\n",
    "* Apresentação dos tipos de variáveis analisadas e suas respectivas análises exploratórias\n",
    "* Propostas de solução para os problemas de negócio (Estudo, Modelo, Dashboards, etc)\n",
    "* Próximos passos do projeto (Ex: Aprovação formal do cliente, implantação e acompanhamento, ect)\n",
    "* Roadmap (Informar o que ficou para a próxima versão)\n",
    "##### Para criar planos de ação, uma abordagem é utilizar a metodologia 5w2h, com o foco em trazer mais resultados no curto prazo.  \n",
    "\n",
    "<center><img width=\"60%\" src=\"_imagens/Exemplo de plano de ação sugeridos.png\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Aula 5 - Deploy - Implantação <br>\n",
    "##### Plano de implantação e monitoramento do modelo/KPI’s: Naturamente vai depender do tipo de negócio, tecnologia e processos da empresa. Mas de uma forma geral, se o seu plano de ação visa melhorar um indicador/KPI, é interessante criar um acompanhamento automático em um Dashboard com indicação clara do momento em que o plano de ação foi implantado. Já para modelos de Machine Learning, defina as métricas a serem utilizadas para o monitaramento do modelo, crie um Dashboard de acomapnhamento do modelo e atualize a documentação. Algumas métricas de monitoramento:\n",
    "* KS/AuROC ou qualquer medida em treino\n",
    "* Distribuição dos scores/clusters em treino\n",
    "* Taxa do Target em treino\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Discas: \n",
    "* Insira gatilhos propostos para iniciar um processo de re-treino ou calibragem do modelo. \n",
    "* Ao implantar a melhoria trazida pelo projeto, não faça tudo de uma vez com toda a sua base. Crie grupos de controle e teste e vá monitorando as diferenças entre os grupos ao longo do tempo. Se o grupo teste estiver melhor, aumente o grupo até que todo o seu processo esteja passando pelas diretrizes na melhoria implantada. (Forma gradual)\n",
    "\n",
    "<center><img width=\"60%\" src=\"_imagens/Exemplo de acompanhamento do estudo apos implantação.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83eae9ed",
   "metadata": {},
   "source": [
    "### <u>Calculando o Valor de um Projeto de Dados e Apresentando os Resultados</u><br>\n",
    "#### Calculando o valor de um projeto de dados: <br>\n",
    "##### Uma parte fundamental de um projeto de dados é a mensuração de resultados e por isso, precisamos apurar quais foram os ganhos obtidos pela implantação do projeto, seja um estudo ou modelo. \n",
    "* O Backteste é muito utilizado para avalair o potencial de resultado financeiro do projeto.\n",
    "* O Teste & Controle é o método para mensurar o valor real que o porjeto está gerando para a empresa.\n",
    "\n",
    " <br>\n",
    " \n",
    "#### Aula 1 - Mensurando o potencial de um projeto de dados <br>\n",
    "##### O Backteste consiste em fazer uma comparação entre os resultados obtidos com o novo modelo churn x método atual, utilizando uma base de dados histórica, normalmente um dataset out-of-time. <br>\n",
    "##### Um dataset out-of-time é um conjunto de dados utilizado em tarefas de avaliação e validação de modelos de previsão ou análise de séries temporais. Esse tipo de dataset é criado separando uma parte dos dados para serem utilizados como dados de treinamento e uma parte para avaliar o desempenho do modelo. A principal característica do dataset out-of-time é que os dados que serão utilizados para avaliação não devem ter sido vistos durante o treinamento do modelo. <br>\n",
    "##### A separação em um dataset out-of-time geralmente envolve a definição de um ponto de corte temporal, onde todos os dados anteriores a esse ponto são usados para treinamento e os dados posteriores são usados para teste. É importante que essa divisão seja feita de forma aleatória e representativa, garantindo que as características dos dados de treinamento e teste sejam semelhantes.<br>\n",
    "##### Uma abordagem comum é utilizar uma janela deslizante, onde o ponto de corte temporal é movido ao longo do tempo para criar vários conjuntos de treinamento e teste. Isso permite avaliar o desempenho do modelo em diferentes períodos de tempo e verificar se ele é capaz de generalizar bem para dados futuros.<br>\n",
    "##### Além disso, é importante tomar cuidado com a presença de tendências temporais nos dados. Se houver uma tendência clara nos dados, é recomendado utilizar técnicas como a validação cruzada temporal ou a validação cruzada em série para garantir uma avaliação mais robusta do modelo.<br>\n",
    "\n",
    "<center><img width=\"60%\" src=\"_imagens/Validação OOT.png\"></center>\n",
    "\n",
    "<br>\n",
    " \n",
    "##### Após o desenvolvimento do modelo utilizando o dataset de Treino, utilizamos o modelo para fazer a predição do dataset de validação e avaliamos o desempenho do modelo nesse conjunto. <br>\n",
    "##### Exemplo: <br>\n",
    "| Métrica | Treino - Novo Modelo | Validação - Novo Modelo | Validação - Método Atual |\n",
    "| --- | --- | --- | --- |\n",
    "| Acurácia | 75% | 72% | 64% |\n",
    "| Precision | 87% | 85% | 78% |\n",
    "| Recall | 95% | 94% | 89% |\n",
    "| ... | ... | ... | ... |\n",
    "| Auroc | 89% | 87% | 79% |\n",
    "\n",
    "##### No exemplo acima, o novo modelo está apresentando resultados melhores que o método atual, mas precisamos converter esse ganho de performance em valores financeiro. <br>\n",
    "\n",
    "##### - Como converter o ganho de performance em valores financeiros? Para medir o impacto financeiro gerado pelo novo modelo devemos avaliar as decisões tomadas a partir deste modelo e as consequências financeiras dessas decisões. Para isso, construimos uma matriz de custo.  <br>\n",
    "##### Exemplo: \n",
    "| Real/Predito | Churn | Não Churn |\n",
    "| --- | --- | --- |\n",
    "| Churn | Resultado: 300,00 | Resultado: -300,00 |\n",
    "| Não churn | Resultado: -200,00 | Resultado: 500,00 |\n",
    "\n",
    "##### No exemplo acima: \n",
    "* Churn é uma métrica utilizada para mostrar o número de clientes que cancelam serviço em um determinado período de tempo.\n",
    "* Os valores de 200,00 e de 500,00 são valores informados pelo setor comercial. \n",
    "* Quando os dados reais mostraram que ocorreria o Churn e o modelo também, tivemos um resultado 300,00, pois ocorreu um custo de retenção de 200,00 e uma receita com o cliente de 500,00. * Quando os dados reais mostraram que ocorreria o Churn e o modelo não preveu isso, tivemos um prejuizo de 300,00, pois deixamos de ganhar 300,00, já que não fizemos nada para reter o cliente (Não gerou o custo de 200,00) e com isso o cliente saiu (Não gerou receita de 500,00).\n",
    "* Quando os dados reais mostraram que não ocorria o Churn e o modelo também, tivemos um resultado de 500,00, pois não precisamos realizar o custo de retenção de 200,00 e tivemos uma receita com o cliente de 500,00.\n",
    "* Quando os dados reais mostraram que não ocorria o Churn e o modelo previu que ocorreria, tivemos um prejuízo de 200,00, pois realizamos um custo de reteção de 200,00 desnecessariamente.\n",
    "\n",
    "##### Após reazalir a construção da matriz de custo, aplicamos a matriz de custo na matriz de confusão de cada método.<br>\n",
    "##### Exemplo: Supondo que o dataset de validação possua 500 clientes, obtivemos as seguintes matrizes de confusão: <br>\n",
    "\n",
    "<center><img width=\"60%\" src=\"_imagens/Matriz de custo x matriz de confusão - Backteste.png\"></center>\n",
    "\n",
    "##### No exemplo acima, considerando as premissas do número de clientes da empresa e a diferença obtida entre o novo modelo e o método atual, podemos concluir que ao utilizar o novo modelo churn a empresa tem um potencial estimado de gerar um valor adicional de aproximadamente de 16 milhões. <br>\n",
    "\n",
    "#### Aula 2 -  Mensurando o impacto gerado após a implantação <br>\n",
    "##### O Teste/Controle ou Teste A/B é um método muito utilizado em pesquisas de experimentação com usuários. O método consiste em apresentar 2 ou mais versões diferentes para o usuário e medir o resultado obtido em cada versão. São identificados padrões que geram melhores resultados, de acordo com os objetivos pré-definidos. (O cliente não pode perder a percepção de valor do produto, por que o cliente pode acabar cancelando o serviço) <br>\n",
    "##### Para aplicar essa técnica a base de clientes será divida aleatoriamente (Amostragem aleatória simples) em 2 grupos: Teste e Controle. A separação dos clientes no grupos Teste e Controle deve ser feita de forma que os clientes desses 2 grupos possuam características semelhantes para não comprometer a comparação.<br>\n",
    "##### Exemplo: O novo modelo será utilizado para estimar a probabilidade de Churn no grupo Teste, que será de 90% dos clientes. O grupo Controle, com 10% dos clientes restantes terão sua probabilidade de Churn estimada com base no método atual. Durante o período de 1 mês foram calculadas as estimativas da probabilidade de Churn de 1.000 clientes, sendo 900 calculadas a partir do novo modelo, enquanto os 100 restantes tiveram a estimativa calculada a partir do método atual. <br>\n",
    "| Métrica | Teste | Controle | \n",
    "| --- | --- | --- | \n",
    "| Acurácia | 75% | 62% | \n",
    "| Precision | 85% | 77% | \n",
    "| Recall | 92% | 89% | \n",
    "| ... | ... | ... | \n",
    "| Auroc | 87% | 78% |\n",
    "\n",
    "##### Comparando os grupos Teste e Controle e utilizando os indicadores de performance adequados, podemos verificar que o novo modelo de churn apresenta melhores resultados. <br>\n",
    "##### - Como converter o ganho de performance em valores financeiros? Para medir o impacto financeiro gerado pelo novo modelo devemos avaliar as decisões tomadas a partir deste modelo e as consequências financeiras dessas decisões. Para isso, construimos uma matriz de custo.  <br>\n",
    "##### Exemplo: \n",
    "| Real/Predito | Churn | Não Churn |\n",
    "| --- | --- | --- |\n",
    "| Churn | Resultado: 300,00 | Resultado: -300,00 |\n",
    "| Não churn | Resultado: -200,00 | Resultado: 500,00 |\n",
    "\n",
    "##### No exemplo acima: \n",
    "* Churn é uma métrica utilizada para mostrar o número de clientes que cancelam serviço em um determinado período de tempo.\n",
    "* Os valores de 200,00 e de 500,00 são valores informados pelo setor comercial. \n",
    "* Quando os dados reais mostraram que ocorreria o Churn e o modelo também, tivemos um resultado 300,00, pois ocorreu um custo de retenção de 200,00 e uma receita com o cliente de 500,00. * Quando os dados reais mostraram que ocorreria o Churn e o modelo não preveu isso, tivemos um prejuizo de 300,00, pois deixamos de ganhar 300,00, já que não fizemos nada para reter o cliente (Não gerou o custo de 200,00) e com isso o cliente saiu (Não gerou receita de 500,00).\n",
    "* Quando os dados reais mostraram que não ocorria o Churn e o modelo também, tivemos um resultado de 500,00, pois não precisamos realizar o custo de retenção de 200,00 e tivemos uma receita com o cliente de 500,00.\n",
    "* Quando os dados reais mostraram que não ocorria o Churn e o modelo previu que ocorreria, tivemos um prejuízo de 200,00, pois realizamos um custo de reteção de 200,00 desnecessariamente.\n",
    "\n",
    "##### Após reazalir a construção da matriz de custo, aplicamos a matriz de custo na matriz de confusão de cada método.<br>\n",
    "##### Exemplo: Como o dataset de validação possue 1.000 clientes, obtivemos as seguintes matrizes de confusão:\n",
    "\n",
    "<center><img width=\"60%\" src=\"_imagens/Matriz de custo x matriz de confusão - Teste e Controle.png\"></center>\n",
    "\n",
    "##### No exemplo acima, considerando as premissas o número de clientes que utilizaram o novo modelo e a diferença obtida entre os grupos Teste e Controle, o Novo Modelo Churn gerou neste primeiro mês 73.800,00. <br>\n",
    "#### Apresentando seu projeto de dados <br>\n",
    "* Maturidade de dados → Apresentação do projeto de uma forma estruturada → Otimização da percepção de valor (Valor financeiro + Valor do conhecimento)\n",
    "* Apresentação deve conter 3 partes principais: \n",
    "    * Introdução e escopo: Descrição de todas as definições necessárias antes de iniciar a exploração dos dados (Objetivo do modelo, Histórico de versões, Visão regulatória, Papéis e responsabilidades, Bases de dados utilizadas, Público alvo, Target, Descrição das variáveis, Premissas e Limitações)\n",
    "    * Metodologia: Apresentação dos insights obtidos com a análise exploratória, descrição da técnica utilizada, resultados obtidos e a definição do macro fluxo de implantação da solução.\n",
    "    * Conclusão e próximos passos: Apresentação dos detalhes do deploy em produção, o acompanhamento do modelo (Monitoramento do desempenho) e roadmap das melhorias (Quais podem ser implementadas no futuro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
