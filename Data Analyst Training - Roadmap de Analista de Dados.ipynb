{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05e1a91",
   "metadata": {},
   "source": [
    "<center><img width=\"25%\" src=\"https://www.camaramirimdoce.sc.gov.br/media/noticia/resumo-da-sessao-10-06-2019-111.png?w=848&h=450&t=P&c=f0f0f0&q=80&v=2\"></center>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Data Analyst Training - Roadmap de Analista de Dados\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8371e05",
   "metadata": {},
   "source": [
    "# **<u>Sumário:</u>**\n",
    "1. [O que é análise de dados](#O-que-é-análise-de-dados)\n",
    "2. [O que faz um analista](#O-que-faz-um-analista:)\n",
    "    * Algumas atribuições\n",
    "    * Soft Skills\n",
    "    * Hard Skills\n",
    "3. [Método cíclico](#Método-cíclico:)\n",
    "    * Etapa 1 - Problema de negócio / Declaração do problema\n",
    "    * Etapa 2 - Definição da Baseline\n",
    "    * Etapa 3 - Planejamento\n",
    "    * Etapa 4 - Coleta de Dados Brutos de Fontes Confiáveis (Várias Fontes Internas e Externas)\n",
    "    * Etapa 5 - Limpeza e Processamento de Dados\n",
    "    * Etapa 6 - Análise Exploratória de Dados\n",
    "    * Etapa 7 - Preparação dos Dados (Modelagem dos dados)\n",
    "    * Etapa 8 - (Pré) Validação dos resultados com os clientes\n",
    "    * Etapa 9 - Storytelling dos processos e resultados\n",
    "4. [Qualidade dos Dados](#Qualidade-dos-Dados:)\n",
    "    * Características dos dados de qualidade\n",
    "    * Práticas recomendadas para garantir qualidade\n",
    "5. [Técnicas de Limpeza](#Técnicas-de-Limpeza:)\n",
    "    * 1 - Limpe observações indesejadas\n",
    "    * 2 - Correção de inconsistências ou imprecisos\n",
    "    * 3 - Padronizar os dados \n",
    "    * 4 - Remoção de valores discrepantes indesejados\n",
    "    * 5 - Corrigir erros de dados contraditórios ou conjuntos cruzados\n",
    "    * 6 - Erros de conversão e sintaxe de tipo\n",
    "    * 7 - Lidar com dados ausentes\n",
    "    * 8 - Validar o conjunto de dados\n",
    "6. [Técnicas de Imputação de Dados Ausentes](#Técnicas-de-Imputação-de-Dados-Ausentes:)\n",
    "    * 1 - Exclusão de lista ou análise de caso completo (ou caso disponível)\n",
    "    * 2 - Exclusão de pares\n",
    "    * 3 - Eliminando variáveis\n",
    "    * 4 - Imputação com média, mediana ou moda\n",
    "    * 5 - Última observação realizada\n",
    "    * 6 - Próxima observação recuada\n",
    "    * 7 - Interpolação Linear\n",
    "    * 8 - Imputação de ponto comum (Adequado para valores ordinais)\n",
    "    * 9 - Adicionando uma categoria para capturar N/A\n",
    "    * 10 - Atribuição de categoria frequente\n",
    "    * 11 - Imputação de valor arbitrário\n",
    "    * 12 - Adicionando uma variável para capturar N/A\n",
    "    * 13 - Imputação de Amostra Aleatória\n",
    "    * 14 - Imputação Múltipla\n",
    "    * 15 - Modelos preditivos / Estatísticos que imputam os dados ausentes\n",
    "7. [Principais Benefícios da Limpeza de Dados](#Principais-Benefícios-da-Limpeza-de-Dados:)\n",
    "8. [Tipos de Análises](#Tipos-de-Análises:)\n",
    "    * 1 - Análise Descritiva\n",
    "    * 2 - Análise Diagnóstica\n",
    "    * 3 - Análise Preditiva\n",
    "    * 4 - Análise Prescritiva\n",
    "9. [Análise Exploratória de Dados (EDA)](#Análise-Exploratória-de-Dados-(EDA):)\n",
    "    * Etapas: \n",
    "        * 1 - Obter os dados (Seleção de dados)\n",
    "        * 2 - Verificar a integridade dos dados\n",
    "        * 3 - Limpar os dados\n",
    "        * 4 - Explorar as variáveis\n",
    "        * 5 - Identificar padrões e tendências\n",
    "        * 6 - Visualizar os dados\n",
    "        * 7 - Transformação de dados\n",
    "    * Tipos:    \n",
    "        * 1 - Análise Uni-variada\n",
    "        * 2 - Análise Bivariada\n",
    "        * 3 - Análise Multivariada\n",
    "        * 4 - Análise de Cluster\n",
    "        * 5 - Análise de Tendências\n",
    "        * 6 - Análise de Outliers\n",
    "        * 7 - Análise de Dados Faltantes\n",
    "        * 8 - Análise de Textos e Processamento de Linguagem Natural\n",
    "        * 9 - Análise de Série Temporal \n",
    "        * 10 - Análise de Regressão\n",
    "        * 11 - Análise Fatorial\n",
    "        * 12 - Análise de Coorte\n",
    "        * 13 - Análise de Sentimentos\n",
    "        * 14 - Simulação ou método de Monte Carlo \n",
    "10. [Principais Benefícios da EDA](#Principais-Benefícios-da-EDA:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dbd3cb",
   "metadata": {},
   "source": [
    "# O que é análise de dados\n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921de132",
   "metadata": {},
   "source": [
    "### O que são Dados? A palavra dados é o plural da palavra datum e datum significa uma única informação. Os dados referem-se a observações coletadas, medições, fatos, registros, descrições e podem ser números, texto ou multimídia, ou seja, vídeo, imagem, áudio, documento, etc. Os dados também podem ser chamados de fatos brutos, destinados a serem processados em informações significativas que podem ser usadas na tomada de decisões pelas organizações. Os dados se metamorfoseiam em informações depois de terem passado por vários processos analíticos de dados. <br>\n",
    "\n",
    "### Os dados podem ser obtidos de pesquisas/questionários, mídia social, organizações, governo, sensores/máquinas/instrumentos, etc. <br>\n",
    "\n",
    "### Eles estão em toda parte, estão sendo gerados em todos os setores, mas há uma diferença em como os dados estão sendo aproveitados em informações em diferentes setores. Por exemplo, plataformas de streaming de música/vídeo usam os dados gerados sobre como você interage com a plataforma, por exemplo, gênero musical e artistas que você ouve, músicas mais tocadas, localização, etc., para apresentar recomendações de músicas e listas de reprodução para você. <br>\n",
    "\n",
    "### Os mecanismos de pesquisa usam o histórico de pesquisa, dados de localização, etc., para obter resultados de pesquisa e anúncios. As empresas de comércio eletrônico usam dados do histórico de compras, histórico de pesquisa e preferências do cliente para criar recomendações personalizadas de produtos e anúncios para seus clientes. <br>\n",
    "\n",
    "### Os dados também podem ser usados para agilizar processos e aumentar a produtividade. Por exemplo, os dados de desempenho da cadeia de suprimentos podem ser examinados para detectar ineficiências e possíveis áreas de melhoria. As empresas podem reduzir custos, acelerar os prazos de entrega e aumentar a satisfação do cliente melhorando a cadeia de suprimentos. <br>\n",
    "\n",
    "### Enfim, podem ser usados para encontrar novas perspectivas de negócios. As empresas podem encontrar lacunas no mercado e criar novos produtos ou serviços para satisfazer os desejos dos clientes, pesquisando as tendências do mercado e o comportamento do consumidor.\n",
    "\n",
    "### Os dados cresceram e se tornaram um recurso útil para organizações em todo o mundo na era digital. A quantidade de dados produzidos cresceu tremendamente à medida que a tecnologia avançou, fornecendo um enorme corpo de conhecimento que pode ser aproveitado para estimular o desenvolvimento, a inovação e a vantagem competitiva. Semelhante ao petróleo, commodities importantes, como dados, precisam ser processadas e refinadas para serem úteis. Embora muitas organizações entendam o valor dos dados, elas podem não estar utilizando todo o seu potencial. <br>\n",
    "\n",
    "###  A capacidade de oferecer insights sobre o comportamento e as preferências do cliente é uma de suas principais vantagens. As empresas podem aprender mais sobre o que seus consumidores desejam, como eles se envolvem com produtos e serviços e quais variáveis afetam suas decisões de compra estudando os dados do cliente. Esses dados podem ser utilizados para melhorar as ofertas de produtos, criar esforços de marketing mais focados e aprimorar a experiência geral do cliente. Quanto mais dependemos de dispositivos e serviços digitais, mais dados geramos e, por sua vez, mais importante se torna para as empresas entender esses dados. Quase todas as empresas estão coletando dados sobres seus clientes, para saber lidar, interpretar e comunicar padrões. <br>\n",
    "\n",
    "### Todos os produtos, serviços e estratégias mais eficazes e bem sucedidos atualmente são orientados por dados, desde nossa compreensão da pandemia até as recomendações pontuais que recebemos de empresas como o Netflix. É uma forma de inteligência de negócio permitindo que empresas e organizações tornem decisões inteligentes com base no que os dados estão dizendo. <br>\n",
    "\n",
    "### A análise de dados abrange a extração e/ou coleta de dados brutos, a preparação e a análise subsequente desses dados e a narrativa, compartilhamento insights importantes, significativos e acionáveis, usando-os para explicar ou prever determinados cenários, como identificar padrões, procurar mudanças, detectar anomalias, possíveis melhorias, possíveis erros, méricas de negócios e resultados, para tomar decisões, elaborar estratégias e os próximos passos (Determinar o melhor curso de ação). <br>\n",
    "\n",
    "### Embora os dados possam fornecer informações valiosas, gerenciá-los e analisá-los pode ser um desafio. Aqui estão alguns dos desafios comuns que as empresas enfrentam quando se trata de gerenciamento de dados:\n",
    "1 - Privacidade e segurança dos dados <br>\n",
    "2 - Qualidade dos dados <br>\n",
    "3 - Análise e interpretação de dados <br>\n",
    "4 - Deixar de investir na tecnologia e no talento certos <br>\n",
    "5 - Não inovar com dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6db291",
   "metadata": {},
   "source": [
    "# O que faz um analista: \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf5a18",
   "metadata": {},
   "source": [
    "### **<u>Algumas atribuições:</u>**\n",
    "Quem fará o diagnóstico das situações.\n",
    "* Trabalhar com as equipes de TI Gestão e / ou cientistas de dados para determinar e realizar o processo de análise de dados.\n",
    "* Coletar dados de fontes primárias e secundárias.\n",
    "* Efetuar modelagem de dados para análise.\n",
    "* Realizar limpeza nos dados e descartar informações irrelevantes.\n",
    "* Analisar e interpretar os resultados utilizando ferramentas estatísticas e técnicas convencionais.\n",
    "* Identificar tendências, correlação e padrões em conjunto de dados complexos através de análise exploratória.\n",
    "* Identificar novas oportunidades para melhoria de processos e insights (percepções)\n",
    "* Concepção, criação, gerenciamento e manutenção de banco de dados relacionais e NoSQL.\n",
    "* Resolver problemas de código e questões relacionadas a dados.\n",
    "* Dominar linguagem como R, Python ou SQL e saber construir visualizações eficientes. \n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Soft Skills:</u>** \n",
    "* Habilidade de comunicação, colaboração e apresentação\n",
    "* Solução de problemas\n",
    "* Pesquisas e curiosidade\n",
    "* Atenção aos detalhes\n",
    "* Uma mentalidade analítica\n",
    "* Uma afinidade com números\n",
    "* Boa capacidade de organização e de cumprir prazos \n",
    "* Algum conhecimento comercial ou visão de negócios\n",
    "* Uma abordagem metódica e lógica\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Hard Skills:</u>**\n",
    "* Microsoft Excel\n",
    "* Conhecimento de linguagem de programação: Python\n",
    "* Conhecimento de linguagem de consulta: SQL\n",
    "* Proficiencia em software de BI e Análise: Tableau, SAS e Rapidminer\n",
    "* Expertise em Visualização de dados\n",
    "* Capacidade de minerar, analisar, modelar e interpretar dados\n",
    "* Capacidade de trabalhar com conjunto de dados grandes e complexos\n",
    "* Capacidade de implantar modelos estatísticos comercialmente viáveis\n",
    "* Capacidade de comunicar descobertas e fazer recomendações acionáveis para o negócio\n",
    "* Sólida compreensão dos processos e princípios de criação de perfil de dados e coleta de requisitos\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Geralmente, existem dois tipos de solicitações que as equipes de negócios levantariam com as equipes de dados: \n",
    "* Solicitações AD HOC (para esta finalidade): São solicitações repentinas levantadas pelas equipes de negócios para responder a certas perguntas que podem surgir devido a alguns eventos/cenários de negócios. Alguns exemplos abaixo: \n",
    "    * Por que as vendas na região Norte caíram no último mês? \n",
    "    * Quais são os níveis de estoque no depósito da ABC agora? \n",
    "Em tais cenários, não precisa construir painéis sempre que tal solicitação for levantada. Geralmente leva de 2 a 5 dias para atender a esses tipos de solicitações ad hoc. \n",
    "OBS: Precisa observar que, se uma determinada consulta for levantada em intervalos regulares como solicitações ad hoc, pode prosseguir e criar um painel para que todo o processo de consulta seja automatizado.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Solicitações de monitoramento contínuo: São solicitações em que as equipes de negócios desejam monitorar determinados KPIs em intervalos regulares (diariamente, semanalmente etc.). Nesses cenários, precisaria criar painéis de autoatendimento adequados, para que as equipes de negócios pudessem fazer a análise usando o painel e não precisassem entrar em contato com as equipes de dados com consultas ad hoc relacionadas a esses KPIs. Esses tipos de solicitações precisariam de várias aprovações, pois o tempo necessário para atendê-las pode variar de 2 semanas a alguns meses.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img width=\"40%\" src=\"_imagens/Analista de dados.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d60d2",
   "metadata": {},
   "source": [
    "# Método cíclico:\n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3e148",
   "metadata": {},
   "source": [
    "### **<u>Etapa 1</u> - Problema de negócio / Declaração do problema**\n",
    "* Compreensão e Entendimento do Negócio\n",
    "* Levantamento de requisitos:\n",
    "    * 1 - Conversar com os usuários finais (público)\n",
    "        * a) Por que é importante essa análise para o negócio? \n",
    "        * b) O que será desenvolvido no projeto? Onde quer chegar? \n",
    "        * c) Quais dores/sonhos/resultados/necessidades que o público procura resolver? \n",
    "    * 2 - Quem vai ter acesso a análise (Dashboard, Planilha, E-mail etc)?\n",
    "        * a) Quem é o público? Interno ou externo? \n",
    "        * b) Em qual nível organizacional (Estratégico, Tático ou Operacional) eles estão? Quais são as suas áreas? \n",
    "        * c) Terá RLS (Row Level Security): Controle de acesso entre áreas ou níveis hierarquicos? \n",
    "    * 3 - Fontes de dados > Quais são os dados, seus tipos, seus tamanhos e onde estão armazenados? Tem um dicionário de dados explicando o que significam? \n",
    "        * a) O que será analisado? Qual é o contexto? \n",
    "        * b) Dados internos ou externos? \n",
    "        * c) Dados nacionais ou internacionais? \n",
    "        * d) O que não pode faltar? \n",
    "        * e) É uma análise de comportamentos, padrões e tendências? \n",
    "        * f) Qual é o banco de dados (SQL Server, Oracle, MySQL)? Estão acessíveis? Alguém conhece as tabelas? Estão online ou fisico\n",
    "        * g) Não é um banco de dados? São exportados pelo sistema? Armazenado em planilha? Alimentada manualmente? Com que frequência?\n",
    "    * 4 - Definição dos Indicadores e KPIs, Regras de Negócio, Regras de Cálculo / Fórmulas > As métricas devem usar os critérios SMART e precisam estar alinhados com as perguntas do negócio e ajudar a identificar portunidades ainda não percebidas e acompanhar desempenho do processo:\n",
    "        * Específico: Bem definido, para que todos da equipe possam entendê-lo.\n",
    "        * Mensurável: é possível determinar se o KPI foi atingido ou não.\n",
    "        * Alcançável: A equipe possui as habilidades e recursos para atingir o KPI.\n",
    "        * Relevante: O KPI está relacionado a objetivos organizacionais mais amplos.\n",
    "        * Relacionado ao tempo: existe um prazo para atingir a meta.\n",
    "    * 5 - Quais serão as formas de visualização? Mapas? Quais serão os gráficos? Quais filtros (dimensões)? Mobile ou Computador?\n",
    "    * 6 - Qual o período será considerado na análise? Em cada formas de visualização.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 2</u> - Definição da Baseline**\n",
    "##### Baseline: Um ponto de partida, uma referência. São os resultados que a solução atual está trazendo para o problema de negócio em questão. É uma previsão do resultado com base na experiência e métricas atuais. \n",
    "* Realizar um levantamento com perguntas (Questionamento e Curiosidade) >>> Registrar as perguntas e respostas num documento e compartilha-la.\n",
    "* Levantamento de hipóteses (suposições) e descobrir como testa-las. \n",
    "* Revise os projetos anteriores/existentes que cobriam um \"terreno\" semelhante.\n",
    "    * Quais foram os principais resultados do projeto?\n",
    "    * Qualquer trabalho ou ativo pode ser reutilizado neste projeto?\n",
    "    * Que erros foram cometidos que devem ser evitados?\n",
    "* No final, será feito uma comparação entre a análise realizada (Conversão da perfomance da análise) x Baseline definida inicialmente para verificar se houve um ganho de valor do produto ou serviço.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 3</u> - Planejamento**\n",
    "* Criar um arquivo com as definições do planejamento do projeto >>> planejamento.txt \n",
    "    * Informar os pré-requisitos: Descrever quais pastas, recursos, arquivos serão criadas, que tipos de arquivos serão importados e etc.\n",
    "    * Realizar a decomposição do problema (Determine o escopo): Descrever os tópicos para resolver o problema de negócio (Usando pseudocódigos). Exemplo:\n",
    "        * Projeto: Crie uma interface de pagamento de cartão onde você recebe o valor a ser pago, tipo de pagamento, hash do cartão e senha para processar a transação. Ao finalizar, mostre na tela se foi bem sucedido ou uma mensagem de erro caso haja falhas.\n",
    "        * Verbos: Receber, Pocessar e Mostrar\n",
    "        * Substantivos: Valor a ser pago, Tpo de pagamento, Hash do cartão, Senha, Mensagem e Sucesso ou Falha\n",
    "        * Pseudocódigos: \n",
    "            * Input \"Receber os dados\" (Utilizando os substantivos: Valor do pagamento, Tipo de Pagamento, Hash do cartão e Senha)\n",
    "            * Função \"Processar Transação\"(Utilizando os inputs: Valor do Pagamento, Tipo de Pagamento e Hash do Cartão) \n",
    "            * Função \"Resposta Transação Finalizada\" (Utilizando o input da Senha e a Função \"Processar Transação\")\n",
    "            * Mostrar \"Mensagem\" (Utilizando os substativos: Mensagem e Sucesso ou Falha)\n",
    "    * Escrever um plano de projeto que estime o impacto, o esforço, os riscos, determine a responsabilidade de cada parte interessada e decide se deve ou não prosseguir com o projeto.\n",
    "    * Definir o marcos para etapas intermediárias, um cronograma para atingir cada marco e escrever uma breve descrição de cada etapa.\n",
    "    * Informar o produto final: Escolher como será a entrega do projeto (Defina uma estratégia de comunicação): \n",
    "        * E-mail, Planilha, Gráfico / Mapas, Dashbords, etc.\n",
    "        * Quais serão as entregas, os requisitos para essas entregas e o que não será incluído no projeto?\n",
    "    * Informar o processo: Escolher quais serão os passos para encontrar as respostas: Como serão feitas as próximas etapas. \n",
    "        * Separar os arquivos em partes (As funções criadas em um arquivo separado, por exemplo)\n",
    "        * Trabalhar de maneira reproduzível (Todas as etapas devem ser documentadas, com texto explicativo e claro (O que fez, por que fez isso e por que é importante) e utilizando as práticas recomendadas)\n",
    "        * Como fazer as etapas de revisão? Com que frequência os dados serão analisados? \n",
    "        * O que fazer ao identificar problemas no caminho? Como será feito recalculo de rota?\n",
    "        * Será criado relatórios que mostrem os andamentos das metas? \n",
    "    * Informar as ferramentas: Escolher quais serão as ferramentas utilizadas para encontrar as respostas: Linguagem da Programação, Ferramentas Estatística e o IDE.\n",
    "        * IDE: Jupyter Notebook Markdown\n",
    "        * Linguagem de Programação: Python e R\n",
    "        * Linguagem de Consulta: SQL\n",
    "        * Ferramentas Estatística: Python e R\n",
    "        * Visualização de dados: Power BI e Tableau\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 4</u> - Coleta de Dados Brutos de Fontes Confiáveis (Várias Fontes Internas e Externas)**\n",
    "##### Os dados separados em vários formatos >>> Tabela RAW (Tabela única)\n",
    "##### Técnicas: ETL (Extração, Transformação e Cargas), ELT (Extração, Cargas, Transformação) e Requisição em API \n",
    "##### Envolve coletar os dados de uma variedade de fontes, seja baixando manualmente ou programaticamente, copiando a web (Raspagem de sites), acessando uma API ou até mesmo conectando-se a um banco de dados. \n",
    "* Consultas em Banco de dados (SQL e NoSQL) ou em cloud: AWS, Google e Azure\n",
    "* Arquivos nos formatos: Excel, CSV, PDF, APIs etc.\n",
    "* Raspagem de sites (Web scraping): Biblioteca BeautifulSoup, Selenium, Requests, Urllib, Scrapy, Webbot, PyAutoGUI, Keyboard e Pynput \n",
    "* Power Query: Linguagem M\n",
    "* Bibliotecas: Regex, Mineração de Imagens, Áudio e Data/Hora\n",
    "    * Categoria de dados: <br>\n",
    "        a) Primário - Coletaram diretamente dos clientes. Geralmente são estruturados e organizados. \n",
    "        Ex: Pesquisa de satisfação do cliente, grupos focais, entrevistas, observação direta, sistema de gerenciamento de relacionamento com o cliente (CRM) ou dados de rastreamento transacional. <br>\n",
    "        b) Secundários - São dados primários de outras organizações. Pode estar disponíveis diretamente da empresa ou por meio de um mercado privado. \n",
    "        Ex: Site, aplicativo ou atividade de mídia social, como histórico de compras on-line ou dados de envio. <br>\n",
    "        c) Terceiros - Coletados e agregados de várias fontes por uma organização tercerizada. São dados não estruturados e em grande quantidade (Big data) \n",
    "        Ex: Repositórios de dados abertos, portais governamentais, relatórios do setor ou realizar pesquisas de mercado.\n",
    "        \n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 5</u> - Limpeza e Processamento de Dados**(Data Cleaning) \n",
    "#### Principal objetivo: Manter o máximo possível de um conjunto de dados intacto. Isso ajuda a melhorar a confiabilidade de seus insights.\n",
    "#### A manipulação ou tratamento se dá em: Dados Incompletos, Ausentes, Duplicados, Inconsistentes ou imprecisos (Dados desatualizados ou que contêm erros estruturais, como erros de digitação, capitalização inconsistente e convenções de nomenclatura irregular), Corrompidos, Formatados incorretamente, Excluir Dados Inúteis e Criar Novos Dados.    \n",
    "##### Tabela RAW (Tabela única) >>> Tabela Cleaned (Tabela limpa)\n",
    "##### Técnicas: Estatística descritiva, Tratamento de N/A (Valores ausentes), Balanceamento de dados e Identificação e tratamento de outliers (discrepantes)\n",
    "* Bibliotecas: Numpy, Pandas e Polars\n",
    "* Excel (Prototipação - é uma forma de visualizar a ideia antes mesmo de tirá-la do papel)\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 6</u> - Análise Exploratória de Dados**(Data Processing)\n",
    "##### Usando técnicas apropriadas que depende das perguntas feitas e do tipo de dados envolvidos, que geralmente procura padrões e tendências.\n",
    "##### Verificando a qualidade comum e problemas estruturais com os dados, avaliando-os visualmente ou programaticamente usando código estatísticos. Dois tipos principais: avaliação visual e avaliação estatística.\n",
    "##### Tabela Cleaned (Tabela limpa) >>> Validação de hipóteses, quebrar uma crença do time de negocio e gerar uma surpresa (Aplique duas ou mais técnicas de análise contra o conjunto de dados para testar as hipóteses.)\n",
    "##### Técnicas: Seleção e filtragem de variáveis, Feature engineering (Funcionalidades), Validação ou rejeição das hipotéses e Análise univariada, bivariada e multivariada\n",
    "* Análise estatística - Biblioteca Numpy\n",
    "* Análise gráfica - Bibliotecas: Matplotlib, Seaborn e Plotly\n",
    "* Levantamento de hipótese (Regressão Linear, Clusterização, Series Temporais, Árvores de Decisão, Random Forest, KNN, Redes Neurais, K-Means e Arima)\n",
    "\n",
    "##### OBS: Mudanças significativas nos números precisam ser alertados e investigadas e sempre comparadas com os dados de periodos anteriores.    \n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 7</u> - Preparação dos Dados (Modelagem dos dados)**(Data Modelling)\n",
    "##### Tabela Cleaned (Tabela limpa) >>> Training Data (Dados de treinamento)\n",
    "##### Técnicas: Encoding de variáveis categóricas, Transofmração de dados, Balanceamento de dados, Padronização e rescala e Seleção de Features (Funcionalidades)\n",
    "* Data Mining (Mineração de dados)\n",
    "* Criação das tabelas fatos e dimensões\n",
    "* Criação dos Indicadores e KPIS (Linguagem DAX) - Estudar o arquivo Indicadores e KPIS.ipynb (Observar qual área / setor, o nível organizacional (Estratégico, Tático ou Operacional), os pontos críticos, quais métricas são necessárias para a criação do indicador e se todas elas estão disponíveis, definir o período de acompanhamento do indicador e a periodicidade de atualização e realizar a criação dos indicadores e KPIS, analisando qual a fórmula utilizar)\n",
    "* Escolha dos tipos de Gráficos e Formatações, conforme nível organizacional, público, regras de negócios e indicadores\n",
    "* Scripts do Python e R\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Se for necessário realizar análises mais complexas, utilizando Machine Learning. Incluir as etapas abaixo: <br>\n",
    "#### a) Algoritmos de Machine Learning\n",
    "##### Training Data (Dados de treinamento) >>> Trained ML Models (Modelos de ML Treinados)\n",
    "##### Técnicas: Algoritmos de Classificação, Algoritmos de Regressão, Algoritmos de Clusterização, Redução de Dimensionalidade e Reinforcement Learning\n",
    "#### b) Metricas de Perfomance (Avaliação do Algoritmo): Knowledge representation (Representação do conhecimento)\n",
    "##### Trained ML Models (Modelos de ML Treinados) >>> Métrica de Perfomace\n",
    "##### Técnicas: Métricas de Classificação, Métricas de Regressão, Métricas de Clusterização e Impacto no negócio\n",
    "#### c) Modelo em Produção\n",
    "##### Trained ML Models (Modelos de ML Treinados) >>> Output API (Ambiente em nuvem)\n",
    "##### Técnicas: API e Tabelas de Banco de Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 8</u> - (Pré) Validação dos resultados com os clientes - Chegar a uma \"conclusão\" (Realizar as correções e os últimos ajustes)** (Data interpretation)\n",
    "\n",
    "<br>\n",
    "\n",
    "### **<u>Etapa 9</u> - Storytelling dos processos e resultados - Visualização, organização e ordenação de acordo com a importância (Compartilhamento)** (Data comunication)\n",
    "* Exibição dos Indicadores e KPIS \n",
    "* Criação dos Gráficos\n",
    "* Exibição de Report (Relatórios) e Dashboards (Painéis)\n",
    "* Criação de Dicionários de Dados\n",
    "* Recomendações acionáveis para o negócio, sobre quais devem ser os próximos passos da empresa.\n",
    "    * Analisar os Indicadores e KPIS \n",
    "\t* Analisar os resultados x metas\n",
    "\t* Analisar os desvios\n",
    "\t* Analisar a performance\n",
    "\t* Realizar uma avaliação final (Recomendações)\n",
    "* Quantificar o benefício para a organização se os objetivos forem alcançados. Se houver incerteza no cálculo, forneça um intervalo ou intervalo de confiança para os benefícios. Liste quaisquer benefícios qualitativos que não possam ser quantificados.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### OBS: Hospedagem dos Projetos: GitHub <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c73b34",
   "metadata": {},
   "source": [
    "# Qualidade dos Dados: \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102e577",
   "metadata": {},
   "source": [
    "É como descrevemos o estado de qualquer conjunto de dados. Ele mede elementos objetivos, como integridade, precisão e consistência e subjetivos, como a adequação de um conjunto de dados a uma tarefa específica. Uma medida da qualidade dos dados é quão bem eles foram limpos, mas o contexto também é um fator importante. Conjuntos de dados de alta qualidade para uma tarefa podem ser completamente inúteis para outra. Eles podem não ter observações importantes ou estar em um formato inútil para um trabalho diferente. <br> <br>\n",
    "Dados de qualidade inferior sempre produzirão resultados ruins, independentemente do modelo e da técnica de otimização usados. Portanto, uma grande quantidade de tempo precisa ser investida para garantir que a qualidade dos dados esteja de acordo com os padrões. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Características dos dados de qualidade: \n",
    "* 1 - Validade dos dados: É o grau em que um conjunto de dados está em conformidade com um formato definido ou conjunto de regras ou restrições. Ex: Tipo de dados, intervalo específico (Como Data), dados obrigatórios e padrões de expressão regular - são convenções pre-estabelecidas (Como o formato do telefone)\n",
    "* 2 - Precisão de dados: É uma medida simples de saber se seus dados estão corretos. \n",
    "* 3 - Completude dos dados: Identificar um conjunto de dados incompleto nem sempre é tão fácil quanto procurar células vazias. \n",
    "* 4 - Consistência dos dados: Refere-se se os dados correspondem a informações de outras fontes. Isso determina sua confiabilidade. Pode inferir quais dados estão corretos observando a entrada mais recente ou determinando a confiabilidade de alguma outra forma. Ex: Pode encontrar clientes com 2 números de telefone ou endereços.\n",
    "* 5 - Uniformidade de dados: Analisa unidades de medida, métricas, etc. para que os dados sejam úteis durante a análise, todas as medições devem ser uniformes. Converter tudo em uma única unidade.\n",
    "* 6 - Relevância dos dados: Verifica se os dados são suficientemente completos, uniformes, consistentes, etc. para cumprir sua determinada tarefa. \n",
    "    \n",
    "<br>\n",
    "\n",
    "### Práticas recomendadas para garantir qualidade:\n",
    "* 1 - Boa governança de dados: Inclui políticas e padrões de dados cuidadosamente elaborados, moldados com a contribuição da alta administração e de outras partes interessadas. Garantir a qualidade dos dados torna-se então um processo em vez de um trabalho autônomo. Também torna mais fácil identificar quando a qualidade dos dados se desvia dos padrões acordados. \n",
    "* 2 - Limpeza de dados: Como parte da \"limpeza\" do negócio, deve limpar os dados regularmente. Esse processo ajuda a identificar problemas a medida que surgem, ajuda a deduzir melhores maneiras de armazenar e coletar seus dados, melhorando sua qualidade geral desde o início. \n",
    "* 3 - Perfil de dados: A criação de perfil de dados envolve a análise de fontes de dados para coletar estatísticas ou insights. Uma análise dos dados em um nível estrutural mais profundo (isolado de seus usos pretendidos). Pode usa-lo para identificar técnicas de capturar com falhas ou pode ajuda-lo a determinar se a qualidade é alta o suficiente para uma determinada tarefa, antes de importa-los para um banco de dados.\n",
    "* 4 - Apoio interdepartamental: Estrutura, processo e comunicação.\n",
    "* 5 - Relátorios de qualidade de dados: Ajuda a medir os KPIS de qualidade de dados e moldará o registro de problemas de qualidade de dados. Ajudam a identificar temas comuns relacionados a como coletar, armazenar e processar os dados. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9bbf7",
   "metadata": {},
   "source": [
    "# Técnicas de Limpeza: \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6cf43",
   "metadata": {},
   "source": [
    "* 1 - Limpe observações indesejadas = Identificação e remoção de duplicatas e dados irrelevantes (Exclusão das colunas desnecessárias)\n",
    "* 2 - Correção de inconsistências ou imprecisos (Dados desatualizados ou que contêm erros estruturais) = Convenções de nomenclatura irregular, erros de digitação e capitalização inconsistente, como categoria ou rótulo incorretos, mesmo significado com nomes diferentes, letras maiúsculas, minúsculas, uso de sublinhados, travessão e outras pontuações e espaços em branco.\n",
    "* 3 - Padronizar os dados = Cada tipo de célula siga as mesmas regras - Ex: Colocar todos em letras maíusculas ou minúsculas ou colcar os dados numéricos usem a mesma unidade de medida.\n",
    "* 4 - Remoção de valores discrepantes indesejados = Remova apenas os outliers se puder provar que os dados diferem drasticamente de outros no conjunto, causando problemas na análise ou tipo de modelo usado.\n",
    "* 5 - Corrigir erros de dados contraditórios ou conjuntos cruzados = Onde tem um registro completo contendo dados inconsistentes ou incompatíveis. - Ex: Notas de alunos sendo associada a um campo que só permite opções para \"aprovado\" ou \"reprovado\".\n",
    "* 6 - Erros de conversão e sintaxe de tipo de dados\n",
    "* 7 - Lidar com dados ausentes <br>\n",
    "    * a) Remover as entradas associadas aos dados ausentes <br>\n",
    "    * b) Imputação de dados ausentes <br>\n",
    "    * c) Sinalizar os dados como ausentes ou \"0\" <br>\n",
    "* 8 - Dimensionamento de recursos = Refere-se à técnica em que dimensiona várias colunas (quantitativas) do seu conjunto de dados para uma escala comum. Para corrigir isso, usamos os dois métodos a seguir: \n",
    "    * Normalização \n",
    "    * Estandardização / Padronização\n",
    "* 9 - Codificação de dados = É onde codifica os recursos categóricos (colunas) do seu conjunto de dados em valores numéricos. \n",
    "* 10 - Validar o conjunto de dados = Regras ou rotina de validação (Verificar se o processo de correção, desduplicação, padronização e sinalização está completo. Envolve o uso de script que verifica o que predefiniu)     \n",
    "\n",
    "<br>\n",
    "\n",
    "#### OBS: “Outlier é qualquer valor em um conjunto de dados que se desvia drasticamente do resto dos pontos de dados.” "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5091af",
   "metadata": {},
   "source": [
    "# Técnicas de Imputação de Dados Ausentes:\n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9878f",
   "metadata": {},
   "source": [
    "#### - Dados faltando completamente ao acaso (Missing Completely At Random - MCAR)\n",
    "Os dados estão ausentes por projetos, devido a uma falha de equipamento ou por que as amostras são perdidas em trânsito ou tecnicamente insatisfatórias. A vantagem estatística de dados MCAR é que a análise permance imparcial, pois a falta não esta relacionada com o conjunto de dados X (Variável completamente observada) ou Y (Variável parcialemente perdida) e sim por outra razão.\n",
    "#### - Dados faltando ao acaso (Missing At Random - MAR)\n",
    "Os dados ausentes estão em uma variável parcialemente ausente (Y) e estão relacionados a alguma outra variável completamente observada (X) no modelo de análise. A falta se relaciona com a variável X, mas não com Y. Ela pode ter outros relacionamentos. Não está especificamente relacionado com a informação em falta. \n",
    "#### - Dados faltando não ao acaso (Missing Not At Random - MNAR)\n",
    "A falta está especificamente relacionada ao que está faltando, por exemplo, uma pessoa não comparece a um teste de drogas por que a pessoa tomou drogas na noite anterior. A unica maneira de obter uma estimativa imparcial dos parâmetros nesse caso é modelar os dados ausentes, mas isso requer compreensão adequada e conhecimento de dominio da variável ausente. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 1 - Exclusão de lista ou análise de caso completo (ou caso disponível)\n",
    "Omitir os casos com dados ausentes e analisar os dados restantes. Se houver uma amostra grande o suficiente, onde \"excluir\" não é um problema e a solução de MCAR for satisfeita, a exclusão pode ser uma estrategia razoavel. \n",
    "<img width=\"40%\" src=\"_imagens/Exclusão de lista.png\">\n",
    "<br>\n",
    "\n",
    "## 2 - Exclusão de pares\n",
    "Apenas as observações ausentes são ignoradas e a análise é feita nas variáveis presentes. A exclusão de pares é conhecida por ser menos tendências para os dados MCAR ou MAR. \n",
    "<img width=\"40%\" src=\"_imagens/Exclusão de pares.png\">\n",
    "<br>\n",
    "\n",
    "## 3 - Eliminando variáveis\n",
    "Se houver muitos dados faltando para uma variável, pode ser uma opção excluir a variável ou a coluna do conjunto de dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 4 - Imputação com média, mediana ou moda\n",
    "Substituir os dados perdidos por estimativas estatísticas. A base teórica da substituição da média é que a média é uma estimativa razoável para uma observação selecionada aleatoriamente de uma distribuição normal. No entanto, com valores ausentes que não são estritamente aleatórios, especialmente na presença de grande desigualdade no número de valores ausentes para as diferentes variáveis, o método de substituição média pode levar a um viés inconsistente. A distorção da variância original e a distorção da covariância com as variáveis restantes dentro do conjutno de dados são duas grandes desvantagens desse método. A mediana pode ser usada quando a variável tem uma distribuição assimétrica. A justificativa para a moda é substituir a população de valores omissos pelo valor mais frequente, uma vez que está é a ocorrência mais provável.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5 - Última observação realizada \n",
    "Se os dados forem dados de séries temporais e sempre que faltar um valor, ele é substituído pelo último valor observado. Este método é vantajoso, pois é fácil de entender e comunicar, porém assume fortemente que o valor do resultado permanece inalterado pelos dados perdidos, o que parece improvável em muitos cenários. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 6 - Próxima observação recuada\n",
    "Uma abordagem semelhante a técnica \"Última observação realizada\", porém funciona de forma oposta, pois pega a 1° observação após o valor ausente e carregando-a para trás.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 7 - Interpolação Linear\n",
    "É um método matemático que ajusta uma função aos dados e usa essa função para extrapolar os dados ausentes. O tipo mais simples é a linear, que significa entre os valores antes dos dados ausentes e o valor. Na Biblioteca do Pandas (Python) existem as opções: Linear, Time, Index, values, Neest, Zero, Slinear, Quadratic, Cubic, Polynomial, Spline e Polinômio por partes.\n",
    "<img width=\"40%\" src=\"_imagens/Interpolação Linear.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## 8 - Imputação de ponto comum (Adequado para valores ordinais)\n",
    "Para uma escala de classificação, usando o ponto médio ou o valor mais comumente escolhido. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 9 - Adicionando uma categoria para capturar N/A (Usados em variáveis categóricas)\n",
    "Este método consiste em trata os dados perdidos como um rótulo ou categoria adicional da variável. Todas as observações ausentes são agrupadas no rótulo recém criado \"Ausente\". Não assume nada sobre a falta dos valores. É muito adequado quando o número de dados ausentes é alto. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 10 - Atribuição de categoria frequente (Equivalente a imputação da média, mediana ou moda)\n",
    "Consiste em substituir todas as ocorrências de valores omissos dentro de uma variável pelo rótulo ou categoria mais frequente da variável. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 11 - Imputação de valor arbitrário (Ex. N° 0, 999, -999)\n",
    "Consiste em substituir todas as ocorrências de valores omissos dentro de uma variável por um valor arbitrário. Idealmente, o valor deve ser diferente da mediana, média e moda e não estar dentros dos valores normais da variável. Isso funciona razoavelmente bem para características numéricas predominantemente positivas em valores e para modelos baseados em árvores em geral. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 12 - Adicionando uma variável para capturar N/A\n",
    "Quando os dados não estão faltando completamente ao acaso, podemos capturar a importância da falta criando uma variável adicional indicando se os dados estavam faltando para aquela observação (1) ou não (0). Normalmente, a imputalçao média, mediana e moda é feita para adicionar uma variável para capturar as observações em que os dados estavam ausentes. \n",
    "<img width=\"30%\" src=\"_imagens/Adicionando uma variável para capturar n.a..png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## 13 - Imputação de Amostra Aleatória (Equivalente a imputação da média, mediana ou moda)\n",
    "A amostragem aleatória consiste em tomar uma observação aleatória do conjunto de observações disponíveis e usar esse valor extraído aleatoriamente para preencher o N/A. São feitas tantas observações aleatórias quantos os valores ausentes presentes na variável. Assume que os dados estão faltando completamente ao acaso. Se este for o caso, faz sentido substituir os valores omissos por valores extraídos da distribuição de variáveis original. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 14 - Imputação Múltipla\n",
    "O conceito chave é usar a distribuição dos dados observados para estimar um conjunto de valores plausíveis para os dados perdidos. Componentes aleatórios são incorporados a esses valores estimados para mostrar sua incerteza. Vários conjuntos são criados e analisados individualmente, mas de forma idêntica, para obter um conjunto de estimativas de parâmetros. As estimativas são combinadas para obter um conjunto de estimativas de parâmetros. O benefício é que a restauração da variabilidade natural dos valores perdidos incorpora a incerteza devido aos dados perdidos, o que resulta em uma inferência estatística válida. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 15 - Modelos preditivos / Estatísticos que imputam os dados ausentes\n",
    "* Regressão Linear = As variáveis existentes são usadas para prever e então o valor previsto é substituído como se fosse um valor realmente obtido. <br>\n",
    "* Floresta aleatória = Usa várias árvores de decisão para estimar valores ausentes e gera estimativas de erro de imputação OOB (Out Of The Bang). Funciona melhor com grandes conjuntos de dados. É um método não para métrica aplicável a vários tipos de variáveis que funcionam bem com dados ausentes aleatoriamente. <br>\n",
    "* K-NN = Imputa os valores de atributos ausentes com base no vizinho K mais próximo. Os vizinhos são determinados com base em uma medida de distância. Uma vez que K vizinhos são determinados, o valor ausente é imputado tomando a média, mediana ou moda de valores de atributos conhecidos do atributo ausente. <br>\n",
    "* Probabilidade máxima (Máxima verossimilhança) = A suposição de que os dados observados são uma amostra extraída de uma distribuição normal multivariada é relativamente fácil de entender. Depois que os parâmetros são estimados usando os dados disponíveis, os dados ausentes são estimados com base nos parâmetros que acabaram de ser estimados. <br>\n",
    "* Expectativa - Maximização = É o método de máxima verossimilhança usado para criar um novo conjunto de dados. Todos os valores omissos são imputados com valores estimados pelos métodos de máxima verossimilhança. <br>\n",
    "* Análise sensitiva = É definida como o estudo que define como a incerteza na saída de um modelo pode ser alocada as diferentes fontes de incerteza em suas entradas. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef310d73",
   "metadata": {},
   "source": [
    "# Principais Benefícios da Limpeza de Dados: \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731547a1",
   "metadata": {},
   "source": [
    "* Manter os dados organizados\n",
    "* Evitar erros\n",
    "* Melhorar a produtividade (Não perde tempo procurando informações em meio a dados ruins)\n",
    "* Evitando custos desnecessários\n",
    "* Mapeamento aprimorado (Dá a chance de corrigi-los antes que exijam uma correção mais demorada e cara)\n",
    "* Ter dados limpos torna muito mais fácil agrupar e mapear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd312b8f",
   "metadata": {},
   "source": [
    "# Tipos de Análises:\n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815417e9",
   "metadata": {},
   "source": [
    "### **1 - <u>Análise Descritiva:</u>** \n",
    "##### - Analisa o que aconteceu; \n",
    "##### - Análise baseadas em fatos; \n",
    "##### - Feitas a partir de resultados obtidos (passado);\n",
    "##### - Utilizada para orientar a construção de estratégias (Ponto de partida);\n",
    "##### - Ajudam a resumir ou descrever as características do conjutno de dados de maneira significativa. Além de detectar possíveis erros ou ocorrências estranhas no conjutno de dados. testar hipóteses iniciais. \n",
    "##### - Exemplos: Relatórios, Dashboards, Segmentação e controle de clientes, análises de negócios, aplicações de métricas e avaliação de resultados; \n",
    "    • Importe as bibliotecas necessárias: import pandas as pd, numpy as np e matplotlib.pyplot as plt\n",
    "    • Carregue os dados em um DataFrame do Pandas: df = pd.read_csv('caminho/do/arquivo.csv')\n",
    "    • Use o método describe() do Pandas para gerar estatísticas descritivas para todas as colunas numéricas do DataFrame: df.describe()\n",
    "    • Para gerar gráficos para visualizar a distribuição dos dados, pode usar a biblioteca matplotlib. \n",
    "        Para criar um histograma: \n",
    "            plt.hist(df) \n",
    "            plt.xlabel('NomeDaColunaX') \n",
    "            plt.ylabel('NomeDaColunaY') \n",
    "            plt.show() \n",
    "        Para gerar um boxplot:\n",
    "            plt.boxplot(df) \n",
    "            plt.show() \n",
    "<br>\n",
    "\n",
    "### **2 - <u>Análise Diagnóstica:</u>** \n",
    "##### - Analisa o por que aconteceu; \n",
    "##### - É concentrada em algo que já aconteceu;\n",
    "##### - Objetivo é encontrar relação de causa e efeito para entender um acontecimento, identificar e responder anomalias para chegar a causa raiz ou verificar resultados mais atipicos; \n",
    "##### - Processo é baseado em probabilidades (Fatores internos e externos) : análise de regressão, séries temporais e teoria da probabilidade; <br>\n",
    "### **3 - <u>Análise Preditiva:</u>**  \n",
    "##### - O que vai acontecer\n",
    "##### - Sua essência está na previsão de cenários futuros com base na análise de padrões revelados pela base de dados passados;\n",
    "##### - Usam bancos de dados, algoritmos estátisticos e técnicos de machine Learning para criar suas previsões;\n",
    "##### - O objetivo é determinar uma tendencia, correlação, causa ou probabilidades;\n",
    "##### - Podemos prever o que deve acontecer se determinadas condições se cumprirem;\n",
    "##### - Exemplos: Detectar fraude (Detectar padrões e previnir comportamentos criminosos), otimizar campanhas de marketing (Usada para determinar as respostas ou compras dos clientes, além de promover oportunidades de vendas cruzadas), melhorar operações (Definição de preços das companhias aéreas) e reduzir riscos (Pontuações de crédito usadas para avaliar a probabilidade de padrões de compra) <br>\n",
    "### **4 - <u>Análise Prescritiva:</u>** \n",
    "##### - O que fazer quando acontecer\n",
    "##### - Prescrição é uma recomendação a algo potencialmente previsto e está diretamente ligada as estratégias de negócios;\n",
    "##### - Utiliza as projeções (Análise preditiva) e direciona esforços para se obter o melhor resultado a partir das possibilidades;\n",
    "##### - Por ser constantemente mutável, os modelos analítico prescritivos são apoiados pela IA, Algoritmos e Machine Learning;\n",
    "##### - O principal objetivo é fornecer subsídios para a tomada de decisões que irão alterar o futuro.\n",
    "##### - As ferramentas ajudam a fazer sugestões com base em padrões diferenciados e percepções de objetivos organizacionais, limitações e fatores de influência. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c6a82",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados (EDA):\n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e01f46",
   "metadata": {},
   "source": [
    "##### - É uma investigação inicial do conjunto de dados, onde procura entender e resumir suas principais características. Entender como são estruturados, identificar padrões e tendências em potencial e detectar quaisquer anomalias. Além de determinar se os métodos de análise que planeja usar posteriormente são realmente apropriados para o conjunto de dados.  <br>\n",
    "##### - Tem como objetivo: Conhecer a situação dos dados. Se familiarizar com dados. Adquirir um conhecimento (ato ou efeito de conhecer/ato de perceber ou compreender por meio da razão e/ou da experiência) <br>\n",
    "##### - É um processo de exploração e descoberta de padrões e relações por meio de técnicas estatísticas e visualizações relevantes. O objetivo é entender melhor os dados e obter insights valiosos que possam orientar futuras análises.(Geração de insights) <br>\n",
    "##### - Geralmente é realizada antes que uma hipótese firme ou um objetivo final seja definido. <br>\n",
    "### - <u>Etapas:</u>\n",
    "* 1 - Obter os dados (Seleção de dados)\n",
    "* 2 - Verificar a integridade dos dados (Integração de dados - Dados ausentes, inconsistências ou erros)\n",
    "* 3 - Limpar os dados (Pode incluir preenchimento de valores ausentes, correção de erros e remoção de duplicatas.)\n",
    "* 4 - Explorar as variáveis (Explorar a distribuição das variáveis, variabilidade e relações entre elas, pode ser feito por meio de tabelas de frequência, gráficos de barras e dispersão)\n",
    "* 5 - Identificar padrões e tendências (Analisar as variáveis e procurar por padrões e tendências. Utilização dos tipos de análises)\n",
    "* 6 - Visualizar os dados (Ajuda a comunicar os padrões e tendências identificados - representação do conhecimento)\n",
    "* 7 - Transformação de dados (Sumarizar as principais conclusões da análise, destacando as descobertas mais importantes e sugerindo possíveis próximos passos)\n",
    "    \n",
    "<br>\n",
    "\n",
    "### - <u>Tipos:</u>\n",
    "* Análise Uni-variada = Analisa a distribuição de uma única variável ou coluna de dados por vez, geralmente por meio de Histogramas, Tabelas, Gráficos de caixa, pizza e barras. \n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise Bivariada = Análise da relação entre duas variáveis, geralmente por meio de gráficos de dispersão e linha, coeficientes de correlação e outras visualizações.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise Multivariada = Análise da relação entre três ou mais variáveis, geralmente por meio de gráficos de dispersão em 3D, análise de componentes principais e outras técnicas estatísticas.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Cluster = Agrupamento de observações semelhantes em grupos distintos com base em suas características comuns. É uma técnica exploratória que busca identificar estruturas dentro de um conjunto de dados. O objetivo é classificar diferentes pontos de dados em grupos (cluster) que são internamente homogêneos e externamente heterogêneos. O clustering é usado para obter informações sobre como os dados são distribuidos em um determinado conjunto de dados ou como uma etapa de pré-processamento para outros algoritmos. \n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Tendências = Identificação de tendências temporais ou sazonais em uma série de dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Outliers = Identificação de observações incomuns ou extremas que podem afetar a análise. É um ponto de dados que difere significativamente de outras observações em seu conjunto de dados. Portanto, é crucial entender e detectar valores discrepantes, como também determinar a melhor maneira de trata-los. Algumas causas comuns de outlier incluem:\n",
    "    * Erro humano ao inserir os dados (Por exemplo, um erro de digitação)\n",
    "    * Outliers intencionais, ou seja, valores fictícios que testam métodos de detecção\n",
    "    * Erros de amostragem como resultado da extração ou combinação de dados de várias fontes\n",
    "    * Outliers naturais é quando os outliers ocorrem \"naturamente\" nos dados, em vez de serem o resultado de um erro. Os outliers naturais são chamados de novidades (News). \n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Dados Faltantes = Identificação e tratamento de dados faltantes em uma análise.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Redes Sociais \n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Textos e Processamento de Linguagem Natural\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Série Temporal = É uma técnica estatística usada para identificar tendências e ciclos ao longo do tempo. São uma sequência de pontos de dados que medem a mesma variável em diferentes momentos. Ao observar as tendências relacionadas ao tempo, podem prever como a variável de interesse pode flutuar no futuro. Os principais padrões procurados são tendências (Aumento ou diminuição estáveis e lineares durante um período prolongado), sazonalidade (Flutuações previsíveis nos dados devido a fatores sazonais em um curto período de tempo) e padrões cíclicos (Ciclos imprevisíveis onde os dados flutuam e podem ocorrer como resultado de condições econômicas ou relacionadas ao setor). Classificados em autorregressivos, integrados e média móvel. \n",
    "\n",
    "        • Importe as bibliotecas necessárias: \n",
    "            import pandas as pd, numpy as np e matplotlib.pyplot as plt\n",
    "            from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "            from statsmodels.tsa.arima.model import ARIMA\n",
    "        • Carregue os dados em um DataFrame do Pandas e defina uma coluna de Data como o índice: df = pd.read_csv('caminho/do/arquivo.csv', index_col='ColunaDeData', parse_dates=True)\n",
    "        • Verifique o intervalo de datas dos dados usando o atributo index do DataFrame: print(df.index.min(), df.index.max())\n",
    "        • Para visualizar os dados em um gráfico de linhas, use o método plot() do Pandas:\n",
    "            df.plot(figsize=(N°,N°))\n",
    "            plt.ylabel('NomeDaColunaComValor')\n",
    "            plt.title('TítuloDoGráfico')\n",
    "            plt.show()\n",
    "        • Para decompor a série temporal em suas componentes de tendência, sazonalidade e ruído, pode usar o método seasonal_decompose() da biblioteca statsmodels:\n",
    "            result = seasonal_decompose(df, model='additive')\n",
    "            result.plot()\n",
    "            plt.show()\n",
    "        • Para ajustar um modelo de previsão a uma série temporal, pode usar a biblioteca statsmodels. Por exemplo, para ajustar um modelo ARIMA à série temporal:\n",
    "            model = ARIMA(df, order=(1, 1, 1))\n",
    "            result = model.fit()\n",
    "            print(result.summary())\n",
    "            \n",
    "<br>\n",
    "\n",
    "* Análise de Regressão = É usada para estimar a relação entre um conjunto de variáveis. Se existe ou não uma relação e não dizem nada sobre causa e efeito, apenas sugerem que um afeta o outro, mas precisa de outras análises para tirar conclusões definitivas. O objetivo é estimar como uma ou mais variáveis independentes podem impactar a variável dependente (É a variável ouu resultado que deseja medir ou prever), a fim de identificar tendências e padrões para realizar previsões e prever tendências futuras.\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise Fatorial = É uma técnica utilizada para reduzir um grande número de variáveis a um número menor de fatores. É útil por que condensa grandes conjuntos de dados em amostras menores e mais gerenciáveis, mas também porque ajuda a descobrir padrões ocultos, como conceitos que não podem ser facilmente medidos ou observados (Ex. Fidelidade e satisfação do cliente). Esses fatores são então levados adiante para uma análise mais aprofundada, permitindo que aprenda mais sobre seus clientes. Funciona encontrado itens fortemente correlacionados (Covariância) para agrupa-las em fatores que pertencem um ao outro (Experiência do cliente positiva com probabilidade de recomendar o produto positiva podem ser reduzidos a único fator \"satisfação\" do cliente)\n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Coorte = É um grupo de pessoas que compartilham uma característica (ou ação) comum durante um determinado período de tempo. Examinando o comportamento do cliente no contexto do ciclo de vida do cliente para identificar padrões em vários pontos da jornada. Permite que as empresas personalizem seus serviços para segmentos específicos de clientes otimizando as ofertas.  \n",
    "\n",
    "<br>\n",
    "\n",
    "* Análise de Sentimentos = É uma técnica que pertence a categoria mais ampla de análise de texto (Dados qualitativos). O processo de classificação e compreensão de dados textuais. O objetivo é interpretar e classificar as emoções transmitidas nos dados textuais sobre marca, produto ou serviço. Classificados em refinada (Positiva, neutra e negativa), detecção de emoção (Identificar palavras associadas a emoções) e análise de sentimentos baseada em aspectos (Permite identificar quais aspectos específicos as emoções ou opiniões se relacionam)\n",
    "\n",
    "<br>\n",
    "\n",
    "* Simulação ou método de Monte Carlo = É uma técnica computadorizada usada para gerar modelos de resultados possíveis e suas distribuições de probabilidade. Usando para realizar análises de riscos avançadas, permitindo que prevejam melhor o que pode acontecer no futuro e tomem decisões de acordo. Substitui todos os valores incertos (entrada) por funções que geram amostras aleátorias de distribuição determinadas e em seguida, executando uma série de cálculos e recálculos para produzir modelos de todos os resultados possíveis e suas distribuições de probabilidade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc69382",
   "metadata": {},
   "source": [
    "# Principais Benefícios da EDA:\n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf88b69",
   "metadata": {},
   "source": [
    "* Detectar dados ausentes e incorretos: Identificar qualquer problema estrutural, reprocessar os dados ou coletar novos dados inteiramente.\n",
    "* Entender a estrutura subjacente dos dados: Mapeamento adequado dos dados para manter a alta qualidade ao transferi-los de sua origem para o banco de dados. \n",
    "* Testar hipoteses e verificar suposições: Ajudará a identificar se está inferindo os resultados corretos com base na compreensão dos dados. Caso contrário, sabe que as suposições estão erradas ou que está fazendo as perguntas erradas sobre o conjunto de dados.\n",
    "* Calcular as variáveis: Quais variáveis independentes afetam quais variáveis dependentes? Determinar as variáveis desde o início o ajudará a extrair as informações mais úteis e como se relacionam entre si.\n",
    "* Criar o modelo mais eficiente: Remove qualquer informação irrelevante\n",
    "* Determinar as margens de erro: Determinar quais dados podem levar a erros inevitáveis em sua análise posterior. \n",
    "* Identificar as ferramentas estatísticas mais adequadas: Determinar quais técnicas e modelos estatísticas o ajudarão a obter o que precisa do conjunto de dados. Ex: Estatística Descritiva como o mínimo, quartil inferior, mediana, quartil superior e máximo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10efb7a9",
   "metadata": {},
   "source": [
    "# Versão 1.1 (30/03/2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
