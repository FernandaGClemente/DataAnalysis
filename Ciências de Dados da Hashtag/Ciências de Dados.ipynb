{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7649a2b5",
   "metadata": {},
   "source": [
    "<center><img width=\"25%\" src=\"https://www.camaramirimdoce.sc.gov.br/media/noticia/resumo-da-sessao-10-06-2019-111.png?w=848&h=450&t=P&c=f0f0f0&q=80&v=2\"></center>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# Ciências de Dados\n",
    "#### Hashtag Treinamentos\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d22a9",
   "metadata": {},
   "source": [
    "### <u>O que é Data Science?</u><br>\n",
    "#### A ciência de dados tem a mesma função do BI dentro de uma empresa, no entanto, a abordagem utilizada para a interpretação desses dados é diferente. Converter dados brutos em insights de negócios que os líderes empresariais e gestores possam usar para tomar decisões baseadas em dados. As empresas viram a disponibilidade de grandes volumes de dados como fonte de vantagem competitiva. As empresas que conseguem utilizar esses dados de forma eficaz tomam melhores decisões e estão à frente da curva de crescimento. Para dar sentido a tais dados, surgiu a necessidade de um novo conjunto de habilidades que incluísse a habilidade para definir e compreender problemas de negócio, habilidades analíticas, habilidades de programação, habilidades estatísticas, habilidades de aprendizado de máquina, visualização de dados e muito mais.<br>\n",
    "#### Teremos a presença de um profissional multidisciplinar conhecido como o cientista de dados. Que dentre as suas especialidades estão a matemática, a estatística, a programação, o conhecimento básico acerca do negócio, já que ele deverá analisar as informações com base no que a empresa espera descobrir e resolução de problemas para capturar dados de maneiras engenhosas, com capacidade de olhar os dados de forma diferente para encontrar padrões, juntamente com as atividades de limpeza, preparação e organização dos dados. Esses dados podem ser Estruturados e Não-Estruturados. <br>\n",
    "* Análise Quantitativa – modelagem matemática, análise estatística, previsões e simulações.\n",
    "* Habilidades em Programação – habilidades em programação para analisar dados brutos e torná-los acessíveis aos usuários de negócio.\n",
    "* Conhecimento do Negócio – conhecimento do ambiente de negócio, para melhor compreender a relevância dos resultados encontrados.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Por meio da análise de um conjunto específico de dados coletados junto à empresa, o cientista de dados se utiliza de métodos científicos para analisar e gerar conhecimentos valiosos para a organização, por meio da exploração de padrões e anomalias no conjunto de informações coletadas. <br>\n",
    "\n",
    "### <u>Principais aplicações:</u><br>\n",
    "#### Assim como no BI, o Data Science tem como principal objetivo auxiliar o gestor a identificar determinadas oportunidades ou riscos para as operações, sendo assim tomado como uma atividade estratégica para o desenvolvimento do negócio. No entanto, o Data Science é muito mais flexível, podendo ser utilizado para avaliar e verificar as ocorrências dentro de conjuntos de dados internos e externos da empresa, podendo analisar riscos e oportunidades fora do ambiente do negócio. <br>\n",
    "#### Cientistas de Dados muitas vezes empregam o método científico para a exploração de dados, formação de hipóteses e testes de hipóteses (através de simulação e modelagem estatística). Cientistas de Dados com foco de negócios podem usar técnicas de aprendizado de máquina (Machine Learning) para encontrar padrões e obter insights de grandes conjuntos de dados que estão relacionados com uma linha de negócio específica ou o negócio em geral. Eles são qualificados em matemática, estatística e programação, e usam essas habilidades para gerar modelos preditivos.<br> \n",
    "#### Conjuntos de dados internos e externos: Você pode criar dados corporativos MASHUPS a partir de fontes internas e externas de dados estruturados e não estruturados com bastante facilidade (DADOS MASHUP é uma combinação de duas ou mais fontes de dados, que são analisadas em conjunto, a fim de fornecer aos usuários uma visão mais completa da situação em foco). <br>\n",
    "* Dados Transacionais – Uma fonte de dados de negócio transacional é o mesmo tipo de dados estruturados utilizados em BI tradicional e inclui dados de gerenciamento, atendimento ao cliente, vendas e dados de marketing, dados operacionais e dados de desempenho do empregado.\n",
    "* Dados de Mídias Sociais – dados não estruturados gerados a partir de e-mails, fóruns, blogs e redes sociais como Twitter, Facebook, LinkedIn, Pinterest e Instagram.\n",
    "* Dados de Máquinas e Operações de Negócio – dados não estruturados gerados automaticamente por máquinas, tal como dados de sensores de automóveis, por exemplo.\n",
    "* Dados de áudio, video, imagem e arquivos pdf – fontes de dados comuns e bem estabelecidas.\n",
    "\n",
    "<br>\n",
    "\n",
    "### <u>Conhecimentos necessários:</u><br>\n",
    "* Conhecimento aprofundado de SAS e/ou R. Para Data Science, R é geralmente preferido.\n",
    "* Codificação Python: Python é a linguagem de codificação mais comum que é usado na ciência dos dados, juntamente com Java, Perl, C / C ++.\n",
    "* Plataforma Hadoop: Embora nem sempre seja um requisito, é importante saber que a plataforma Hadoop é preferida para a área. A experiência em Hive ou Pig é uma grande vantagem.\n",
    "* Banco de dados/Codificação SQL: Embora o NoSQL e o Hadoop sejam o foco principal para cientistas, os candidatos preferenciais podem escrever e executar consultas complexas em SQL.\n",
    "* Trabalhando com dados não estruturados: é extremamente importante que um cientista possa trabalhar com dados não estruturados, seja de mídias sociais, feeds de vídeo, áudio ou outras fontes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### <u>Diferença entre Analista de BI e Cientista de dados</u><br>\n",
    "#### Um Cientista de Dados deve ter um forte senso de negócios e a capacidade de comunicar efetivamente conclusões baseadas em dados para os tomadores de decisão das empresas. Um Cientista de Dados não apenas abordará problemas de negócios, ele também escolherá os problemas certos que tenham mais valor para a organização. O trabalho de um Analista de BI é encontrar padrões e tendências nos dados históricos de uma organização. Enquanto o BI seja amplamente baseado na exploração de tendências passadas, a Ciência de Dados consiste em encontrar preditores e o significado dessas tendências. Assim, o objetivo principal de um Analista de BI é avaliar o impacto de certos eventos nas operações cotidianas de uma empresa ou comparar o desempenho de uma empresa com o de outras empresas no mesmo mercado. O Cientista de Dados é encarregado de avaliar como esses eventos impactam o futuro da empresa! <br>\n",
    "#### E as ferramentas que ambos utilizam também são diferentes, já que Analistas de BI e Cientistas de Dados possuem objetivos diferentes a partir da análise de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1753ae6",
   "metadata": {},
   "source": [
    "# **Sumário:**\n",
    "1. [Introdução](#Introdução)\n",
    "    * Conceito\n",
    "    * Método científico\n",
    "    * Framework\n",
    "2. [Biblioteca Pandas](#Biblioteca-Pandas)\n",
    "    * Conceito\n",
    "    * Códigos\n",
    "3. [Biblioteca NumPy](#Biblioteca-NumPy)\n",
    "    * Conceito\n",
    "    * Códigos\n",
    "4. [Introdução a Estatística](#Introdução-a-Estatística)\n",
    "    * Conceito\n",
    "    * Códigos\n",
    "5. [Biblioteca Matplotlib](#Biblioteca-Matplotlib)\n",
    "    * Conceito\n",
    "    * Códigos\n",
    "6. [API: Consumo e Tratamento de Dados de Fontes Externas](#API:-Consumo-e-Tratamento-de-Dados-de-Fontes-Externas)\n",
    "    * Conceito\n",
    "    * Passo a passo de como utilizar uma API\n",
    "    * O que é JSON?\n",
    "    * O que é endpoint da API?\n",
    "    * O que é API RESTful?\n",
    "7. [Introdução ao Aprendizado de Máquinas](#Introdução-ao-Aprendizado-de-Máquinas)\n",
    "    * Conceito\n",
    "    * Tipos de aprendizado\n",
    "    * Escolher o melhor modelo de ML\n",
    "8. [Biblioteca Scikit-Learn](#Biblioteca-Scikit-Learn)\n",
    "    * Conceito    \n",
    "    * Introdução da Regressão Linear\n",
    "    * Introdução do Classificador Linear Perceptron\n",
    "    * Modelo de classificação\n",
    "        * Avaliação do modelo: Matriz de confusão\n",
    "        * Avaliação do modelo: Acurácia\n",
    "        * Avaliação do modelo: Precisão \n",
    "        * Avaliação do modelo: Recall\n",
    "    * Separando os dados em Treino e Teste (Módulo train_test_split)\n",
    "    * Introdução do Modelo Árvore de Decisão\n",
    "    * Introdução do Modelo Regressão Logística\n",
    "9. [Análise Exploratória de dados](#Análise-Exploratória-de-dados)\n",
    "    * Passo a passo\n",
    "    * Visualização do KDE (Kernel Density Estimation)\n",
    "    * Heatmap de correlação\n",
    "    * Pandas Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6077a6",
   "metadata": {},
   "source": [
    "### **Introdução** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580de72e",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "#### É o processo de exploração, manipulação e análise dos dados para a descoberta e previsão, através da criação de hipóteses, testes e validação (Embasamento estatístico e matemático) com o objetivo de responder perguntas do negócio e/ou fazer recomendações capazes de serem diferenciais de negócio, além de poder ser feito de forma escalável e replicável. \n",
    "1. Dados (Armazenamento, Processamento e Visualização. Informações brutas, registros de ocorrência, observações.)\n",
    "2. Informações (Dados processados, com contexto e significado, possuem relevância e proposito)  \n",
    "3. Conhecimento (Informação após a interpretação, requer análise, entendimento, avaliação e contexto) \n",
    "4. Inteligência (Conhecimento aplicado, analise para tomada de decisão) \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Existe um `Método Científico`: Observação → Hipóteses → Experimento (Teste, validação e monitoramento) → Análise/Resultado → Conclusão.\n",
    "\n",
    "<br>\n",
    "\n",
    "### <u>Framework (Conjuntos de passos):</u> <br>\n",
    "#### CRISP-DM (CROSS INDUSTRY STANDARD PROCESS FOR DATA MINING) \n",
    "1. Entendimento do negócio: Definição do problema + Alinhamento de expectativas (Previsão de Erros, Prazos, se é viável ou não, Indicadores, Definição do que é o \"pronto\") \n",
    "2. Entendimento dos dados \n",
    "3. Engenharia de dados (Caso esteja faltando os dados → Saber como fazer as inúmeras formas de busca) \n",
    "    * a) Abra o prompt de comando do Anaconda\n",
    "    * b) Entrar na pasta do projeto → UTilizando o comando cd\n",
    "    * c) Criar o Ambiente Virtual → conda create -n \"nome do meu ambiente\" python (Só precisa fazer esse passo uma única vez)\n",
    "    * d) Ativar o Ambiente Virtual → conda activate \"nome do meu ambiente\"\n",
    "    * e) Instalação do IDE → py -m pip install jupyter\n",
    "    * f) Abrindo o IDE → py -m jupyter notebook\n",
    "    * g) Instalação as bibliotecas necessárias no Ambiente Virtual\n",
    "        * Abra outro pront de comando do Anaconda e ative o Ambiente Virtual\n",
    "        * conda install pip ou conda install \"Biblioteca\"\n",
    "        * pip install \"Biblioteca\"\n",
    "    * h) Reinicie o IDE\n",
    "    * i) Importe a base para o python\n",
    "4. Preparação dos dados: Realização de um pré-processamento, como validar se os dados vieram corretos, se as frequências que recebemos os dados está sendo suficientes, se existe dados corrompidos, valores vazios, com tipos incorretos, saber quais são as fontes e formatos, se recebemos todos os dados, quais colunas ou bases não poderemos considerar. (Power Query – Linguagem M) > Limpeza\n",
    "5. Análise/Modelagem: \n",
    "    * a) Análise exploratória (Entendemos as relações, limites, outliers, tendências e padrões - Respondendo as perguntas) \n",
    "    * b) Tratamento dos dados (Os dados já estão no modelo e já foram analisados de forma exploratória, com isso podemos incluir novas colunas, criar variáveis explicativas, selecionar variáveis, criar agrupamentos, combinações e derivações dos dados. Utilizar técnicas para normalizar valores, tratar valores discrepantes, não deixar que informações escalares afetem o modelo, retirar ruídos desnecessários. (Power BI – Linguagem DAX)) \n",
    "    * c) Definição do modelo (Conjunto de regras para realizar determinada ação para solucionar o problema de negócio)\n",
    "6. Validação do modelo (Validar o modelo com outros valores X) \n",
    "7. Preparação / Visualização: Análise Explanatória (Entendemos as conclusões, evidências e sofisticação) e a realização de uma checagem/Feedback com o cliente (Implementação do modelo X Voltar ao entendimento do negócio) + Apresentação do resultado: Gráficos, emails, relatários\n",
    "    * a) Criação de Dashboard (Comparado ao que? Bom ou Ruim? / Melhores e Piores? Métricas corretas? Tem apenas um tema? Tamanho, Resolução, Proporção, excesso ou falta de níveis)\n",
    "    * b) 7 elementos em cada Dashboard\n",
    "    * c) 20 linhas em cada gráfico\n",
    "    * d) Formas de organizar a informação (Localização, alfabeticamente, tempo, categoria e hierarquia)\n",
    "##### Melhoria contínua, monitoramento e ajustes no modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a648892",
   "metadata": {},
   "source": [
    "### **Biblioteca Pandas** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50981610",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "#### O Pandas é a biblioteca inicial do processo de Ciencia de Dados. \n",
    "* Os DataFrames lembram o Excel e é usado desde a importação da base até a criação do modelo, incluindo a análise e visuallização dos dados, tratamentos, agregações, etc.\n",
    "* Instalação da biblioteca → pip install pandas\n",
    "* Cada coluna em uma Dataframe é um Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40840f5b",
   "metadata": {},
   "source": [
    "### <u>Códigos</u> <br>\n",
    "#### - Importação do Pandas\n",
    "    import pandas as pd\n",
    "<br>\n",
    "\n",
    "##### - Leitura de dados tabulares (Extensões: csv, xlsx, html, hdf5, json, gbq, sql, parquet, etc)\n",
    "    dataframe = pd.read_extensoes(r'Caminho/arquivo.extensao')\n",
    "<br>\n",
    "\n",
    "##### - Gravação de dados tabulares (Extensões: csv, xlsx, html, hdf5, json, gbq, sql, parquet, etc)\n",
    "    pd.to_extensoes(r'Caminho/arquivo.extensao')\n",
    "<br>\n",
    "\n",
    "##### - Transformando um dicionário em DataFrame (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
    "    dataframe = pd.DataFrame(dict) \n",
    "<br>\n",
    "\n",
    "##### - Transformando uma coluna em DataFrame em Series (Coluna = Série e ambas possuem indice e valores)\n",
    "    series = dataframe['NomeDaColuna']\n",
    "    Com os valores da coluna → series.values ou dataframe['NomeDaColuna'].values\n",
    "    Com os valores dos indices → series.index ou dataframe['NomeDaColuna'].index\n",
    "<br>\n",
    "\n",
    "##### - Verificando o tipo de dado (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html)\n",
    "    Tipo de cada coluna → dataframe.dtypes\n",
    "    Tipo do objeto → type(dataframe)  \n",
    "<br>\n",
    "\n",
    "##### - Visualização do DataFrame (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)\n",
    "    5 primeiras linhas → dataframe.head() \n",
    "    Nr primeiras linhas → dataframe.head(Nr) \n",
    "    5 últimas linhas → dataframe.tail() \n",
    "    Nr últimas linhas → dataframe.tail(Nr) \n",
    "    Algums colunas → dataframe[['NomeDaColunaA', 'NomeDaColunaZ']] \n",
    "<br>\n",
    "\n",
    "##### - Informações do DataFrame (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)\n",
    "    dataframe.info() \n",
    "<br>\n",
    "\n",
    "##### - Filtrando apenas alguns dados (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html e https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html)\n",
    "    dataframe.loc[ FiltroDaLinha, FiltraDaColuna ]\n",
    "    FiltroDaLinha → dataframe[Coluna] OperadorDeComparação Nr \n",
    "        (Ex: [base['Salario'] > 1000), ['NomeDaColuna',...] ou :) \n",
    "    FiltroDaColuna → dataframe[Coluna] OperadorDeComparação Nr \n",
    "        (Ex: Vazio ou :, [base['Salario'] > 1000])\n",
    "##### ou \n",
    "    dataframe.iloc[ IndiceDaLinha, IndiceDaColuna ]\n",
    "##### ou \n",
    "    dataframe[FiltroDaLinha].NomeDaColuna\n",
    "<br>\n",
    "\n",
    "##### - Operações matemática de agregação\n",
    "    Média: dataframe[NomeDaColuna].mean() \n",
    "    Mínimo: dataframe[NomeDaColuna].min() \n",
    "    Máximo: dataframe[NomeDaColuna].max() \n",
    "    Contagem: dataframe[NomeDaColuna].count() \n",
    "    Mediana: dataframe[NomeDaColuna].median() \n",
    "    Desvio padrão: dataframe[NomeDaColuna].std() \n",
    "    Variância: Desvio padrão ** 2\n",
    "<br>\n",
    "\n",
    "##### - Contando os valores vazios de cada coluna (True = 1 e False = 0) (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html)\n",
    "    dataframe.isnull().sum() \n",
    "<br>\n",
    "\n",
    "##### - Contando a quantidade de valores únicos de cada coluna (Cardinalidade alta, significa muitos elementos distintos e provavelmente, a coluna não será uma boa opção para utilizar no modelo)\n",
    "    dataframe.nunique()\n",
    "<br>\n",
    "\n",
    "##### - Contando os valores que não estão vazios de cada coluna (True = 1 e False = 0) (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.notnull.html)\n",
    "    dataframe.notnull().sum() \n",
    "<br>\n",
    "\n",
    "##### - Contando os valores de uma coluna (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html)\n",
    "    dataframe[NomeDaColuna].value_counts()\n",
    "<br>\n",
    "\n",
    "##### - Exibe um resumo estatístico de cada coluna numérica (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)\n",
    "    dataframe.describe()\n",
    "<br>\n",
    "\n",
    "##### - Exclusão de linhas ou colunas (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)\n",
    "    Excluir coluna → dataframe.drop(NomeDaColuna, axis=1, inplace=True) \n",
    "        (Ex: Excluir a coluna A: dataframe = dataframe.drop('ColunaA', axis=1, inplace=True) \n",
    "    Excluir várias colunas → dataframe.drop({Lista de Colunas}, axis=1, inplace=True) \n",
    "        (Ex: Excluir as colunas A e B: dataframe.drop({'ColunaA', 'ColunaB'}, axis=1, inplace=True))\n",
    "    Excluir linha → dataframe.drop(Nr_Indice, axis=0, inplace=True) \n",
    "        (Ex: Excluir a linha de indice 1: dataframe = dataframe.drop(1, axis=0, inplace=True)) \n",
    "    Excluir várias linhas → dataframe.drop([Lista de indices], axis=0, inplace=True) \n",
    "        (Ex: Excluir as linhas de indices 1 e 2: dataframe = dataframe.drop([1,2], axis=0, inplace=True)) \n",
    "<br>\n",
    "\n",
    "##### - Renomear de linhas ou colunas (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)\n",
    "    Renomear linha → dataframe.rename(index=None, columns=None, axis=None, inplace=False) \n",
    "        (Ex: Renomear a linha 1: dataframe.rename(index={1:'NomeNovo'}, axis=0, inplace=True) \n",
    "    Renomear coluna → dataframe.rename(index=None, columns=None, axis=None, inplace=False) \n",
    "        (Ex: Renomear a coluna A: dataframe.rename(columns={'ColunaA':'NomeNovo'}, axis=1, inplace=True)\n",
    "<br>\n",
    "\n",
    "##### - Ordenar pela linha ou pela coluna (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)\n",
    "    Linha → dataframe.sort_values(by=Indice, axis=0, ascending=True, inplace=False, ignore_index=False) \n",
    "    Coluna → dataframe.sort_values(by='NomeDaColuna', axis=1, ascending=True, inplace=False, ignore_index=False)\n",
    "* ascending = True → Ordem crescente\n",
    "* ascending = False → Ordem decrescente\n",
    "\n",
    "<br>\n",
    "\n",
    "##### - Agrupamento de colunas ou linhas (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)\n",
    "    dataframe.groupby(by='NomeDaColunaLinha', axis=0 ou 1, sort=True, dropna=True) \n",
    "    Agregar por 1 coluna ou linha → dataframe.groupby(by='NomeDaColunaAgrupada')['NomeDaColunaAfetadaPelaFunção'].função_agregação() \n",
    "    Agregar por mais de 1 coluna ou linha → dataframe.groupby(by='ListaDeColunasOuLinhasAgrupadas')[['ListaDeColunasAfetadasPelaFunção']].função_agregação()\n",
    "* sort = True → Ordem crescente\n",
    "* dropna = True → Os valores NA juntamente com a linha/coluna serão descartados\n",
    "\n",
    "<br>\n",
    "\n",
    "##### - Variação dos resultados apos agrupamento, em porcentagem (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html)\n",
    "    dataframe.groupby(by='NomeDaColunaAgrupada')['NomeDaColunaAfetadaPelaFunção'].função_agregação().pct_change() * 100 \n",
    "    Variação Percentual = ((Valor Atual - Valor Anterior) / Valor Anterior) * 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca57d1f",
   "metadata": {},
   "source": [
    "### **Biblioteca NumPy** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2d8b2",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "#### O NumPy é a principal biblioteca de computação numérica do Python, otimizada para cálculos pesados e serve de base para o Pandas.\n",
    "* No NumPy tranalha-se com arrays, que são estruturas de dados multidimensionais (1D, 2D, 3D), trabalham com dados do mesmo tipo, são mutáveis, ordenados e possuem comprimentos variáveis\n",
    "* Instalação da biblioteca: pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa2d20",
   "metadata": {},
   "source": [
    "### <u>Códigos</u> <br>\n",
    "#### Importação do Numpy\n",
    "    import numpy as np\n",
    "<br>\n",
    "\n",
    "#### Transformando uma lista em um array\n",
    "    np.array(lista)\n",
    "<br>\n",
    "\n",
    "#### Verificando o tipo de dado\n",
    "    Tipo de cada dado → array.dtype \n",
    "    Tipo do objeto → type(array)\n",
    "<br>\n",
    "\n",
    "#### Operações básicas\n",
    "    Soma 2 arrays → array + array\n",
    "    Soma todos os dados → array.sum() \n",
    "    Soma apenas os dados das linhas → array.sum(axis=1) \n",
    "    Soma apenas os dados das colunas → array.sum(axis=0) \n",
    "    Soma acumulada dos dados das linhas → array.cumsum(axis=1) \n",
    "    Soma de um nr com todos os dados → array + Nr \n",
    "    Subtração → array - array \n",
    "    Subtração de um nr com todos os dados → array - Nr \n",
    "    Multiplicação → array * array \n",
    "    Multiplicação de um nr com todos os dados → array * Nr \n",
    "    Divisão → array / array\n",
    "    Potência → array ** array\n",
    "<br>\n",
    "\n",
    "#### Criando array utilizando Range\n",
    "    Uma dimensão (Vetor) → np.arange(nr_inicial, nr_final, nr_passo) \n",
    "    Duas dimensões (Matriz) → np.arange(nr_inicial, nr_final, nr_passo).reshape(qtde_linhas, qtde_colunas)\n",
    "<br>\n",
    "\n",
    "#### Verificando a dimensão  do array\n",
    "    array.ndim\n",
    "<br>\n",
    "\n",
    "#### Verificando a quantidade de linhas e colunas do array\n",
    "    array.shape\n",
    "<br>\n",
    "\n",
    "#### Criando array utilizando números inteiros e decimais\n",
    "    np.array([números,...])\n",
    "<br>\n",
    "\n",
    "#### Criando array apenas com valores zero ou um\n",
    "    np.zeros(qtde_zero) \n",
    "    np.ones(qtde_um)\n",
    "<br>\n",
    "\n",
    "#### Criando array com número inicial, final e a quantidade de dados (Igualmente espaçados)\n",
    "    np.linspace(nr_inicial, nr_final, qtde)\n",
    "<br>\n",
    "\n",
    "#### Filtrando apenas alguns dados\n",
    "    array[indice da linha][indice da coluna] \n",
    "        (Ex: array[Nr][Nr]) \n",
    "    array[Filtro] \n",
    "    Filtro → array OperadorDeComparação Nr \n",
    "        (Ex: [array >,<,=,!= Nr]) \n",
    "    Filtro → (array OperadorDeComparação Nr) OperadorLógico (array OperadorDeComparação Nr) \n",
    "        (Ex: [array >,<,=,!= Nr] &,I [array >,<,=,!= Nr])] \n",
    "<br>\n",
    "\n",
    "#### Operações matemática de agregação\n",
    "    Mínimo das linhas → array.min(axis=1)  \n",
    "    Mínimo das colunas → array.min(axis=0) \n",
    "    Mínimo de todos os dados → array.min()  \n",
    "    Máximo das linhas → array.max(axis=1) \n",
    "    Máximo das colunas → array.max(axis=0) \n",
    "    Máximo de todos os dados → array.max() \n",
    "    Média das linhas → array.mean(axis=1) \n",
    "    Média das colunas → array.mean(axis=0) \n",
    "    Média apenas da primeira linha → array[0].mean() ou array.mean(axis=1)[0] \n",
    "    Mediana de todos os dados → np.median(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be69b7",
   "metadata": {},
   "source": [
    "### **Introdução a Estatística** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a59a2",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "#### O que é? Coleta, Organização, Apresentação, Interpretação e Análise <br>\n",
    "\n",
    "### <u>Estatística descritiva</u> <br>\n",
    "#### Descrição e apresentação dos dados de forma a facilitar o entendimento de um grande conjunto de dados. Fornece um resumo sobre os dados apresentados, podendo ser visual (tabelas de frequência, histogramas, gráficos) ou quantitativo (média, mediana e moda) <br>\n",
    "#### Tabela de Frequência: Apresenta os dados e a quantidade de ocorrências / repetições daquele dado. Serve para a visualização e entedimento de grandes conjuntos de dados.\n",
    "* Sintaxe: pd.crosstab(index = df.Coluna, columns = 'contagem')\n",
    "* Exibição gráfica de uma distribuição de frequência: Histograma.\n",
    "    * Sintaxe: df.Coluna.hist() → https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.hist.html#matplotlib.axes.Axes.hist\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### Resumo estatístivo: É uma forma de apresentar os principais conceitos sobre a distribuição dos dados de forma resumida, usando para isso: \n",
    "* Média: É basicamente a posição central de determinado conjunto de dados. É alterada por outliers.\n",
    "* Mediana: É o valor central de um conjunto de dados ordenados. \n",
    "* Moda: Valor mais frequente do conjunto de dados. A moda será afetada se a quantidade de dados adicionados for suficiente para fazer com que um dado seja o mais frequente.\n",
    "* Desvio Padrão\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Média X Mediana: \n",
    "* A mediana será afetada pela quantidade de dados adicionado na base.\n",
    "* A média será afetada pelo valor dos dados adicionados na base.\n",
    "* A média pode ser utilizada para analisar uma variável em relação a outra (Evitar outliers utilizando um limitador de valor). Podemos usar a média e os outliers para identificação de fraudes.\n",
    "* A mediana pode ser utilizada para analisar o tempo de uma variável.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Teorema de Chebyshev: Para qualquer distribuição de dados, a proporção mínima de dados que está a menos de k desvios padrões é dada por → % = 1 - 1 / K ** 2. \n",
    "##### Exemplo: Se o desvio padrão estiver a 2 pontos da média (% = 1 - 1 / 2 ** 2 → % = 75), então pelo menos 75% dos dados estaram concentrados na região abaixo:  \n",
    "<center><img width=\"35%\" src=\"_imagens/Teorema de Chebyshev.PNG\"></center>\n",
    "<br>\n",
    "\n",
    "#### Separatrizes: São pontos que dividem o conjunto de dados em partes iguais.\n",
    "* Mediana é uma separatriz que divide o conjunto de dados ao meio.\n",
    "* Quartis são separatrizes que dividem a base em 4 partes iguais.\n",
    "    * Q1 = 25%\n",
    "    * Q2 = 50%\n",
    "    * Q3 = 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad36d9",
   "metadata": {},
   "source": [
    "### **Biblioteca Matplotlib** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ab62e",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "####  Permite a criação de visualizações estáticas mais simples até os gráficos mais complexos e elaborados. \n",
    "* Pyplot é o módulo do Matplotlib que possui os gráficos básicos\n",
    "* Documentação: https://matplotlib.org/\n",
    "* Instalação da biblioteca → pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c895f83",
   "metadata": {},
   "source": [
    "### <u>Códigos</u> <br>\n",
    "#### - Importação do Matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "<br>\n",
    "\n",
    "#### - Criação dos plots\n",
    "    Estabelecendo os valores do eixo X → x = objeto \n",
    "    Estabelecendo os valores do eixo Y → y = objeto \n",
    "<br>\n",
    "\n",
    "#### - O objeto pode ser uma lista, uma array, uma coluna de um Dataframe etc \n",
    "    Estabelecendo 1 gráfico em apenas uma figura → \n",
    "        ax.TipoDeGráfico(DadosDoEixoX, DadosDoEixoY, label=\"Legenda\") \n",
    "    Estabelecendo 2 gráficos em apenas uma figura → \n",
    "        ax.TipoDeGráfico(DadosDoEixoX, DadosDoEixoY, label=\"LegendaGráfico1\") \n",
    "        ax.TipoDeGráfico(DadosDoEixoX, DadosDoEixoZ, label=\"LegendaGráfico2\")\n",
    "    Estabelecendo a quantidade de figuras e eixos → \n",
    "        fig, ax = plt.subplots(nrows=QtdeDeLinhas, ncols=QtdeDeColunas, figsize=(TamanhoX, TamanhoY), gridspec_kw={'height_ratios':[Nr,Nr]}) \n",
    "        ax[OrdemDosGráficosNaFigura].TipoDeGráfico(DadosDoEixoX, DadosDoEixoY, label=\"LegendaGráfico1\")\n",
    "        ax[OrdemDosGráficosNaFigura].TipoDeGráfico(DadosDoEixoX, DadosDoEixoZ, label=\"LegendaGráfico2\")\n",
    "    Estabelecendo as características dos eixos e o tipo de grafico → \n",
    "        ax.TipoDeGráfico(DadosDoEixoX, DadosDoEixoY, DemaisArgumentosConformeGráficoEscolhido) \n",
    "* figsize é o tamanho da figura\n",
    "* gridspec_kw é a proporção de cada gráfico        \n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Criação dos eixos\n",
    "    Estabelecendo o limite do eixo x → ax.set(xlim=(Nr,Nr)) \n",
    "    Estabelecendo o limite do eixo y → ax.set(ylim=(Nr,Nr)) \n",
    "    Estabelecendo os ticks(marcações) do eixo x → ax.set(xlim=(Nr,Nr), xticks=objeto)) ou ax.set_xticks([0,Nr]) Caso deseje que o eixo não apareça: ax.xaxis.set_visible(False) \n",
    "    Estabelecendo os ticks(marcações) do eixo y → ax.set(ylim=(Nr,Nr), yticks=objeto)) ou ax.set_yticks([0,Nr]) Caso deseje que o eixo não apareça: ax.yaxis.set_visible(False) \n",
    "* O objeto pode ser uma lista, um range (np.arange(Nr,NR)), etc \n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Inclusão de texto no gráfico (Rótulos)\n",
    "    plt.annotate(\"Texto\",(Nr_PosicaoX, Nr_PosicaoY))\n",
    "    \n",
    "    Exemoplo: \n",
    "    fig, ax = plt.subplots()\n",
    "    w = NrFloat\n",
    "    ax.TipoDeGráfico(DadosDoEixoX+w/2, DadosDoEixoY, label=\"LegendaGráfico1\", width=w)\n",
    "    ax.TipoDeGráfico(DadosDoEixoX-w/2, DadosDoEixoZ, label=\"LegendaGráfico2\", width=w)\n",
    "    for i in DadosDoEixoX.index: \n",
    "        ax.annotate(DadosDoEixoY[i],(DadosDoEixoX[i]+w/2,DadosDoEixoY[i]),ha=\"Alinhamento\",xytext=(NrDoDeslocamentoX,NrDoDeslocY),textcoords=\"offset points\", fontweight=\"bold\")\n",
    "        ax.annotate(DadosDoEixoZ[i],(DadosDoEixoX[i]-w/2,DadosDoEixoZ[i]),ha=\"Alinhamento\",xytext=(NrDoDeslocamentoX,NrDoDeslocY),textcoords=\"offset points\", fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "* Alinhamento = \"center\", \"left\" ou \"right\"\n",
    "* xytext = deslocamento de x,y do texto\n",
    "* fontsize = tamanho da fonte \n",
    "* fontweight = \"normal\" ou \"bold\" \n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Inclusão de legendas\n",
    "    handles, labels = scatter.legend_elemnts() \n",
    "    ax.legend(handles=handles, labels=[\"Textos das legendas\"]) \n",
    "##### ou \n",
    "    ax.TipoDeGráfico(DadosDoEixoX, DadosDoEixoY, label=\"LegendaGráfico\") \n",
    "    plt.legend()\n",
    "<br>\n",
    "\n",
    "#### - Exibição dos plots \n",
    "    plt.show()\n",
    "<br>\n",
    "\n",
    "#### - Retirar as bordas da figura\n",
    "    ax.spines[\"top\"].set_visible(false) \n",
    "    ax.spines[\"right\"].set_visible(false) \n",
    "    ax.spines[\"left\"].set_visible(false) \n",
    "    ax.spines[\"bottom\"].set_visible(false) \n",
    "<br>\n",
    "\n",
    "#### - Alterar as cores de determinados valores de X\n",
    "    ax.TipoDeGráfico(DadosDoEixoX[0:Nr], DadosDoEixoY[0:Nr], label=\"LegendaGráfico\") \n",
    "    ax.TipoDeGráfico(DadosDoEixoX[Nr:Nr], DadosDoEixoY[0:Nr], label=\"LegendaGráfico\", color=\"NrHexadecimal\") \n",
    "\n",
    "<img width=\"45%\" src=\"_imagens/Componentes de uma figura Matplotlib.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Dicas para reduzir o esforço cognitivo para entender o gráfico:\n",
    "* Comece com 0 no eixo Y.\n",
    "* Eixo secundário não é uma boa ideia.\n",
    "* Problema identificado → Explicar o problema (Analista de dados) → Apresentar os dados (Analista de dados) → Entendimento dos dados (Público) → Conclusão (Público). Antes de apresentar os dados é necessário que o analista de dados se certifique que o público precisam: Entender os gráficos, que eles podem questionar os valores e com isso terá que ser explicado o conteúdo e caso necessário, justificar enganos. \n",
    "* Utilize os princípios: Aproximidade, Similaridade, Acercamento, Fechamento, Continuidade e Conexão.\n",
    "* Utilize o contraste e atributos pé-atentivos.\n",
    "* Ajustar as barras para ficarem lado a lado:\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        w = NrFloat\n",
    "        ax.TipoDeGráfico(DadosDoEixoX+w/2, DadosDoEixoY, label=\"LegendaGráfico1\", width=w)\n",
    "        ax.TipoDeGráfico(DadosDoEixoX-w/2, DadosDoEixoZ, label=\"LegendaGráfico2\", width=w)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "<br>\n",
    "\n",
    "#### Especial: Gráfico boxplot: Permite visualizar os percentis, valores máximo, mínimo, interquartil, valores extremos (outliers), a simetria e dispersão dos dados. \n",
    "        \n",
    "        fig, ax = plt.subplots(ncols=Nr, figsize=(NrDaAltura,NrDaLargura))\n",
    "        ax.boxplot(x, DataFrame.NomeDaColuna ou Lista)\n",
    "        plt.show()\n",
    "        \n",
    "* interquartil = Q3 - Q1 ou DataFrame.NomeDaColuna.describe()['75%'] - DataFrame.NomeDaColuna.describe()['25%']\n",
    "* máx = Q3 + 1,5 * interquatil ou  DataFrame.NomeDaColuna.describe()['75%'] + 1,5 * interquatil\n",
    "* mín = Q1 - 1,5 * interquartil ou DataFrame.NomeDaColuna.describe()['25%'] - 1,5 * interquatil\n",
    "* Verificando quais são os valores acima do máximo (outliers) = DataFrame[DataFrame.NomeDaColuna > máx]\n",
    "* Verificando quais são os valores abaixo do mínimo (outliers) = DataFrame[DataFrame.NomeDaColuna < mín]\n",
    "> https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.boxplot.html#matplotlib.axes.Axes.boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd35cb2",
   "metadata": {},
   "source": [
    "### **API: Consumo e Tratamento de Dados de Fontes Externas** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad536d7b",
   "metadata": {},
   "source": [
    "#### Uma API ou Interface de programação de aplicações (do inglês \"Application Programming Interface\"), é um conjunto de regras e específicações que permite que diferentes softwares se comuniquem entre si. Em termos simples, é um usuário faz uma solicitação atraves da API ao sistema ou aplicação e recebe a resposta de volta. <br>\n",
    "##### Em um sistema, uma interface é um ponto de contato entre dois componentes de software. Uma interface de software pode ser uma interface gráfica do usuário (GUI), uma interface de linha de comando (CLI) ou uma interface de progração de aplicativos (API). <br>\n",
    "#### Um sistema pode comunicar-se com várias interfaces que possue diferentes funções. Aplicativos menores (Interfaces secundárias) se comunicam com o aplicativo principal (Interface principal), que por sua vez se comunica com o sistema principal, através da API. <br>\n",
    "* Solicitação (Request) = Tudo começa quando uma plaicação faz uma solicitação a uma API.\n",
    "* Processamento =  A API recebe a solicitação, processa-a, e então envia a solicitação ao sistema correto, como um banco de dados (Database) ou outro servidor (web server)\n",
    "* Resposta (Response) = O sistema retorna a informação solicitada de volta para a API, que por sua vez envia essa informação de volta para a aplicação inicial.\n",
    "* Apresentação (Client app) = A aplicação recebe a informação e a apresenta ao usuário (client) de uma forma legível. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Vantagens de usar API:\n",
    "1. Segurança: Controla quem e como os dados são acessados.\n",
    "2. Flexibilidade: Permite mudanças no banco de dados sem afetar os usuários da API.\n",
    "3. Integração: Facilita a conexão com outros sistemas ou plataformas.\n",
    "4. Automatização: Processos podem ser automatizados entre diferentes sistemas, reduzindo a necessidade de intervenção manual e aumentando a eficiência.\n",
    "5. Padronização: Os desenvolvedores têm um conjunto padrão de regras para seguir, o que ajuda a manter a consistência e a qualidade do software.\n",
    "6. Escalabilidade: APIs são projetadas para lidar com grandes volumes de solicitações e podem ser facilmente escaladas à medida que a demanda aumenta.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Desvantagens de usar APIs:\n",
    "1. Latência: Geralmente mais lento do que o acesso direto devido à sobrecarga de comunicação.\n",
    "2. Complexidade: Pode ser mais complexo para configurar e manter.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Vantagens do Acesso Direto a Bancos de Dados:\n",
    "1. Desempenho: Acesso mais rápido e direto aos dados.\n",
    "2. Simplicidade: Menos camadas entre a aplicação e os dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Desvantagens do Acesso Direto a Bancos de Dados:\n",
    "1. Segurança: Riscos potenciais se o banco de dados estiver exposto.\n",
    "2. Acoplamento: Mudanças no banco de dados podem afetar diretamente a aplicação.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Conclusão: A escolha entre APIs e acesso direto depende das necessidades do projeto. APIs são ótimas para segurança e integração, enquanto o acesso direto pode ser útil para desempenho em ambientes controlados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417002f6",
   "metadata": {},
   "source": [
    "### <u>Passo a passo de como utilizar uma API</u> <br>\n",
    "1. Estudo da documentação: Ela fornecerá informações sobre como a API funciona, seus endpoints, parâmetros disponíveis, formatos de respostas e etc. \n",
    "2. Entendimento dos status codes HTTP para interpretar as respostas da API. Ajudará a diagnosticar problemas e entender o status das suas requisições. \n",
    "    * O que são status codes ou códigos de status? são códigos de três dígitos que os servidores HTTP enviam para indicar o resultado de uma requisição feita por um cliente. Eles informam ao cliente se a requisição foi bem-sucedida, se houve algum erro, ou se é necessário realizar alguma ação adicional. Esses códigos são padronizados e fazem parte do protocolo HTTP, permitindo que clientes e servidores de diferentes implementações se comuniquem de forma eficaz. Principais status codes e seus significados:\n",
    "    * 2xx: Sucesso\n",
    "        * 200 OK: A requisição foi bem-sucedida. O resultado depende do método HTTP utilizado.\n",
    "        * 201 Created: A requisição foi bem-sucedida e um novo recurso foi criado como resultado.\n",
    "        * 204 No Content: A requisição foi bem-sucedida, mas não há conteúdo para enviar na resposta.\n",
    "    * 3xx: Redirecionamento\n",
    "        * 301 Moved Permanently: O recurso solicitado foi movido permanentemente para uma nova URL.\n",
    "        * 302 Found (ou Moved Temporarily): O recurso solicitado foi movido temporariamente para uma nova URL.\n",
    "        * 304 Not Modified: O recurso não foi modificado desde a última requisição.\n",
    "    * 4xx: Erros do cliente\n",
    "        * 400 Bad Request: A requisição não pode ser processada devido a um erro do cliente, como sintaxe de requisição inválida.\n",
    "        * 401 Unauthorized: O cliente não está autenticado para acessar o recurso solicitado.\n",
    "        * 403 Forbidden: O cliente está autenticado, mas não tem permissão para acessar o recurso.\n",
    "        * 404 Not Found: O recurso solicitado não foi encontrado no servidor.\n",
    "    * 5xx: Erros do servidor\n",
    "        * 500 Internal Server Error: O servidor encontrou uma situação que não sabe como lidar.\n",
    "        * 502 Bad Gateway: O servidor, ao atuar como gateway ou proxy, recebeu uma resposta inválida do servidor upstream.\n",
    "        * 503 Service Unavailable: O servidor não está pronto para lidar com a requisição, geralmente devido a manutenção ou sobrecarga.\n",
    "3. Registro e Autenticação para obter uma chave de API ou token. Esse token é usado para autenticar suas requisições e, muitas vezes, para rastrear e limitar seu uso.\n",
    "4. Configuração do ambiente de desenvolvimento. Instalação das bibliotecas ou outras ferramentas específicas.\n",
    "    * Biblioteca Requests é utilizada para fazer requisições HTTP → pip install requests e import requests\n",
    "    * Biblioteca Pandas é utilizada para manipular os dados → pip install pandas e import pandas as pd\n",
    "    * Biblioteca Pprint é utilizada para exibir os dados de forma formatada e legível → pip install pprint e import pprint\n",
    "    * Biblioteca Json é utilizada para trabalhar com arquivos JSON → pip install json e import json\n",
    "5. Realização de Requisições simples para testas a conexão e entender a estrutura das respostas. Isso pode envolver a obtenção de um único item ou listar vários itens.\n",
    "6. Tratamento de Parâmetros e Cabeçalhos.\n",
    "   * Enviar parâmetros (por exemplo, para filtrar resultados) \n",
    "   * Definir cabeçalhos adequados (como tokens de autenticação ou informações de tipo de conteúdo).\n",
    "7. Paginação e Limites: Muitas APIs paginam os resultados ou impõem limites de quantos itens você pode solicitar de uma vez.\n",
    "8. Tratamento de Respostas e Erros: Analise as respostas, extraia os dados necessários e implemente tratamentos de erro adequados para lidar com possíveis falhas ou limitações.\n",
    "9. Integração com Aplicações: Integre os dados ou funcionalidades da API em sua aplicação, como um site, aplicativo móvel ou sistema backend.\n",
    "10. Implemente Otimizações (Cache, Rate Limiting, etc.):Como caching de respostas frequentes, para melhorar o desempenho e garantir que você não exceda os limites da API.\n",
    "11. Monitoramento e Manutenção: Verifique regularmente a saúde das requisições e esteja atento a quaisquer mudanças ou atualizações na API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896dc65",
   "metadata": {},
   "source": [
    "### <u>O que é JSON?</u> <br>\n",
    "#### JSON (JavaScript Object Notation) é um formato de dados muito utilizado para transmitir dados estruturados pela web (Transmitir dados entre um servidor e um cliente da web). Ele é baseado na sintaxe de objetos JavaScript, mas pode ser lido e interpretado por muitas linguagens de programação modernas. <br>\n",
    "#### o JSON consiste em pares de chave-valor, onde as chaves são sempre strings e os valores podem ser strings, números, booleanos, objetos ou arrays. <br>\n",
    "#### Os objetos JSON são delimitados por chaves { } e contêm pares de chave-valor. <br>\n",
    "#### Os arrays JSON são delimitados por colchetes [ ]. e contêm uma lista de valores separados por vírgula. <br>\n",
    "#### Objetos JSON são usados para representar registros individuais, enquanto arrays JSON são usados para representar uma lista de registros. <br>\n",
    "#### Exemplo:  <br>\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"nome\": \"João\",\n",
    "        \"idade\": 30,\n",
    "        \"cidade\": \"São Paulo\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"Maria\",\n",
    "        \"idade\": 27,\n",
    "        \"cidade\": \"Rio de Janeiro\"\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"Pedro\",\n",
    "        \"idade\": 35,\n",
    "        \"cidade\": \"Belo Horizonte\"\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c0e16",
   "metadata": {},
   "source": [
    "### <u>O que é endpoint da API?</u> <br>\n",
    "##### Um endpoint de uma API refere-se a um ponto específico de acesso na API para o qual as requisições podem ser feitas. Em termos mais simples, é uma URL específica da API onde você pode acessar determinados dados ou funcionalidades. Cada endpoint é associado a uma função específica, como recuperar dados, atualizar dados, deletar dados, entre outras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7639a0",
   "metadata": {},
   "source": [
    "### <u>Códigos</u> <br>\n",
    "#### - Realização de requisições\n",
    "    variavel_constante_URL = 'Link da API' \n",
    "    variavel_reposta = requests.get(variavel_constante_URL) \n",
    "##### ou\n",
    "    variavel_constante_URL = 'Link da API' \n",
    "    variavel_reposta = requests.get(\"\".join(variavel_constante_URL, \"StringComplentarDoEndpoint\" ou str(variavel)))\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Verificar o status code de uma resposta\n",
    "    variavel_reposta.status_code \n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Exibindo os cabeçalhos\n",
    "    variavel_reposta.headers\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Exibindo os conteúdos da resposta no formato JSON\n",
    "    variavel_json = variavel_reposta.json()\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Exibindo de uma forma mais formatada\n",
    "    pprint(variavel_json) ou pprint(variavel_json[Indice:Indice])\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Salvando os dados em um arquivo JSON\n",
    "    with open(\"pasta/NomeDoArquivo.json\", \"w\") as variavel_arquivo: json.dump(variavel_json, variavel_arquivo)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Lendo os dados de um arquivo JSON\n",
    "    with open(\"pasta/NomeDoArquivo.json\", \"r\") as variavel_arquivo: variavel_arquivo_json = json.load(variavel_arquivo)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Transformando JSON em DataFrame\n",
    "    df = pd.DataFrame(variavel_json) \n",
    "##### ou\n",
    "    df = pd.DataFrame(variavel_reposta.json())\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Importantando a chave de acesso para dentro do jupter notebook\n",
    "    load_dotenv() \n",
    "    key = os.getenv(\"API_KEY\")\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Definindo o formato da URL como Json, caso a API permita.\n",
    "    variavel_constante_URL = f'Link da API&file_type=json'\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Paginação: Limitando os resultados da solicitação \n",
    "    variavel_constante_LIMIT = NrInteiro \n",
    "    variavel_constante_URL = f'Link da API&limit={LIMIT}'\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Deslocamento: \"Selecionando\" quais itens deseja solicitar\n",
    "    variavel_constante_OFFSET = NrInteiro \n",
    "    variavel_constante_URL = f'Link da API&offset={OFFSET}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0efcd",
   "metadata": {},
   "source": [
    "### <u>O que é API RESTful?</u> <br>\n",
    "#### Uma API RESTful é uma API que segue o padrão REST (Representational State Transfer). Esse padrão é baseado em 6 princípios:\n",
    "* Cliente-servidor: o cliente e o servidor são entidades independentes e cada um deles pode ser substituído, desde que a interface entre eles seja mantida.\n",
    "* Stateless: cada requisição do cliente para o servidor deve conter todas as informações necessárias para o servidor entender a requisição, sem guardar nenhum tipo de informação de requisições anteriores. Isso significa que nenhuma informação de estado é mantida entre requisições.\n",
    "* Cache: o cliente pode fazer requisições de cache para o servidor. O servidor deve informar se as requisições podem ser cacheadas ou não.\n",
    "* Interface uniforme: a interface entre o cliente e o servidor deve ser uniforme para todas as requisições. Isso significa que o protocolo HTTP deve ser seguido, utilizando os verbos HTTP corretos (GET, POST, PUT, DELETE, etc).\n",
    "* Sistema em camadas: o cliente acessa o servidor através de camadas intermediárias, como um proxy. Essas camadas intermediárias podem oferecer serviços como cache, load balancing, segurança, etc.\n",
    "* Código sob demanda (opcional): o servidor pode estender a funcionalidade do cliente enviando código para ser executado no cliente. Isso é opcional e não é muito utilizado.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Fonte: https://www.redhat.com/pt-br/topics/api/what-is-a-rest-api <br>\n",
    "\n",
    "#### <u>API com cadastro de chave</u><br>\n",
    "##### Para consumir a API será necessário gerar uma chave (key) e para isso, é preciso criar uma conta no site. A key é um código que identifica o usuário e permite o acesso à API. Este é um cenário muito comum, pois muitas APIs são pagas e precisam de uma key para serem consumidas. <br>\n",
    "##### Um problema comum é que, ao compartilhar o código, a key fica exposta. Uma solução é armazenar a key em uma variável de ambiente. Variáveis de ambiente são variáveis que ficam armazenadas no sistema operacional e podem ser acessadas por qualquer programa. <br>\n",
    "##### Para não expor a key no código, vamos armazená-la em uma variável de ambiente. Para isso, vamos usar a biblioteca python-dotenv. Para instalar, basta rodar o comando abaixo no terminal:\n",
    "    \n",
    "    pip install python-dotenv\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Agora, vamos criar um arquivo `.env` na raiz do projeto e adicionar o número da key da API: \n",
    "    \n",
    "    API_KEY = Número\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Para ler a variável de ambiente, vamos usar a biblioteca `os` e a biblioteca `dotenv`:\n",
    "    \n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Pode limitar a quantidade de dados que será solicitado para a API, utilizando o método Páginação, caso a documentação da API permita isso, basta acrescentar a variavel_constante_LIMIT e a inclusão de \"&limit={LIMIT}\" na variavel_constante_URL. <br>\n",
    "##### Caso deseje incluir um deslocamento (pular) entre os elementos, caso a documentação da API permita isso, basta acrescentar a variavel_constante_OFFSET e a inclusão de \"&offset={OFFSET}\" na variavel_constante_URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521bcec",
   "metadata": {},
   "source": [
    "### **Introdução ao Aprendizado de Máquinas** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161492c6",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "#### O que é Machine Learning? É o campo de estudo que dá aos computadores a capacidade de aprender sem serem explicitamentes programados. Em ciencia de dados, conseguimos encontrar padrões que antes seriam imperceptíveis e usar esses padrões para gerar diferenciais de negócio.\n",
    "* IA: Permiti que as máquinas resolvam uma tarefa assim como um humano faria (como aprendizado, reconhecimento de imagem, otimização, robôs,...).\n",
    "    * ML: O aprendizado será feito através de métodos estatísticos a partir de uma grande base de dados.\n",
    "    * Processamento de linguagem natural\n",
    "    * Reconhecimento de fala\n",
    "    * Automações\n",
    "    * Visão computacional\n",
    "    * Planejamento e otimização\n",
    "    * Sistemas especialistas\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### O que é o aprendizado? É permiti que o algoritmo busque padrões nos dados que poderão ser usados para prever comportamentos futuros. \n",
    "#### Dado → Padrões → Modelo → Novos dados → Análise da performace → Previsão \n",
    "* Análise da performace: Pode ser classificada como Underfitting (Subajuste), Balanced (Equilibrado) e Overfitting (Sobreajuste)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Machine Learning será util se:\n",
    "* Temos dados disponíveis. (Amostra suficientemente grande)\n",
    "* Existe um padrão nos dados. \n",
    "* O problema a ser resolvido está bem definido.\n",
    "* Não existe uma fórmula matemática para determinar o resultado.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### <u>Tipos de aprendizado</u><br>\n",
    "##### a) Aprendizado supervisionado:\n",
    "* Features X (Input) = Os dados são rotulados\n",
    "* Target Y (Output) = Os dados são rotulados\n",
    "\n",
    "<br>\n",
    "\n",
    "##### b) Aprendizado não supervisionado:\n",
    "* Features X (Input) = Os dados são rotulados\n",
    "* Target Y (Output) = Os dados não são rotulados\n",
    "\n",
    "<br>\n",
    "\n",
    "##### c) Aprendizado semi supervisionado:\n",
    "* Features X (Input) = Os dados são rotulados\n",
    "* Target Y (Output) = Alguns dados são rotulados\n",
    "\n",
    "<br>\n",
    "\n",
    "##### d) Aprendizado por esforço: Permite que o modelo faça suas próprias escolhas e aprenda com elas. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### <u>Escolher o melhor modelo de ML</u><br>\n",
    "##### Precisa analisar a quantidade de Esforço empregado X Tempo gasto X Diminuição dos Erros X Acordado com o Cliente\n",
    "* Evitar: Muito esforço para pouco retorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f03d0e",
   "metadata": {},
   "source": [
    "### **Biblioteca Scikit-Learn** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975713d",
   "metadata": {},
   "source": [
    "### <u>Conceito</u> <br>\n",
    "#### Scikit-learn é uma biblioteca de aprendizado de máquina de código aberto que oferece suporte ao aprendizado supervisionado e não supervisionado. Ele também fornece várias ferramentas para ajuste de modelo, pré-processamento de dados, seleção de modelo, avaliação de modelo e muitos outros utilitários.\n",
    "* Ferramentas simples e eficientes para análise preditiva de dados.\n",
    "* Acessível a todos e reutilizável em vários contextos e construído em NumPy, SciPy e Matplotlib\n",
    "> Documentação: https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff853e6e",
   "metadata": {},
   "source": [
    "### <u>Introdução da Regressão Linear</u><br>\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression <br>\n",
    "\n",
    "##### Equação da reta é dada por: f(x) = b + ax <br>\n",
    "* a = coeficiente angular da reta → (y2 = ax2 + b) - (y1 = ax1 + b) → y2-y1 = ax2-ax1 → y2-y1 = a(x2-x1) → a = y2-y1/x2-x1\n",
    "* b = coeficiente linear → Se x = 0, então y = b + a * 0, ou seja, y = b (x = 0, é onde a reta corta o eixo y)\n",
    "\n",
    "<img width=\"45%\" src=\"_imagens/Equação da reta.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Regressão Linear (Reta que mais se aproxima dos pontos): y = b + ax + E (Erro) → y = intercept / coeficiente linear / b + coeficiente angular / coef / variável1 * x  + Erro <br>\n",
    "##### Coeficiente de determinação: R² ou R² ajustado <br>\n",
    "\n",
    "### <u>Códigos</u><br>\n",
    "##### - Importação das bibliotecas\n",
    "    from sklearn.linear_model import LinearRegression ou from sklearn import linear_model\n",
    "    from sklearn.metrics import mean_absolute_error \n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "<br>\n",
    "\n",
    "##### - Definição dos valores de X e Y\n",
    "    var_X = df_dados.ColunaX ou df_dados['Coluna1', 'Coluna2'] ou array\n",
    "    var_y = df_dados.ColunaY ou df_dados['Coluna1', 'Coluna2'] ou array\n",
    "<br>\n",
    "\n",
    "##### - Criação do modelo de Regressão Linear\n",
    "    modelo = LinearRegression() ou modelo = linear_model.LinearRegression() \n",
    "<br>\n",
    "\n",
    "##### - Ajustando o modelo de Regressão Linear\n",
    "    modelo.fit(X.values.reshape(-1, 1),y)\n",
    "<br>\n",
    "\n",
    "##### - Resultado do modelo de Regressão Linear (Em porcentagem) = Significa o quanto o modelo está \"acertando\"\n",
    "    modelo.score(X.values.reshape(-1, 1),y)\n",
    "<br>\n",
    "\n",
    "##### - Gerando o coeficiente linear, que é representado pelo valor 'b' na equação da reta\n",
    "    modelo.intercept_\n",
    "<br>\n",
    "\n",
    "##### - Gerando o coeficiente angular \"a\" é o número que acompanha a variável \"x\" na equação da reta. Ele indica a inclinação da reta em relação ao eixo x.\n",
    "    modelo.coef_[0]\n",
    "<br>\n",
    "\n",
    "##### - Utilização dos dois coeficientes para calcular a equação da reta\n",
    "    mod_x = np.arange(Nr,Nr) \n",
    "    mod_y = modelo.intercept_ + mod_x * modelo.coef_[0]\n",
    "<br>\n",
    "\n",
    "##### - Prevendo o comportamento de outros valores X usando o modelo \n",
    "    var_previsao = modelo.predict(mod_x)\n",
    "<br>\n",
    "\n",
    "##### - Analisando os erros do modelo \n",
    "    mean_absolute_error(X, mod_y)\n",
    "<br>\n",
    "\n",
    "##### - Analisando os erros do modelo em porcentagem\n",
    "    mean_absolute_percentage_error(X, mod_y)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e493262",
   "metadata": {},
   "source": [
    "### <u>Modelo de classificação</u> <br>\n",
    "| Transação | Modelo | Modelo |                        \n",
    "| --- | --- | --- | \n",
    "| --- | Não é fraude | É fraude |\n",
    "| Não é fraude | OK | Bloqueio/Insatisfação |\n",
    "| É fraude | Perdeu dinheiro  | OK |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Dados reais | Modelo | Modelo | \n",
    "| --- | --- | --- | \n",
    "| --- | (+) | (-) |\n",
    "| (+) | Verdadeiro/positivo | Falso/negativo |\n",
    "| (-) | Falso/positivo | Verdadeiro/negativo |\n",
    "\n",
    "<br>\n",
    "\n",
    "### <u>Avaliando o modelo de confusão</u> <br>\n",
    "#### Calcule a matriz de confusão para avaliar a precisão de uma classificação.\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html <br>\n",
    "* Transação (Dados reais) for + e o modelo for + significa que é verdadeiro positivo, ou seja, o modelo previu que não era fraude e a transação realmente não era fraude.\n",
    "* Transação (Dados reais) for - e o modelo for - siginifica que é verdadeiro negativo, ou seja, o modelo previu que era fraude e a transação realmente era fraude.\n",
    "* Transação (Dados reais) for + e o modelo for - significa que é falso negativo, ou seja, o modelo previu que era fraude, porém a transação não era fraude (Bloqueio da transação).\n",
    "* Transação (Dados reais) for - e o modelo for + significa que falso positivo, ou seja, o modelo previu que não era fraude, porém a transação era fraudulenta (Perdeu dinheiro). \n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        y_true = ['-','-','-','+','+','+'] \n",
    "        y_pred = ['-','-','+','+','-','-'] \n",
    "        confusion_matrix(y_true, y_pred, labels=['+','-']) ou confusion_matrix(y_true, y_pred)\n",
    "\n",
    "| Dados reais | Modelo | Modelo | \n",
    "| --- | --- | --- | \n",
    "| --- | (+) | (-) |\n",
    "| (+) | 1 | 2 |\n",
    "| (-) | 1 | 2 |\n",
    "* Verdadeiro positivo → y_true = ['+'] e y_pred = ['+'] ou seja, 1 resultado\n",
    "* Verdadeiro negativo → y_true = ['-','-'] e y_pred = ['-','-'] ou seja, 2 resultados\n",
    "* Falso negativo → y_true = ['+','+'] e y_pred = ['-','-'] ou seja, 2 resultados\n",
    "* Falso positivo → y_true = ['-'] e y_pred = ['+'] ou seja, 1 resultado\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Resumindo: \n",
    "* Positivo (Previu que não era fraude e acertou) ou negativo (Previu que era fraude e errou) é em relação ao modelo. \n",
    "* Verdadeiro ou falso é em relação aos resultado dos dados com o modelo (Acertou → +/+ ou -/- ou errou → +/- ou -/+).\n",
    "    * Verdadeiro: (+ com +) → Previu que não era e acertou e (- com -) → Previu que era e acertou\n",
    "    * Falso: (+ com -) → Previu que era e errou e (- com +) → Previu que não era e errou \n",
    "* Falso ou negativo = 0\n",
    "* Verdadeiro ou positivo = 1\n",
    "\n",
    "<br>\n",
    "\n",
    "#### a) Acurácia: <br>\n",
    "##### Quantos valores foram previsto de forma correta (Pontuação de classificação de precisão) \n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score \n",
    "\n",
    "        Sintaxe: Verdadeiro positivo + Verdadeiro negativo / Total \n",
    "        Exemplo: 1 + 2 / 1 + 2 + 1 + 2 = 50%\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy_score(y_true,y_pred)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### b) Precisão: <br>\n",
    "##### Dos valores que eu previ como positivo quantos eu acertei (Evitar ao máximo falsos positivos! Classificar um cliente como bom pagador sem ele realmente ser) <br>   \n",
    "##### A precisão é a razão onde está o número de verdadeiros positivos e o número de falsos positivos. A precisão é intuitivamente a capacidade do classificador de não rotular como positiva uma amostra negativa. <br>   \n",
    "##### O melhor valor é 1 e o pior valor é 0. \n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "        Sintaxe: Verdadeiro positivo / Verdadeiro positivo + Falso positivo\n",
    "        Exemplo: 1 / 1 + 1 = 50%\n",
    "        \n",
    "        from sklearn.metrics import precision_score\n",
    "        precision_score(y_true,y_pred, pos_label='+')\n",
    "\n",
    "<br>\n",
    "\n",
    "#### c) Recall: <br>\n",
    "##### Dos clientes que eram positivos quantos foram classificados de forma correta (Evitar ao máximo falsos negativos! Falar que uma transação não é fraude sendo que é) <br>\n",
    "##### O recall é a razão onde está o número de verdadeiros positivos e o número de falsos negativos. O recall é intuitivamente a capacidade do classificador de encontrar todas as amostras positivas. <br>\n",
    "##### O melhor valor é 1 e o pior valor é 0. \n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "\n",
    "        Sintaxe: Verdadeiro positivo / Verdadeiro positivo + Falso negativo\n",
    "        Exemplo: 1 / 1 + 2 = 33%\n",
    "        \n",
    "        from sklearn.metrics import recall_score\n",
    "        recall_score(y_true,y_pred, pos_label='+')\n",
    "\n",
    "<img width=\"25%\" src=\"_imagens/Modelo de classificação - Recall e Precisão.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71845ec",
   "metadata": {},
   "source": [
    "### <u>Introdução do classificador Linear Perceptron</u><br>\n",
    "##### Equação da reta é dada por: w0 + w1.x + w2.y = 0 <br>\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron <br>\n",
    "#### No aprendizado de máquina , o perceptron (ou neurônio de McCulloch-Pitts ) é um algoritmo para aprendizado supervisionado de classificadores binários . Um classificador binário é uma função que pode decidir se uma entrada, representada por um vetor de números, pertence ou não a alguma classe específica. É um tipo de classificador linear , ou seja, um algoritmo de classificação que faz suas previsões com base em uma função preditora linear combinando um conjunto de pesos com o vetor de características .\n",
    "\n",
    "<img width=\"35%\" src=\"_imagens/perceptron.png\">\n",
    "\n",
    "#### \"w0 + w1.x + w2.y = 0\" é uma equação linear que representa uma reta em um espaço bidimensional (forma geral da equação de uma reta), onde w0, w1 e w2 são os coeficientes da equação. Essa equação é conhecida como a forma geral de uma equação linear em duas variáveis. A variável x representa a coordenada no eixo x e a variável y representa a coordenada no eixo y. Esses coeficientes representam os pesos atribuídos a cada variável na equação. O coeficiente w0 é o termo independente, que não depende de nenhuma variável. O coeficiente w1 é o peso atribuído à variável x e o coeficiente w2 é o peso atribuído à variável y. <br>\n",
    "\n",
    "#### A diferença entre a forma geral da equação de uma reta e a forma de y = ax + b está na representação dos coeficientes. Na forma geral, os coeficientes w0, w1 e w2 representam a inclinação em relação aos eixos x e y, enquanto na forma de y = ax + b, os coeficientes a e b representam diretamente a inclinação e o ponto de interseção no eixo y. <br>\n",
    "\n",
    "#### A equação geral de uma reta é dada por w0 + w1.x + w2.y = 0, onde w0, w1 e w2 são constantes e x e y são as coordenadas dos pontos na reta. Para encontrar o coeficiente angular (a) da reta a partir dessa equação, você pode seguir os seguintes passos:\n",
    "* Isolando o termo w2.y na equação, temos: w2.y = -w0 - w1.x\n",
    "* Dividindo toda a equação por w2, temos: y = (-w0/w2) - (w1/w2).x\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Comparando essa equação com a forma y = ax + b, podemos ver que o coeficiente angular (a) é dado por -(w1/w2). Portanto, para encontrar o coeficiente angular de uma reta a partir da sua equação na forma geral, você pode calcular -(w1/w2). <br>\n",
    "\n",
    "#### Exemplo: O DataFrame tem 3 colunas: A, B e a Target(Alvo). O Perceptron aplicará um peso W1 na coluna A (A * W1) e W2 na coluna B (B * W2). Depois somará os dois valores com o W0, que tem um valor fixo, (w0 + w1.x + w2.y = 0). Se o somatório dos 3 valores for maior que o limite, então será Resposta1 e se o somatório for menor que o limite, então será Resposta2<br>\n",
    "\n",
    "### <u>Códigos</u> <br>\n",
    "#### - Importação da biblioteca\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### - Definição dos valores de X e Y\n",
    "    var_X = df_dados.ColunaX ou df_dados['Coluna1', 'Coluna2'] ou array\n",
    "    var_y = df_dados.ColunaY ou df_dados['Coluna1', 'Coluna2'] ou array (Será o valor que quer encontrar - Target/Alvo) - Ex: df_dados.coluna_target\n",
    "<br>\n",
    "\n",
    "#### - Criação do modelo (clf = instância do modelo LinearSVC)\n",
    "    modelo_clf = Perceptron(tol=1e-3, random_state=0)  \n",
    "<br>\n",
    "\n",
    "#### - Ajustando o modelo \n",
    "    modelo_clf.fit(var_X, var_y)\n",
    "<br>\n",
    "\n",
    "#### - Resultado do modelo (Em porcentagem) = Significa o quanto o modelo está \"acertando\"\n",
    "* Próximo de 1 está acertando\n",
    "* Próximo de 0 não está acertando\n",
    "\n",
    "        modelo_clf.score(var_X,var_y)\n",
    "<br>\n",
    "\n",
    "#### - Gerando o coeficiente linear, que é representado pelo valor 'b' na equação da reta\n",
    "##### w0 = modelo.intercept_\n",
    "    modelo_clf.intercept_[0]\n",
    "<br>\n",
    "\n",
    "#### - Gerando o coeficiente angular \"a\" é o número que acompanha a variável \"x\" na equação da reta. Ele indica a inclinação da reta em relação ao eixo x.\n",
    "##### w1 = modelo.coef_[0][0]\n",
    "##### w2 = modelo.coef_[0][1]\n",
    "    modelo_clf.coef_[0][0]\n",
    "    modelo_clf.coef_[0][1]\n",
    "<br>\n",
    "\n",
    "#### - Utilização dos dois coeficientes para calcular a equação da reta\n",
    "    mod_x = np.arange(Nr,Nr) \n",
    "    mod_y = (- modelo.intercept_[0] - modelo_clf.coef_[0][0] * mod_x)/modelo_clf.coef_[0][1]\n",
    "<br>\n",
    "\n",
    "#### - Calculo de como chegou ao mod_y:\n",
    "    w0 + w1.x + w2.y = 0\n",
    "    w2.y = - w0 - w1.x\n",
    "    y = (- w0 - w1.x) / w2\n",
    "    y = (- modelo_clf.intercept_[0] - modelo_clf.coef_[0][0] * mod_x)/modelo_clf.coef_[0][1]\n",
    "<br>    \n",
    "\n",
    "#### - Prevendo o comportamento de X usando o modelo\n",
    "    var_previsao = modelo_clf.predict(mod_x)\n",
    "<br>  \n",
    "\n",
    "#### - Calculo da matriz de confusão para avaliar a precisão de uma classificação:\n",
    "    confusion_matrix(var_y, var_previsao)\n",
    "<br>    \n",
    "   \n",
    "#### - Quantos valores foram previsto de forma correta (Pontuação de classificação de precisão)\n",
    "    accuracy_score(var_y, var_previsao)\n",
    "<br>     \n",
    "\n",
    "#### - A precisão é a razão onde está o número de verdadeiros positivos e o número de falsos positivos. A precisão é intuitivamente a capacidade do classificador de não rotular como positiva uma amostra negativa.\n",
    "    precision_score(var_y, var_previsao, average='TipoAverage')\n",
    "<br> \n",
    "\n",
    "#### - O recall é a razão onde está o número de verdadeiros positivos e o número de falsos negativos. O recall é intuitivamente a capacidade do classificador de encontrar todas as amostras positivas.\n",
    "    recall_score(var_y, var_previsao, average='TipoAverage')\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Entendo o average do Precisão e do Recall:\n",
    "* binary (binários - Apenas 1 e 0): Relate apenas resultados para a classe especificada por pos_label (padrão=1).\n",
    "* micro: Calcule métricas globalmente contando o total de verdadeiros positivos, falsos negativos e falsos positivos.\n",
    "* macro: Calcule as métricas para cada rótulo e encontre sua média não ponderada.\n",
    "* weighted: Calcule as métricas para cada rótulo e encontre sua média ponderada pelo suporte (o número de instâncias verdadeiras para cada rótulo)\n",
    "* samples (amostra): Calcule as métricas para cada instância e encontre sua média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a70cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o pandas e a base\n",
    "import pandas as pd\n",
    "df_treino = pd.read_excel(\"_dataset/BaseInadimplencia.xlsx\",sheet_name='treino')\n",
    "df_teste = pd.read_excel(\"_dataset/BaseInadimplencia.xlsx\",sheet_name='teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a397ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma reta capaz de separar esses pontos\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(df_treino.SaldoConta, df_treino['SaldoInvestimento'],c=df_treino.Situacao)\n",
    "ax.scatter(df_teste.SaldoConta, df_teste['SaldoInvestimento'],c=df_teste.Situacao)\n",
    "x = [6,-6]\n",
    "y = [-6,6]\n",
    "ax.plot(x, y)\n",
    "\n",
    "plt.show();\n",
    "\n",
    "## Resultado: Essa reta que separa os dados é dada por y=-x e tudo que estiver acima da reta deve ser classificado como 1 e tudo que estiver abaixo da reta como 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando essa reta em uma função para classificar os pontos\n",
    "# clf = instância do modelo LinearSVC (sklearn.svm.LinearSVC → Classificação de vetores de suporte linear.)\n",
    "def clf(x,y):\n",
    "    # y_modelo = -x_modelo\n",
    "    y_modelo = -x\n",
    "\n",
    "    # Se y_modelo > y_dado → classificação = 0\n",
    "    if y_modelo >= y:\n",
    "        return 0\n",
    "    # Se y_modelo < y_dado → classificação = 1\n",
    "    elif y_modelo < y:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a função aos dados de treino\n",
    "df_treino['predict'] = df_treino.apply(lambda x:clf(x['SaldoConta'],x['SaldoInvestimento']),axis=1)\n",
    "df_teste['predict'] = df_teste.apply(lambda x:clf(x['SaldoConta'],x['SaldoInvestimento']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a base com a nova coluna\n",
    "df_treino.head(2)\n",
    "\n",
    "# Usar o groupby para o treino\n",
    "df_treino.groupby(['Situacao','predict'])['predict'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99951533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a base com a nova coluna\n",
    "df_teste.head(2)\n",
    "\n",
    "# Usar o groupby para o teste\n",
    "df_teste.groupby(['Situacao','predict'])['predict'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Matriz de confusão para a base de treino\n",
    "confusion_matrix(df_treino.Situacao,df_treino.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Matriz de confusão para a base de teste\n",
    "confusion_matrix(df_teste.Situacao,df_teste.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Acurácia para a base de treino\n",
    "accuracy_score(df_treino.Situacao,df_treino.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Acurácia para a base de teste\n",
    "accuracy_score(df_teste.Situacao,df_teste.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Precisão para a base de treino\n",
    "precision_score(df_treino.Situacao,df_treino.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Precisão para a base de teste\n",
    "precision_score(df_teste.Situacao,df_teste.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# Recall para a base de treino\n",
    "recall_score(df_treino.Situacao,df_treino.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# Recall para a base de teste\n",
    "recall_score(df_teste.Situacao,df_teste.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c1af4",
   "metadata": {},
   "source": [
    "### <u>Separando os dados em Treino e Teste</u> <br>\n",
    "* Qual o motivo de fazermos isso? Usamos os dados do passado para criar um modelo para fazer previsões sobre o futuro. Para fazer essa previsão sobre o futuro, precisamos separar os dados históricos em treino, validação e teste para avaliarmos o modelo.\n",
    "    * Treino: vamos usar para ajustar o modelo\n",
    "    * Validação: usar para otimizar o modelo (ajustar parâmetros, testar diferentes hiperparâmetros, realizar melhorias, etc)\n",
    "    * Teste: avaliar o modelo final já ajustado\n",
    "\n",
    "<br>\n",
    "\n",
    "### <u>Introdução do módulo train_test_split:</u> <br> \n",
    "##### Divide as matrizes ou matrizes em subconjuntos aleatórios de treinamento e teste.\n",
    "1. Importação da biblioteca: \n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "2. Importação dos dados\n",
    "3. Definição dos valores de x e y\n",
    "4. Separação dos valores de x e y em treino e teste \n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=Nr_float, random_state=Nr_Fixo)\n",
    "\n",
    "<br>\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html <br>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação da biblioteca: from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importação dos dados\n",
    "df_base = pd.read_excel(\"_dataset/BaseInadimplencia.xlsx\", sheet_name='Base')\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos valores de x e y\n",
    "X = df_base[['SaldoConta','SaldoInvestimento']]\n",
    "y = df_base['Situacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos valores de x e y em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eaa595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos valores de X de treino com 670 dados de um total de 1000 (1000 x 67%)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos valores de X de teste com 330 dados de um total de 1000 (1000 x 33%)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56bd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma reta capaz de separar esses pontos\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_train.SaldoConta, X_train['SaldoInvestimento'],c=y_train)\n",
    "ax.scatter(X_test.SaldoConta, X_test['SaldoInvestimento'],c=y_test)\n",
    "x = [6,-6]\n",
    "y = [-6,6]\n",
    "ax.plot(x, y)\n",
    "\n",
    "plt.show()\n",
    "# Essa reta que separa os dados é dada por y=-x e tudo que estiver acima da reta deve ser classificado como 1 e tudo que estiver abaixo da reta como 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b603c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando essa reta em uma função para classificar os pontos\n",
    "def clf(x,y):\n",
    "    # y_modelo = -x_modelo\n",
    "    y_modelo = -x\n",
    "\n",
    "    # Se y_modelo > y_dado → classificação = 0\n",
    "    if y_modelo >= y:\n",
    "        return 0\n",
    "    # Se y_modelo < y_dado → classificação = 1\n",
    "    elif y_modelo < y:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a função aos dados de treino\n",
    "y_pred_train = X_train.apply(lambda x:clf(x['SaldoConta'],x['SaldoInvestimento']),axis=1)\n",
    "y_pred_test = X_test.apply(lambda x:clf(x['SaldoConta'],x['SaldoInvestimento']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Matriz de confusão para a base de treino\n",
    "confusion_matrix(y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão para a base de teste\n",
    "confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ed2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Acurácia para a base de treino\n",
    "accuracy_score(y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia para a base de teste\n",
    "accuracy_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Precisão para a base de treino\n",
    "precision_score(y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisão para a base de teste\n",
    "precision_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# Recall para a base de treino\n",
    "recall_score(y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2918e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall para a base de teste\n",
    "recall_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e974b83",
   "metadata": {},
   "source": [
    "### <u>Introdução do modelo Árvore de Decisão</u> <br>\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/tree.html <br>\n",
    "\n",
    "1. Importação das blibliotecas\n",
    "2. Importação da base de dados\n",
    "3. Visuzalição dos dados e das informações dos dados\n",
    "4. Analisando os dados e escolhendo quais serão as variáveis que serão estudadas pelo modelo\n",
    "5. Criação do gráfico de dispersão com os valores das variáveis\n",
    "6. Criação da equação da reta (caso esteja comparando com a regressão linear)\n",
    "7. Criação da função para classificar um novo ponto no modelo da árvore e da reta (caso esteja comparando com a regressão linear)\n",
    "8. Separação do X e do y\n",
    "9. Separação dos dados em teste e treino\n",
    "10. Aplicação da função nos dados de treino e teste, utilizando o modelo da árvore e da reta (caso esteja comparando com a regressão linear)\n",
    "11. Criação da matrix da confusão com os dados de teste, utilizando o modelo da árvore e da reta (caso esteja comparando com a regressão linear)\n",
    "12. Cálculo da acurácia com os dados de teste, utilizando o modelo da árvore e da reta (caso esteja comparando com a regressão linear)\n",
    "13. Cálculo da precisão com os dados de teste, utilizando o modelo da árvore e da reta (caso esteja comparando com a regressão linear)\n",
    "14. Cálculo da recall com os dados de teste, utilizando o modelo da árvore e da reta (caso esteja comparando com a regressão linear)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Importação da biblioteca\n",
    "    from sklearn import tree\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### - Definição dos valores de X e Y\n",
    "    var_X = df_dados.ColunaX ou df_dados['Coluna1', 'Coluna2'] ou array\n",
    "    var_y = df_dados.ColunaY ou df_dados['Coluna1', 'Coluna2'] ou array (Será o valor que quer encontrar - Target/Alvo) - Ex: df_dados.coluna_target\n",
    "<br>\n",
    "\n",
    "#### - Criação do modelo (clf = instância do modelo LinearSVC)\n",
    "    modelo_clf = tree.DecisionTreeClassifier()\n",
    "<br>\n",
    "\n",
    "#### - Ajustando o modelo \n",
    "    modelo_clf.fit(var_X, var_y)\n",
    "<br>\n",
    "\n",
    "#### - Resultado do modelo (Em porcentagem) = Significa o quanto o modelo está \"acertando\"\n",
    "* Próximo de 1 está acertando\n",
    "* Próximo de 0 não está acertando\n",
    "\n",
    "        modelo_clf.score(var_X,var_y)\n",
    "        \n",
    "#### - Visualização das decisões tomadas por essa árvore\n",
    "    tree.plot_tree(modelo_clf);\n",
    "\n",
    "##### Exemplo de visualização das decisões da árvore: \n",
    "\n",
    "<img width=\"25%\" src=\"_imagens/Exemplo de arvore de decisão.png\"> \n",
    "\n",
    "* x[0] <= 2.45, significa que os valores de x menores ou igual a 2.45 possuem y = 0 e os valores maiores que 2.45 possuem y = 1.\n",
    "* gini = 0.5, significa que a metade dos valores de y são 1 e a outra metade, 0.\n",
    "* samples = 100, significa que o tamanho da amostra é igual a 100.\n",
    "* value = [50,50], siginifica que amostra está dividida em 50 igual a 1 e 50 igual 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### - Prevendo o comportamento de X usando o modelo\n",
    "    var_previsao = modelo_clf.predict(var_X)\n",
    "<br>\n",
    "\n",
    "#### - Calculo da matriz de confusão para avaliar a precisão de uma classificação:\n",
    "    confusion_matrix(var_y, var_previsao)\n",
    "<br>    \n",
    "\n",
    "#### - Quantos valores foram previsto de forma correta (Pontuação de classificação de precisão)\n",
    "    accuracy_score(var_y, var_previsao)\n",
    "<br> \n",
    "\n",
    "#### - A precisão é a razão onde está o número de verdadeiros positivos e o número de falsos positivos. A precisão é intuitivamente a capacidade do classificador de não rotular como positiva uma amostra negativa.    \n",
    "    precision_score(var_y, var_previsao, average='TipoAverage')\n",
    "<br> \n",
    "\n",
    "#### - O recall é a razão onde está o número de verdadeiros positivos e o número de falsos negativos. O recall é intuitivamente a capacidade do classificador de encontrar todas as amostras positivas.    \n",
    "    recall_score(var_y, var_previsao,average='TipoAverage')\n",
    "<br> \n",
    "\n",
    "#### Entendo o average do Precisão e do Recall:\n",
    "* binary (binários - Apenas 1 e 0): Relate apenas resultados para a classe especificada por pos_label (padrão=1).\n",
    "* micro: Calcule métricas globalmente contando o total de verdadeiros positivos, falsos negativos e falsos positivos.\n",
    "* macro: Calcule as métricas para cada rótulo e encontre sua média não ponderada.\n",
    "* weighted: Calcule as métricas para cada rótulo e encontre sua média ponderada pelo suporte (o número de instâncias verdadeiras para cada rótulo)\n",
    "* samples (amostra): Calcule as métricas para cada instância e encontre sua média    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das blibliotecas\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação da base de dados\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "df_iris = pd.DataFrame(data.data)\n",
    "df_iris.columns = data.feature_names\n",
    "df_iris['target'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuzalição dos dados e das informações dos dados\n",
    "display(df_iris.head(2))\n",
    "df_iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e57898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando os dados e escolhendo quais serão as variáveis que serão estudadas pelo modelo\n",
    "df_iris.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação do X e do y\n",
    "X = df_iris[['petal length (cm)','petal width (cm)']]\n",
    "y = df_iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70073b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos dados em teste e treino\n",
    "# 1/3 = 0,33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd749ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação da função nos dados de treino e teste, utilizando o modelo da árvore \n",
    "clf_Arvore = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Arvore.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Arvore.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e744e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(clf_Arvore);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_predictArvore = clf_Arvore.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fa0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da matrix da confusão com os dados de teste, utilizando o modelo da árvore \n",
    "confusion_matrix(y_test, var_predictArvore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377914b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da acurácia com os dados de teste, utilizando o modelo da árvore \n",
    "accuracy_score(y_test, var_predictArvore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb21a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da precisão com os dados de teste, utilizando o modelo da árvore \n",
    "precision_score(y_test, var_predictArvore,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b00ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da recall com os dados de teste, utilizando o modelo da árvore\n",
    "recall_score(y_test, var_predictArvore,average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ec77d",
   "metadata": {},
   "source": [
    "### <u>Introdução do modelo Regressão Logística</u> <br>\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression <br>\n",
    "\n",
    "#### - Importação da biblioteca\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### - Definição dos valores de X e Y\n",
    "    var_X = df_dados.ColunaX ou df_dados['Coluna1', 'Coluna2'] ou array\n",
    "    var_y = df_dados.ColunaY ou df_dados['Coluna1', 'Coluna2'] ou array (Será o valor que quer encontrar - Target/Alvo) - Ex: df_dados.coluna_target\n",
    "<br>\n",
    "\n",
    "#### - Criação do modelo (clf = instância do modelo LinearSVC)\n",
    "    modelo_clf = LogisticRegression(random_state=Nr_fixo)\n",
    "    \n",
    "<br>\n",
    "\n",
    "#### - Ajustando o modelo \n",
    "    modelo_clf.fit(var_X, var_y)\n",
    "<br>\n",
    "\n",
    "#### - Resultado do modelo (Em porcentagem) = Significa o quanto o modelo está \"acertando\"\n",
    "* Próximo de 1 está acertando\n",
    "* Próximo de 0 não está acertando\n",
    "\n",
    "        modelo_clf.score(var_X,var_y)\n",
    "<br>\n",
    "\n",
    "#### - Prevendo o comportamento de X usando o modelo\n",
    "    var_previsao = modelo_clf.predict(var_X)\n",
    "<br>\n",
    "\n",
    "#### - Calculo da matriz de confusão para avaliar a precisão de uma classificação:\n",
    "    confusion_matrix(var_y, var_previsao)\n",
    "<br>    \n",
    "\n",
    "#### - Quantos valores foram previsto de forma correta (Pontuação de classificação de precisão)\n",
    "    accuracy_score(var_y, var_previsao)\n",
    "<br> \n",
    "\n",
    "#### - A precisão é a razão onde está o número de verdadeiros positivos e o número de falsos positivos. A precisão é intuitivamente a capacidade do classificador de não rotular como positiva uma amostra negativa.    \n",
    "    precision_score(var_y, var_previsao, average='TipoAverage')\n",
    "<br> \n",
    "\n",
    "#### - O recall é a razão onde está o número de verdadeiros positivos e o número de falsos negativos. O recall é intuitivamente a capacidade do classificador de encontrar todas as amostras positivas.    \n",
    "    recall_score(var_y, var_previsao,average='TipoAverage')\n",
    "<br> \n",
    "\n",
    "#### Entendo o average do Precisão e do Recall:\n",
    "* binary (binários - Apenas 1 e 0): Relate apenas resultados para a classe especificada por pos_label (padrão=1).\n",
    "* micro: Calcule métricas globalmente contando o total de verdadeiros positivos, falsos negativos e falsos positivos.\n",
    "* macro: Calcule as métricas para cada rótulo e encontre sua média não ponderada.\n",
    "* weighted: Calcule as métricas para cada rótulo e encontre sua média ponderada pelo suporte (o número de instâncias verdadeiras para cada rótulo)\n",
    "* samples (amostra): Calcule as métricas para cada instância e encontre sua média    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0b22a",
   "metadata": {},
   "source": [
    "### **Análise Exploratória de dados** \n",
    "[&#x1F53C;](#Sumário:) Clique para voltar para o sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95e3da",
   "metadata": {},
   "source": [
    "### <u>Passo a passo</u><br>\n",
    "1. Importação dos dados (pd.read_extensão)\n",
    "2. Visuzalição das primeiras e últimas linhas (head/tail)\n",
    "3. Verificação do tamanho da base (shape)\n",
    "4. Verificação das informações (info)\n",
    "5. Contagem da quantidade de valores nulos (isnull().sum())\n",
    "6. Verificação das informações estatísticas (describe)\n",
    "7. Verificação da quantidade de valores diferentes na base (nunique)\n",
    "8. Visuzalição dos dados de forma gráfica (bar, hist, boxplot e scatter)\n",
    "9. Cálculo dos quartis, interquartil, valor máximo e valor mínimo\n",
    "10. Verificação dos valores acima do máximo ou abaixo do mínimo (outliers)\n",
    "11. Verificação das quantidades de registros em cada intervalo (<Q1, entre Q1 e Q2, entre Q2 e Q3,>Q3)\n",
    "12. Contagem da quantidade de valores de cada índice distinto (value_counts)\n",
    "13. Criação da combinação dos gráficos Histograma e KDE (sns.pairplot(DataFrame,hue=\"Coluna\"))\n",
    "14. Criação da matriz para mostrar a correlação de cada par de variáveis (corr)\n",
    "15. Criação da heatmap de correlação (sns.heatmap)\n",
    "16. Tratando valores vazios e outliers (Ex: Substituindo os valores nulos por essa média)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd336d",
   "metadata": {},
   "source": [
    "### <u>Visualização do KDE (Kernel Density Estimation)</u><br>\n",
    "> https://seaborn.pydata.org/generated/seaborn.kdeplot.html <br>\n",
    "> https://seaborn.pydata.org/generated/seaborn.pairplot.html <br>\n",
    "* Mede a chance de uma variável aleatória assumir determinado valor\n",
    "* A probabilidade é dada pela integral abaixo da curva na faixa de valor selecionada\n",
    "* Parece uma \"versão de linha suavizada\" de um histograma\n",
    "\n",
    "        sns.kdeplot(DataFrame.NomeDaColunaX,hue=DataFrame.NomeDaColunaValor)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe23de87",
   "metadata": {},
   "source": [
    "### <u>Heatmap de correlação</u><br>\n",
    "> https://matplotlib.org/stable/users/explain/colors/colormaps.html <br>\n",
    "\n",
    "### <u>Códigos</u><br>\n",
    "##### - Verificando a correlação entre os dados\n",
    "    DataFrame.corr()\n",
    "<br>\n",
    "\n",
    "##### - Tornando a correlação visual    \n",
    "    sns.heatmap(DataFrame.corr(),annot=True,cmap='CódigoDaCor')\n",
    "    plt.show()   \n",
    "    \n",
    "##### OBS: Correlação: Mostra o quanto duas variáveis estão relacionadas. Que vai de -1 (forte correlação negativa) até +1 (forte correlação positiva), onde 0 acontece quando não existe correlação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c896d",
   "metadata": {},
   "source": [
    "### <u>Pandas Profiling</u><br>\n",
    "> Documentação: https://pypi.org/project/pandas-profiling/ <br>\n",
    "\n",
    "#### - Instalação da biblioteca\n",
    "    pip install pandas-profiling\n",
    "<br>   \n",
    "\n",
    "#### - Importação das bibliotecas\n",
    "    import pandas as pd\n",
    "    from pandas_profiling import ProfileReport\n",
    "<br>   \n",
    "\n",
    "#### - Geração do relatório\n",
    "    DataFrame = pd.read_extensão('Arquivo.extensão')\n",
    "    var_profile = ProfileReport(DataFrame, title=\"Relatório de perfil do Pandas\")\n",
    "<br>   \n",
    "\n",
    "#### - Vizualização do relatório\n",
    "    var_profile\n",
    "<br>   \n",
    "\n",
    "#### - Exportando o relatório no formato html\n",
    "    var_profile.to_file(\"NomeDoArquivo.html\")\n",
    "<br> \n",
    "\n",
    "#### OBS: Se o erro w_median = data[weights == np.max(weights)][0] acontecer, consegue resolver fazendo:\n",
    "* 1. Buscando por utils_pandas.py\n",
    "* 2. Abrindo esse arquivo\n",
    "* 3. Trocando a linha acima por: w_median = data[np.where(weights == np.max(weights))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O Problema de Negócio (As perguntas do CEO)\n",
    "\n",
    "# Planejamento da solução\n",
    "\n",
    "# Planejamento do Produto Final\n",
    "\t# Como vamos entregar as respostas das perguntas de negócio?\n",
    "\t# 1. Vamos escrever um e-mail?\n",
    "\t# 2. Vamos enviar uma planilha?\n",
    "\t# 3. Vamos enviar um gráfico em uma ferramenta de visualização.\n",
    "\t# 4. Vamos enviar um algoritmo treinado?\n",
    "\n",
    "# Planejamento do Processo\n",
    "\t# Quais são os passos para encontrar as respostas?\n",
    "\t# 1. Coletar um conjunto de dados ( planilha de dados )\n",
    "\t# 2. Manipular os dados\n",
    "\t\t# Faz o carregamento de um arquivo salvo no computador\n",
    "\t\t# uploaded = files.upload()\n",
    "\t\t# df = pd.read_csv( io.BytesIO( uploaded['NomeDoArquivo.csv'] ) )\n",
    "\t\t# print( type( df ) ) # verificar o tipo de objeto.\n",
    "\t\t# Mostrar o número de linhas e colunas do cojunto de dados\n",
    "\t\t# print( df.shape )\n",
    "\t\t# Mostrar as 6 primeiras linhas\n",
    "\t\t# print( df.head() )\n",
    "\t\t# Mostrar os tipos das variáveis de cada coluna\n",
    "\t\t# print( df.dtypes() )\n",
    "\t\t# Mostrar o número de NA em cada coluna.\n",
    "\t\t# print( df.isna().sum() )\n",
    "\t\t# Mostrar as estatísticas do conjunto de dados.\n",
    "\t\t# print( df.info() )\n",
    "\t\t# Comando padrão para seleção de colunas\n",
    "\t\t# df.loc[linhas, colunas]\n",
    "\t\t# Selecionando colunas → cols = ['NomeDaColuna1', 'NomeDaColuna2', 'NomeDaColuna3']\n",
    "\t\t# Selecionando as linhas → # Vamos usar o símbolo : para selecionar todas as linhas\n",
    "\t\t# df1 = df.loc[:, cols]\n",
    "\t\t# calculando a média\n",
    "\t\t# media = np.mean( df1 )[0]\n",
    "\t\t# print( media )\n",
    "\t\t# calculando o desvio padrao\n",
    "\t\t# desvio_padrao = np.std( df1 )[0]\n",
    "\t\t# print( desvio_padrao )\n",
    "\t\t# desenhando o gráfico\n",
    "\t\t# hist = px.histogram( df1[df1['price'] < 2000], x='price', nbins=20 )\n",
    "\t\t# hist.show()\n",
    "\n",
    "# Planejamento das Ferramentas\n",
    "\t# Quais as ferramentas que precisamos usar para encontrar as respostas?\n",
    "\t# 1. Linguagem de programação\n",
    "\t# 2. Estatística\n",
    "\t# 3. Ferramenta para usar a linguagem de programação\n",
    "\n",
    "# Respondendo as perguntas de negócio\n",
    "# Mais perguntas do CEO\n",
    "\n",
    "# Exemplos:\n",
    "\n",
    "# Qual é o valor do aluguel (diária) mais caro de cada região da base de dados da cidade de Nova York,\n",
    "# apenas para os imóveis disponível para alugar?\n",
    "# R: O diária mais cara no Bronx é de U$ 800\n",
    "# R: O diária mais cara no Brooklyn é de U$ 8000\n",
    "# R: O diária mais cara no Manhattan é de U$ 10000\n",
    "# R: O diária mais cara no Queens é de U$ 2600\n",
    "# R: O diária mais cara no Staten Island é de U$ 625\n",
    "\n",
    "# df_grouped = df[['price', 'neighbourhood_group']].groupby( 'neighbourhood_group' )\n",
    "# data_plot = df_grouped.max().reset_index()\n",
    "# px.bar(data_plot,x = 'neighbourhood_group',y = 'price')\n",
    "\n",
    "# Conseguimos saber onde estão localizados os imóveis com o valor do aluguel mais caro, na cidade de Nova York,\n",
    "# apenas para os imóveis disponível para alugar?\n",
    "# f = folium.Figure( width=1024, height=768 )\n",
    "# data_plot = df[['price', 'neighbourhood_group', 'latitude', 'longitude']].groupby( ['neighbourhood_group'] ).max().reset_index()\n",
    "# map = folium.Map(location=[data_plot['latitude'].mean(),data_plot['longitude'].mean()],zoom_start=14,control_scale=True )\n",
    "\n",
    "# for index, location_info in data_plot.iterrows():\n",
    "# folium.Marker( [location_info['latitude'], location_info['longitude']],popup=location_info['neighbourhood_group'] ).add_to( map )\n",
    "\n",
    "#Conseguimos saber onde estão localizados os imóveis pelo seu tipo, apenas para os imóveis disponível para alugar?\n",
    "\n",
    "# data = df[['neighbourhood_group', 'room_type', 'latitude', 'longitude']].sample(50)\n",
    "# data_plot = data.copy()\n",
    "# data_plot['color'] = 'NA'\n",
    "# map = folium.Map(location=[data_plot['latitude'].mean(),data_plot['longitude'].mean()],zoom_start=14,control_scale=True )\n",
    "# data_plot.loc[data_plot['room_type'] == 'Private room', 'color'] = 'darkgreen'\n",
    "# data_plot.loc[data_plot['room_type'] == 'Entire home/apt', 'color'] = 'darkred'\n",
    "# data_plot.loc[data_plot['room_type'] == 'Shared room', 'color'] = 'purple'\n",
    "\n",
    "# for index, location_info in data_plot.iterrows():\n",
    "# folium.Marker( [location_info['latitude'], location_info['longitude']],popup=location_info['neighbourhood_group'],icon=folium.Icon( color=location_info['color']), ).add_to( map )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
